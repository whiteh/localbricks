openapi: 3.0.0
info:
  description: "The Jobs API allows you to create, edit, and delete jobs."
  title: Jobs API 2.1
  version: "2.1"
servers:
- description: "In the URL, substitute `<databricks-instance>` with the domain name\
    \ of your deployment. Use the form `<account>.cloud.databricks.com`."
  url: https://<databricks-instance>/api/
paths:
  /2.0/jobs/runs/export:
    get:
      description: Export and retrieve the job run task.
      operationId: jobs_runs_export
      parameters:
      - description: The canonical identifier for the run. This field is required.
        explode: true
        in: query
        name: run_id
        required: true
        schema:
          example: 455644833
          format: int64
          type: integer
        style: form
      - description: "Which views to export (CODE, DASHBOARDS, or ALL). Defaults to\
          \ CODE."
        explode: true
        in: query
        name: views_to_export
        required: false
        schema:
          $ref: '#/components/schemas/ViewsToExport'
        style: form
      responses:
        "200":
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/JobsRunsExport_200_response'
          description: Run was exported successfully.
        "400":
          content:
            application/json:
              example:
                error_code: INVALID_PARAMETER_VALUE
                message: Invalid value for parameter job_id
              schema:
                $ref: '#/components/schemas/Error'
          description: The request was malformed. See JSON response for error details.
        "401":
          content:
            application/json:
              example:
                error_code: PERMISSION_DENIED
                message: Unauthorized access.
              schema:
                $ref: '#/components/schemas/Error'
          description: The request was unauthorized.
        "500":
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Error'
          description: The request was not handled correctly due to a server error.
      security:
      - bearerAuth: []
      summary: Export and retrieve a job run
      x-openapi-router-controller: openapi_server.controllers.default_controller
  /2.1/jobs/create:
    post:
      description: Create a new job.
      operationId: jobs_create
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/JobsCreate_request'
        required: true
      responses:
        "200":
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/JobsCreate_200_response'
          description: Job was created successfully
        "400":
          content:
            application/json:
              example:
                error_code: INVALID_PARAMETER_VALUE
                message: Invalid value for parameter job_id
              schema:
                $ref: '#/components/schemas/Error'
          description: The request was malformed. See JSON response for error details.
        "401":
          content:
            application/json:
              example:
                error_code: PERMISSION_DENIED
                message: Unauthorized access.
              schema:
                $ref: '#/components/schemas/Error'
          description: The request was unauthorized.
        "500":
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Error'
          description: The request was not handled correctly due to a server error.
      security:
      - bearerAuth: []
      summary: Create a new job
      x-openapi-router-controller: openapi_server.controllers.default_controller
  /2.1/jobs/delete:
    post:
      description: Deletes a job.
      operationId: jobs_delete
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/JobsDelete_request'
        required: true
      responses:
        "200":
          content:
            application/json:
              schema:
                properties: {}
                type: object
          description: Job was deleted successfully.
        "400":
          content:
            application/json:
              example:
                error_code: INVALID_PARAMETER_VALUE
                message: Invalid value for parameter job_id
              schema:
                $ref: '#/components/schemas/Error'
          description: The request was malformed. See JSON response for error details.
        "401":
          content:
            application/json:
              example:
                error_code: PERMISSION_DENIED
                message: Unauthorized access.
              schema:
                $ref: '#/components/schemas/Error'
          description: The request was unauthorized.
        "500":
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Error'
          description: The request was not handled correctly due to a server error.
      security:
      - bearerAuth: []
      summary: Delete a job
      x-openapi-router-controller: openapi_server.controllers.default_controller
  /2.1/jobs/get:
    get:
      description: Retrieves the details for a single job.
      operationId: jobs_get
      parameters:
      - description: The canonical identifier of the job to retrieve information about.
          This field is required.
        explode: true
        in: query
        name: job_id
        required: true
        schema:
          example: 11223344
          format: int64
          type: integer
        style: form
      responses:
        "200":
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/JobsGet_200_response'
          description: Job was retrieved successfully.
        "400":
          content:
            application/json:
              example:
                error_code: INVALID_PARAMETER_VALUE
                message: Invalid value for parameter job_id
              schema:
                $ref: '#/components/schemas/Error'
          description: The request was malformed. See JSON response for error details.
        "401":
          content:
            application/json:
              example:
                error_code: PERMISSION_DENIED
                message: Unauthorized access.
              schema:
                $ref: '#/components/schemas/Error'
          description: The request was unauthorized.
        "500":
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Error'
          description: The request was not handled correctly due to a server error.
      security:
      - bearerAuth: []
      summary: Get a single job
      x-openapi-router-controller: openapi_server.controllers.default_controller
  /2.1/jobs/list:
    get:
      description: Retrieves a list of jobs.
      operationId: jobs_list
      parameters:
      - description: The number of jobs to return. This value must be greater than
          0 and less or equal to 25. The default value is 20.
        explode: true
        in: query
        name: limit
        required: false
        schema:
          default: 20
          example: 25
          maximum: 25
          minimum: 1
          type: integer
        style: form
      - description: "The offset of the first job to return, relative to the most\
          \ recently created job."
        explode: true
        in: query
        name: offset
        required: false
        schema:
          default: 0
          example: 0
          minimum: 0
          type: integer
        style: form
      - description: Whether to include task and cluster details in the response.
        explode: true
        in: query
        name: expand_tasks
        required: false
        schema:
          default: false
          example: false
          type: boolean
        style: form
      responses:
        "200":
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/JobsList_200_response'
          description: List of jobs was retrieved successfully.
        "400":
          content:
            application/json:
              example:
                error_code: INVALID_PARAMETER_VALUE
                message: Invalid value for parameter job_id
              schema:
                $ref: '#/components/schemas/Error'
          description: The request was malformed. See JSON response for error details.
        "401":
          content:
            application/json:
              example:
                error_code: PERMISSION_DENIED
                message: Unauthorized access.
              schema:
                $ref: '#/components/schemas/Error'
          description: The request was unauthorized.
        "500":
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Error'
          description: The request was not handled correctly due to a server error.
      security:
      - bearerAuth: []
      summary: List all jobs
      x-openapi-router-controller: openapi_server.controllers.default_controller
  /2.1/jobs/reset:
    post:
      description: Overwrites all the settings for a specific job. Use the Update
        endpoint to update job settings partially.
      operationId: jobs_reset
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/JobsReset_request'
        required: true
      responses:
        "200":
          content:
            application/json:
              schema:
                properties: {}
                type: object
          description: Job was overwritten successfully.
        "400":
          content:
            application/json:
              example:
                error_code: INVALID_PARAMETER_VALUE
                message: Invalid value for parameter job_id
              schema:
                $ref: '#/components/schemas/Error'
          description: The request was malformed. See JSON response for error details.
        "401":
          content:
            application/json:
              example:
                error_code: PERMISSION_DENIED
                message: Unauthorized access.
              schema:
                $ref: '#/components/schemas/Error'
          description: The request was unauthorized.
        "500":
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Error'
          description: The request was not handled correctly due to a server error.
      security:
      - bearerAuth: []
      summary: Overwrites all settings for a job
      x-openapi-router-controller: openapi_server.controllers.default_controller
  /2.1/jobs/run-now:
    post:
      description: Run a job and return the `run_id` of the triggered run.
      operationId: jobs_run_now
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/JobsRunNow_request'
        required: true
      responses:
        "200":
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/JobsRunNow_200_response'
          description: Run was started successfully.
        "400":
          content:
            application/json:
              example:
                error_code: INVALID_PARAMETER_VALUE
                message: Invalid value for parameter job_id
              schema:
                $ref: '#/components/schemas/Error'
          description: The request was malformed. See JSON response for error details.
        "401":
          content:
            application/json:
              example:
                error_code: PERMISSION_DENIED
                message: Unauthorized access.
              schema:
                $ref: '#/components/schemas/Error'
          description: The request was unauthorized.
        "500":
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Error'
          description: The request was not handled correctly due to a server error.
      security:
      - bearerAuth: []
      summary: Trigger a new job run
      x-openapi-router-controller: openapi_server.controllers.default_controller
  /2.1/jobs/runs/cancel:
    post:
      description: "Cancels a run. The run is canceled asynchronously, so when this\
        \ request completes, the run may still be running. The run are terminated\
        \ shortly. If the run is already in a terminal `life_cycle_state`, this method\
        \ is a no-op."
      operationId: jobs_runs_cancel
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/JobsRunsCancel_request'
        required: true
      responses:
        "200":
          content:
            application/json:
              schema:
                properties: {}
                type: object
          description: Run was cancelled successfully.
        "400":
          content:
            application/json:
              example:
                error_code: INVALID_PARAMETER_VALUE
                message: Invalid value for parameter job_id
              schema:
                $ref: '#/components/schemas/Error'
          description: The request was malformed. See JSON response for error details.
        "401":
          content:
            application/json:
              example:
                error_code: PERMISSION_DENIED
                message: Unauthorized access.
              schema:
                $ref: '#/components/schemas/Error'
          description: The request was unauthorized.
        "500":
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Error'
          description: The request was not handled correctly due to a server error.
      security:
      - bearerAuth: []
      summary: Cancel a job run
      x-openapi-router-controller: openapi_server.controllers.default_controller
  /2.1/jobs/runs/delete:
    post:
      description: Deletes a non-active run. Returns an error if the run is active.
      operationId: jobs_runs_delete
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/JobsRunsDelete_request'
        required: true
      responses:
        "200":
          content:
            application/json:
              schema:
                properties: {}
                type: object
          description: Run was deleted successfully.
        "400":
          content:
            application/json:
              example:
                error_code: INVALID_PARAMETER_VALUE
                message: Invalid value for parameter job_id
              schema:
                $ref: '#/components/schemas/Error'
          description: The request was malformed. See JSON response for error details.
        "401":
          content:
            application/json:
              example:
                error_code: PERMISSION_DENIED
                message: Unauthorized access.
              schema:
                $ref: '#/components/schemas/Error'
          description: The request was unauthorized.
        "500":
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Error'
          description: The request was not handled correctly due to a server error.
      security:
      - bearerAuth: []
      summary: Delete a job run
      x-openapi-router-controller: openapi_server.controllers.default_controller
  /2.1/jobs/runs/get:
    get:
      description: Retrieve the metadata of a run.
      operationId: jobs_runs_get
      parameters:
      - description: The canonical identifier of the run for which to retrieve the
          metadata. This field is required.
        explode: true
        in: query
        name: run_id
        required: true
        schema:
          example: 455644833
          format: int64
          type: integer
        style: form
      - description: Whether to include the repair history in the response.
        explode: true
        in: query
        name: include_history
        required: false
        schema:
          example: true
          type: boolean
        style: form
      responses:
        "200":
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/JobsRunsGet_200_response'
          description: Run was retrieved successfully
        "400":
          content:
            application/json:
              example:
                error_code: INVALID_PARAMETER_VALUE
                message: Invalid value for parameter job_id
              schema:
                $ref: '#/components/schemas/Error'
          description: The request was malformed. See JSON response for error details.
        "401":
          content:
            application/json:
              example:
                error_code: PERMISSION_DENIED
                message: Unauthorized access.
              schema:
                $ref: '#/components/schemas/Error'
          description: The request was unauthorized.
        "500":
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Error'
          description: The request was not handled correctly due to a server error.
      security:
      - bearerAuth: []
      summary: Get a single job run
      x-openapi-router-controller: openapi_server.controllers.default_controller
  /2.1/jobs/runs/get-output:
    get:
      description: "Retrieve the output and metadata of a run. When a notebook task\
        \ returns a value through the dbutils.notebook.exit() call, you can use this\
        \ endpoint to retrieve that value. Databricks restricts this API to return\
        \ the first 5 MB of the output. To return a larger result, you can store job\
        \ results in a cloud storage service.\nThis endpoint validates that the run_id\
        \ parameter is valid and returns an HTTP status code 400 if the run_id parameter\
        \ is invalid.\nRuns are automatically removed after 60 days. If you to want\
        \ to reference them beyond 60 days, you must save old run results before they\
        \ expire. To export using the UI, see Export job run results. To export using\
        \ the Jobs API, see Runs export."
      operationId: jobs_runs_get_output
      parameters:
      - description: The canonical identifier for the run. This field is required.
        explode: true
        in: query
        name: run_id
        required: true
        schema:
          example: 455644833
          format: int64
          type: integer
        style: form
      responses:
        "200":
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/JobsRunsGetOutput_200_response'
          description: Run output was retrieved successfully.
        "400":
          content:
            application/json:
              example:
                error_code: INVALID_PARAMETER_VALUE
                message: Invalid value for parameter job_id
              schema:
                $ref: '#/components/schemas/Error'
          description: The request was malformed. See JSON response for error details.
        "401":
          content:
            application/json:
              example:
                error_code: PERMISSION_DENIED
                message: Unauthorized access.
              schema:
                $ref: '#/components/schemas/Error'
          description: The request was unauthorized.
        "500":
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Error'
          description: The request was not handled correctly due to a server error.
      security:
      - bearerAuth: []
      summary: Get the output for a single run
      x-openapi-router-controller: openapi_server.controllers.default_controller
  /2.1/jobs/runs/list:
    get:
      description: List runs in descending order by start time.
      operationId: jobs_runs_list
      parameters:
      - description: "If active_only is `true`, only active runs are included in the\
          \ results; otherwise, lists both active and completed runs. An active run\
          \ is a run in the `PENDING`, `RUNNING`, or `TERMINATING`. This field cannot\
          \ be `true` when completed_only is `true`."
        explode: true
        in: query
        name: active_only
        required: false
        schema:
          default: false
          example: false
          type: boolean
        style: form
      - description: "If completed_only is `true`, only completed runs are included\
          \ in the results; otherwise, lists both active and completed runs. This\
          \ field cannot be `true` when active_only is `true`."
        explode: true
        in: query
        name: completed_only
        required: false
        schema:
          default: false
          example: false
          type: boolean
        style: form
      - description: "The job for which to list runs. If omitted, the Jobs service\
          \ lists runs from all jobs."
        explode: true
        in: query
        name: job_id
        required: false
        schema:
          example: 11223344
          format: int64
          type: integer
        style: form
      - description: "The offset of the first run to return, relative to the most\
          \ recent run."
        explode: true
        in: query
        name: offset
        required: false
        schema:
          default: 0
          example: 0
          format: int32
          type: integer
        style: form
      - description: "The number of runs to return. This value must be greater than\
          \ 0 and less than 25\\. The default value is 25\\. If a request specifies\
          \ a limit of 0, the service instead uses the maximum limit."
        explode: true
        in: query
        name: limit
        required: false
        schema:
          default: 25
          example: 25
          format: int32
          maximum: 25
          minimum: 1
          type: integer
        style: form
      - description: "The type of runs to return. For a description of run types,\
          \ see [Run](https://docs.databricks.com/dev-tools/api/latest/jobs.html#operation/JobsRunsGet)."
        explode: true
        in: query
        name: run_type
        required: false
        schema:
          enum:
          - JOB_RUN
          - WORKFLOW_RUN
          - SUBMIT_RUN
          example: JOB_RUN
          type: string
        style: form
      - description: Whether to include task and cluster details in the response.
        explode: true
        in: query
        name: expand_tasks
        required: false
        schema:
          default: false
          example: false
          type: boolean
        style: form
      - description: Show runs that started _at or after_ this value. The value must
          be a UTC timestamp in milliseconds. Can be combined with _start_time_to_
          to filter by a time range.
        explode: true
        in: query
        name: start_time_from
        required: false
        schema:
          example: 1642521600000
          type: integer
        style: form
      - description: Show runs that started _at or before_ this value. The value must
          be a UTC timestamp in milliseconds. Can be combined with _start_time_from_
          to filter by a time range.
        explode: true
        in: query
        name: start_time_to
        required: false
        schema:
          example: 1642608000000
          type: integer
        style: form
      responses:
        "200":
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/JobsRunsList_200_response'
          description: List of runs was retrieved successfully.
        "400":
          content:
            application/json:
              example:
                error_code: INVALID_PARAMETER_VALUE
                message: Invalid value for parameter job_id
              schema:
                $ref: '#/components/schemas/Error'
          description: The request was malformed. See JSON response for error details.
        "401":
          content:
            application/json:
              example:
                error_code: PERMISSION_DENIED
                message: Unauthorized access.
              schema:
                $ref: '#/components/schemas/Error'
          description: The request was unauthorized.
        "500":
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Error'
          description: The request was not handled correctly due to a server error.
      security:
      - bearerAuth: []
      summary: List runs for a job
      x-openapi-router-controller: openapi_server.controllers.default_controller
  /2.1/jobs/runs/repair:
    post:
      description: "Re-run one or more tasks. Tasks are re-run as part of the original\
        \ job run, use the current job and task settings, and can be viewed in the\
        \ history for the original job run."
      operationId: jobs_runs_repair
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/JobsRunsRepair_request'
        required: true
      responses:
        "200":
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/JobsRunsRepair_200_response'
          description: Run repair was initiated.
        "400":
          content:
            application/json:
              example:
                error_code: INVALID_PARAMETER_VALUE
                message: Invalid value for parameter job_id
              schema:
                $ref: '#/components/schemas/Error'
          description: The request was malformed. See JSON response for error details.
        "401":
          content:
            application/json:
              example:
                error_code: PERMISSION_DENIED
                message: Unauthorized access.
              schema:
                $ref: '#/components/schemas/Error'
          description: The request was unauthorized.
        "500":
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Error'
          description: The request was not handled correctly due to a server error.
      security:
      - bearerAuth: []
      summary: Repair a job run
      x-openapi-router-controller: openapi_server.controllers.default_controller
  /2.1/jobs/runs/submit:
    post:
      description: Submit a one-time run. This endpoint allows you to submit a workload
        directly without creating a job. Runs submitted using this endpoint don’t
        display in the UI. Use the `jobs/runs/get` API to check the run state after
        the job is submitted.
      operationId: jobs_runs_submit
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/JobsRunsSubmit_request'
        required: true
      responses:
        "200":
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/JobsRunsSubmit_200_response'
          description: Run was created and started successfully.
        "400":
          content:
            application/json:
              example:
                error_code: INVALID_PARAMETER_VALUE
                message: Invalid value for parameter job_id
              schema:
                $ref: '#/components/schemas/Error'
          description: The request was malformed. See JSON response for error details.
        "401":
          content:
            application/json:
              example:
                error_code: PERMISSION_DENIED
                message: Unauthorized access.
              schema:
                $ref: '#/components/schemas/Error'
          description: The request was unauthorized.
        "500":
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Error'
          description: The request was not handled correctly due to a server error.
      security:
      - bearerAuth: []
      summary: Create and trigger a one-time run
      x-openapi-router-controller: openapi_server.controllers.default_controller
  /2.1/jobs/update:
    post:
      description: "Add, update, or remove specific settings of an existing job. Use\
        \ the Reset endpoint to overwrite all job settings."
      operationId: jobs_update
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/JobsUpdate_request'
        required: true
      responses:
        "200":
          content:
            application/json:
              schema:
                properties: {}
                type: object
          description: Job was updated successfully.
        "400":
          content:
            application/json:
              example:
                error_code: INVALID_PARAMETER_VALUE
                message: Invalid value for parameter job_id
              schema:
                $ref: '#/components/schemas/Error'
          description: The request was malformed. See JSON response for error details.
        "401":
          content:
            application/json:
              example:
                error_code: PERMISSION_DENIED
                message: Unauthorized access.
              schema:
                $ref: '#/components/schemas/Error'
          description: The request was unauthorized.
        "500":
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Error'
          description: The request was not handled correctly due to a server error.
      security:
      - bearerAuth: []
      summary: Partially updates a job
      x-openapi-router-controller: openapi_server.controllers.default_controller
components:
  responses:
    Unauthorized:
      content:
        application/json:
          example:
            error_code: PERMISSION_DENIED
            message: Unauthorized access.
          schema:
            $ref: '#/components/schemas/Error'
      description: The request was unauthorized.
    BadRequest:
      content:
        application/json:
          example:
            error_code: INVALID_PARAMETER_VALUE
            message: Invalid value for parameter job_id
          schema:
            $ref: '#/components/schemas/Error'
      description: The request was malformed. See JSON response for error details.
    NotFound:
      content:
        application/json:
          examples:
            resource_does_not_exist:
              value:
                error_code: ENDPOINT_NOT_FOUND
                message: No API endpoint found
          schema:
            $ref: '#/components/schemas/Error'
      description: The requested resource does not exist.
    InternalError:
      content:
        application/json:
          schema:
            $ref: '#/components/schemas/Error'
      description: The request was not handled correctly due to a server error.
  schemas:
    ClusterInstance:
      example:
        cluster_id: 0923-164208-meows279
        spark_context_id: spark_context_id
      properties:
        cluster_id:
          description: "The canonical identifier for the cluster used by a run. This\
            \ field is always available for runs on existing clusters. For runs on\
            \ new clusters, it becomes available once the cluster is created. This\
            \ value can be used to view logs by browsing to `/#setting/sparkui/$cluster_id/driver-logs`.\
            \ The logs continue to be available after the run completes.\n\nThe response\
            \ won’t include this field if the identifier is not available yet."
          example: 0923-164208-meows279
          title: cluster_id
          type: string
        spark_context_id:
          description: |-
            The canonical identifier for the Spark context used by a run. This field is filled in once the run begins execution. This value can be used to view the Spark UI by browsing to `/#setting/sparkui/$cluster_id/$spark_context_id`. The Spark UI continues to be available after the run has completed.

            The response won’t include this field if the identifier is not available yet.
          title: spark_context_id
          type: string
      title: ClusterInstance
    ClusterSpec:
      example:
        new_cluster:
          spark_conf:
            key: ""
          cluster_log_conf:
            s3:
              encryption_type: encryption_type
              enable_encryption: true
              endpoint: endpoint
              canned_acl: canned_acl
              destination: destination
              region: region
              kms_key: kms_key
            dbfs:
              destination: destination
          enable_elastic_disk: true
          policy_id: policy_id
          spark_env_vars:
            key: ""
          node_type_id: node_type_id
          ssh_public_keys:
          - ssh_public_keys
          - ssh_public_keys
          custom_tags:
            key: custom_tags
          num_workers: 0
          autoscale:
            max_workers: 1
            min_workers: 6
          instance_pool_id: instance_pool_id
          aws_attributes:
            ebs_volume_count: 2
            zone_id: zone_id
            ebs_volume_size: 7
            ebs_volume_throughput: 3
            spot_bid_price_percent: 5
            availability: SPOT
            ebs_volume_type: GENERAL_PURPOSE_SSD
            first_on_demand: 5
            ebs_volume_iops: 9
            instance_profile_arn: instance_profile_arn
          spark_version: spark_version
          driver_node_type_id: driver_node_type_id
          init_scripts:
          - S3:
              encryption_type: encryption_type
              enable_encryption: true
              endpoint: endpoint
              canned_acl: canned_acl
              destination: destination
              region: region
              kms_key: kms_key
            file:
              destination: destination
            dbfs:
              destination: destination
          - S3:
              encryption_type: encryption_type
              enable_encryption: true
              endpoint: endpoint
              canned_acl: canned_acl
              destination: destination
              region: region
              kms_key: kms_key
            file:
              destination: destination
            dbfs:
              destination: destination
          driver_instance_pool_id: driver_instance_pool_id
        libraries:
        - cran:
            package: geojson
            repo: https://my-repo.com
          egg: dbfs:/my/egg
          pypi:
            package: simplejson==3.8.0
            repo: https://my-repo.com
          maven:
            repo: https://my-repo.com
            coordinates: org.jsoup:jsoup:1.7.2
            exclusions:
            - slf4j:slf4j
            - '*:hadoop-client'
          jar: dbfs:/my-jar.jar
          whl: dbfs:/my/whl
        - cran:
            package: geojson
            repo: https://my-repo.com
          egg: dbfs:/my/egg
          pypi:
            package: simplejson==3.8.0
            repo: https://my-repo.com
          maven:
            repo: https://my-repo.com
            coordinates: org.jsoup:jsoup:1.7.2
            exclusions:
            - slf4j:slf4j
            - '*:hadoop-client'
          jar: dbfs:/my-jar.jar
          whl: dbfs:/my/whl
        existing_cluster_id: 0923-164208-meows279
      properties:
        existing_cluster_id:
          description: "If existing_cluster_id, the ID of an existing cluster that\
            \ is used for all runs of this job. When running jobs on an existing cluster,\
            \ you may need to manually restart the cluster if it stops responding.\
            \ We suggest running jobs on new clusters for greater reliability."
          example: 0923-164208-meows279
          title: existing_cluster_id
          type: string
        new_cluster:
          $ref: '#/components/schemas/NewCluster'
        libraries:
          description: An optional list of libraries to be installed on the cluster
            that executes the job. The default value is an empty list.
          items:
            $ref: '#/components/schemas/Library'
          title: libraries
          type: array
      title: ClusterSpec
    CronSchedule:
      example:
        quartz_cron_expression: 20 30 * * * ?
        timezone_id: Europe/London
        pause_status: PAUSED
      properties:
        quartz_cron_expression:
          description: "A Cron expression using Quartz syntax that describes the schedule\
            \ for a job. See [Cron Trigger](http://www.quartz-scheduler.org/documentation/quartz-2.3.0/tutorials/crontrigger.html)\
            \ for details. This field is required."
          example: 20 30 * * * ?
          title: quartz_cron_expression
          type: string
        timezone_id:
          description: "A Java timezone ID. The schedule for a job is resolved with\
            \ respect to this timezone. See [Java TimeZone](https://docs.oracle.com/javase/7/docs/api/java/util/TimeZone.html)\
            \ for details. This field is required."
          example: Europe/London
          title: timezone_id
          type: string
        pause_status:
          description: Indicate whether this schedule is paused or not.
          enum:
          - PAUSED
          - UNPAUSED
          example: PAUSED
          title: pause_status
          type: string
      required:
      - quartz_cron_expression
      - timezone_id
      title: CronSchedule
    GitSource:
      description: |-
        This functionality is in Public Preview.

        An optional specification for a remote repository containing the notebooks used by this job's notebook tasks.
      example:
        git_commit: e0056d01
        git_tag: release-1.0.0
        git_provider: github
        git_branch: main
        git_url: https://github.com/databricks/databricks-cli
        git_snapshot:
          used_commit: 4506fdf41e9fa98090570a34df7a5bce163ff15f
      oneOf:
      - required:
        - git_branch
        - git_provider
        - git_url
      - required:
        - git_provider
        - git_tag
        - git_url
      - required:
        - git_commit
        - git_provider
        - git_url
      properties:
        git_url:
          description: |-
            URL of the repository to be cloned by this job.
            The maximum length is 300 characters.
          example: https://github.com/databricks/databricks-cli
          title: git_url
          type: string
        git_provider:
          description: Unique identifier of the service used to host the Git repository.
            The value is case insensitive.
          enum:
          - gitHub
          - bitbucketCloud
          - azureDevOpsServices
          - gitHubEnterprise
          - bitbucketServer
          - gitLab
          - gitLabEnterpriseEdition
          - awsCodeCommit
          example: github
          title: git_provider
          type: string
        git_branch:
          description: |-
            Name of the branch to be checked out and used by this job. This field cannot be specified in conjunction with git_tag or git_commit.
            The maximum length is 255 characters.
          example: main
          title: git_branch
          type: string
        git_tag:
          description: |-
            Name of the tag to be checked out and used by this job. This field cannot be specified in conjunction with git_branch or git_commit.
            The maximum length is 255 characters.
          example: release-1.0.0
          title: git_tag
          type: string
        git_commit:
          description: |-
            Commit to be checked out and used by this job. This field cannot be specified in conjunction with git_branch or git_tag.
            The maximum length is 64 characters.
          example: e0056d01
          title: git_commit
          type: string
        git_snapshot:
          $ref: '#/components/schemas/GitSnapshot'
      title: GitSource
    GitSnapshot:
      description: Read-only state of the remote repository at the time the job was
        run. This field is only included on job runs.
      example:
        used_commit: 4506fdf41e9fa98090570a34df7a5bce163ff15f
      properties:
        used_commit:
          description: "Commit that was used to execute the run. If git_branch was\
            \ specified, this points to the HEAD of the branch at the time of the\
            \ run; if git_tag was specified, this points to the commit the tag points\
            \ to."
          example: 4506fdf41e9fa98090570a34df7a5bce163ff15f
          title: used_commit
          type: string
      readOnly: true
      title: GitSnapshot
    Job:
      example:
        settings:
          schedule:
            quartz_cron_expression: 20 30 * * * ?
            timezone_id: Europe/London
            pause_status: PAUSED
          max_concurrent_runs: 10
          name: A multitask job
          format: MULTI_TASK
          email_notifications:
            on_failure:
            - user.name@databricks.com
            no_alert_for_skipped_runs: false
            on_start:
            - user.name@databricks.com
            on_success:
            - user.name@databricks.com
          job_clusters:
          - job_cluster_key: auto_scaling_cluster
            new_cluster:
              spark_version: 7.3.x-scala2.12
              node_type_id: i3.xlarge
              spark_conf:
                spark.speculation: true
              aws_attributes:
                availability: SPOT
                zone_id: us-west-2a
              autoscale:
                min_workers: 2
                max_workers: 16
          timeout_seconds: 86400
          tasks:
          - task_key: Sessionize
            description: Extracts session data from events
            depends_on: []
            existing_cluster_id: 0923-164208-meows279
            spark_jar_task:
              main_class_name: com.databricks.Sessionize
              parameters:
              - --data
              - dbfs:/path/to/data.json
            libraries:
            - jar: dbfs:/mnt/databricks/Sessionize.jar
            timeout_seconds: 86400
            max_retries: 3
            min_retry_interval_millis: 2000
            retry_on_timeout: false
          - task_key: Orders_Ingest
            description: Ingests order data
            depends_on: []
            job_cluster_key: auto_scaling_cluster
            spark_jar_task:
              main_class_name: com.databricks.OrdersIngest
              parameters:
              - --data
              - dbfs:/path/to/order-data.json
            libraries:
            - jar: dbfs:/mnt/databricks/OrderIngest.jar
            timeout_seconds: 86400
            max_retries: 3
            min_retry_interval_millis: 2000
            retry_on_timeout: false
          - task_key: Match
            description: Matches orders with user sessions
            depends_on:
            - task_key: Orders_Ingest
            - task_key: Sessionize
            new_cluster:
              spark_version: 7.3.x-scala2.12
              node_type_id: i3.xlarge
              spark_conf:
                spark.speculation: true
              aws_attributes:
                availability: SPOT
                zone_id: us-west-2a
              autoscale:
                min_workers: 2
                max_workers: 16
            notebook_task:
              notebook_path: /Users/user.name@databricks.com/Match
              base_parameters:
                name: John Doe
                age: "35"
            timeout_seconds: 86400
            max_retries: 3
            min_retry_interval_millis: 2000
            retry_on_timeout: false
          git_source:
            git_commit: e0056d01
            git_tag: release-1.0.0
            git_provider: github
            git_branch: main
            git_url: https://github.com/databricks/databricks-cli
            git_snapshot:
              used_commit: 4506fdf41e9fa98090570a34df7a5bce163ff15f
          tags:
            cost-center: engineering
            team: jobs
        created_time: 1601370337343
        creator_user_name: user.name@databricks.com
        job_id: 11223344
        run_as_user_name: user.name@databricks.com
      properties:
        job_id:
          description: The canonical identifier for this job.
          example: 11223344
          format: int64
          title: job_id
          type: integer
        creator_user_name:
          description: The creator user name. This field won’t be included in the
            response if the user has already been deleted.
          example: user.name@databricks.com
          title: creator_user_name
          type: string
        run_as_user_name:
          description: "The user name that the job runs as. `run_as_user_name` is\
            \ based on the current job settings, and is set to the creator of the\
            \ job if job access control is disabled, or the `is_owner` permission\
            \ if job access control is enabled."
          example: user.name@databricks.com
          title: run_as_user_name
          type: string
        settings:
          $ref: '#/components/schemas/JobSettings'
        created_time:
          description: The time at which this job was created in epoch milliseconds
            (milliseconds since 1/1/1970 UTC).
          example: 1601370337343
          format: int64
          title: created_time
          type: integer
      title: Job
    JobEmailNotifications:
      example:
        on_failure:
        - user.name@databricks.com
        no_alert_for_skipped_runs: false
        on_start:
        - user.name@databricks.com
        on_success:
        - user.name@databricks.com
      properties:
        on_start:
          description: "A list of email addresses to be notified when a run begins.\
            \ If not specified on job creation, reset, or update, the list is empty,\
            \ and notifications are not sent."
          example:
          - user.name@databricks.com
          items:
            type: string
          title: on_start
          type: array
        on_success:
          description: "A list of email addresses to be notified when a run successfully\
            \ completes. A run is considered to have completed successfully if it\
            \ ends with a `TERMINATED` `life_cycle_state` and a `SUCCESSFUL` result_state.\
            \ If not specified on job creation, reset, or update, the list is empty,\
            \ and notifications are not sent."
          example:
          - user.name@databricks.com
          items:
            type: string
          title: on_success
          type: array
        on_failure:
          description: "A list of email addresses to be notified when a run unsuccessfully\
            \ completes. A run is considered to have completed unsuccessfully if it\
            \ ends with an `INTERNAL_ERROR` `life_cycle_state` or a `SKIPPED`, `FAILED`,\
            \ or `TIMED_OUT` result_state. If this is not specified on job creation,\
            \ reset, or update the list is empty, and notifications are not sent."
          example:
          - user.name@databricks.com
          items:
            type: string
          title: on_failure
          type: array
        no_alert_for_skipped_runs:
          description: "If true, do not send email to recipients specified in `on_failure`\
            \ if the run is skipped."
          example: false
          title: no_alert_for_skipped_runs
          type: boolean
      title: JobEmailNotifications
    JobSettings:
      example:
        schedule:
          quartz_cron_expression: 20 30 * * * ?
          timezone_id: Europe/London
          pause_status: PAUSED
        max_concurrent_runs: 10
        name: A multitask job
        format: MULTI_TASK
        email_notifications:
          on_failure:
          - user.name@databricks.com
          no_alert_for_skipped_runs: false
          on_start:
          - user.name@databricks.com
          on_success:
          - user.name@databricks.com
        job_clusters:
        - job_cluster_key: auto_scaling_cluster
          new_cluster:
            spark_version: 7.3.x-scala2.12
            node_type_id: i3.xlarge
            spark_conf:
              spark.speculation: true
            aws_attributes:
              availability: SPOT
              zone_id: us-west-2a
            autoscale:
              min_workers: 2
              max_workers: 16
        timeout_seconds: 86400
        tasks:
        - task_key: Sessionize
          description: Extracts session data from events
          depends_on: []
          existing_cluster_id: 0923-164208-meows279
          spark_jar_task:
            main_class_name: com.databricks.Sessionize
            parameters:
            - --data
            - dbfs:/path/to/data.json
          libraries:
          - jar: dbfs:/mnt/databricks/Sessionize.jar
          timeout_seconds: 86400
          max_retries: 3
          min_retry_interval_millis: 2000
          retry_on_timeout: false
        - task_key: Orders_Ingest
          description: Ingests order data
          depends_on: []
          job_cluster_key: auto_scaling_cluster
          spark_jar_task:
            main_class_name: com.databricks.OrdersIngest
            parameters:
            - --data
            - dbfs:/path/to/order-data.json
          libraries:
          - jar: dbfs:/mnt/databricks/OrderIngest.jar
          timeout_seconds: 86400
          max_retries: 3
          min_retry_interval_millis: 2000
          retry_on_timeout: false
        - task_key: Match
          description: Matches orders with user sessions
          depends_on:
          - task_key: Orders_Ingest
          - task_key: Sessionize
          new_cluster:
            spark_version: 7.3.x-scala2.12
            node_type_id: i3.xlarge
            spark_conf:
              spark.speculation: true
            aws_attributes:
              availability: SPOT
              zone_id: us-west-2a
            autoscale:
              min_workers: 2
              max_workers: 16
          notebook_task:
            notebook_path: /Users/user.name@databricks.com/Match
            base_parameters:
              name: John Doe
              age: "35"
          timeout_seconds: 86400
          max_retries: 3
          min_retry_interval_millis: 2000
          retry_on_timeout: false
        git_source:
          git_commit: e0056d01
          git_tag: release-1.0.0
          git_provider: github
          git_branch: main
          git_url: https://github.com/databricks/databricks-cli
          git_snapshot:
            used_commit: 4506fdf41e9fa98090570a34df7a5bce163ff15f
        tags:
          cost-center: engineering
          team: jobs
      properties:
        name:
          default: Untitled
          description: An optional name for the job.
          example: A multitask job
          title: name
          type: string
        tags:
          description: "A map of tags associated with the job. These are forwarded\
            \ to the cluster as cluster tags for jobs clusters, and are subject to\
            \ the same limitations as cluster tags. A maximum of 25 tags can be added\
            \ to the job."
          example:
            cost-center: engineering
            team: jobs
          title: tags
          type: object
        tasks:
          description: A list of task specifications to be executed by this job.
          example:
          - task_key: Sessionize
            description: Extracts session data from events
            depends_on: []
            existing_cluster_id: 0923-164208-meows279
            spark_jar_task:
              main_class_name: com.databricks.Sessionize
              parameters:
              - --data
              - dbfs:/path/to/data.json
            libraries:
            - jar: dbfs:/mnt/databricks/Sessionize.jar
            timeout_seconds: 86400
            max_retries: 3
            min_retry_interval_millis: 2000
            retry_on_timeout: false
          - task_key: Orders_Ingest
            description: Ingests order data
            depends_on: []
            job_cluster_key: auto_scaling_cluster
            spark_jar_task:
              main_class_name: com.databricks.OrdersIngest
              parameters:
              - --data
              - dbfs:/path/to/order-data.json
            libraries:
            - jar: dbfs:/mnt/databricks/OrderIngest.jar
            timeout_seconds: 86400
            max_retries: 3
            min_retry_interval_millis: 2000
            retry_on_timeout: false
          - task_key: Match
            description: Matches orders with user sessions
            depends_on:
            - task_key: Orders_Ingest
            - task_key: Sessionize
            new_cluster:
              spark_version: 7.3.x-scala2.12
              node_type_id: i3.xlarge
              spark_conf:
                spark.speculation: true
              aws_attributes:
                availability: SPOT
                zone_id: us-west-2a
              autoscale:
                min_workers: 2
                max_workers: 16
            notebook_task:
              notebook_path: /Users/user.name@databricks.com/Match
              base_parameters:
                name: John Doe
                age: "35"
            timeout_seconds: 86400
            max_retries: 3
            min_retry_interval_millis: 2000
            retry_on_timeout: false
          items:
            $ref: '#/components/schemas/JobTaskSettings'
          maxItems: 100
          title: tasks
          type: array
        job_clusters:
          description: A list of job cluster specifications that can be shared and
            reused by tasks of this job. Libraries cannot be declared in a shared
            job cluster. You must declare dependent libraries in task settings.
          example:
          - job_cluster_key: auto_scaling_cluster
            new_cluster:
              spark_version: 7.3.x-scala2.12
              node_type_id: i3.xlarge
              spark_conf:
                spark.speculation: true
              aws_attributes:
                availability: SPOT
                zone_id: us-west-2a
              autoscale:
                min_workers: 2
                max_workers: 16
          items:
            $ref: '#/components/schemas/JobCluster'
          maxItems: 100
          title: job_clusters
          type: array
        email_notifications:
          $ref: '#/components/schemas/JobEmailNotifications'
        timeout_seconds:
          description: An optional timeout applied to each run of this job. The default
            behavior is to have no timeout.
          example: 86400
          format: int32
          title: timeout_seconds
          type: integer
        schedule:
          $ref: '#/components/schemas/CronSchedule'
        max_concurrent_runs:
          description: "An optional maximum allowed number of concurrent runs of the\
            \ job.\n\nSet this value if you want to be able to execute multiple runs\
            \ of the same job concurrently. This is useful for example if you trigger\
            \ your job on a frequent schedule and want to allow consecutive runs to\
            \ overlap with each other, or if you want to trigger multiple runs which\
            \ differ by their input parameters.\n\nThis setting affects only new runs.\
            \ For example, suppose the job’s concurrency is 4 and there are 4 concurrent\
            \ active runs. Then setting the concurrency to 3 won’t kill any of the\
            \ active runs. However, from then on, new runs are skipped unless there\
            \ are fewer than 3 active runs.\n\nThis value cannot exceed 1000\\. Setting\
            \ this value to 0 causes all new runs to be skipped. The default behavior\
            \ is to allow only 1 concurrent run."
          example: 10
          format: int32
          title: max_concurrent_runs
          type: integer
        git_source:
          $ref: '#/components/schemas/GitSource'
        format:
          description: Used to tell what is the format of the job. This field is ignored
            in Create/Update/Reset calls. When using the Jobs API 2.1 this value is
            always set to `"MULTI_TASK"`.
          enum:
          - SINGLE_TASK
          - MULTI_TASK
          example: MULTI_TASK
          title: format
          type: string
      title: JobSettings
    JobTask:
      properties:
        notebook_task:
          $ref: '#/components/schemas/NotebookTask'
        spark_jar_task:
          $ref: '#/components/schemas/SparkJarTask'
        spark_python_task:
          $ref: '#/components/schemas/SparkPythonTask'
        spark_submit_task:
          $ref: '#/components/schemas/SparkSubmitTask'
        pipeline_task:
          $ref: '#/components/schemas/PipelineTask'
        python_wheel_task:
          $ref: '#/components/schemas/PythonWheelTask'
    TaskKey:
      description: "A unique name for the task. This field is used to refer to this\
        \ task from other tasks.\nThis field is required and must be unique within\
        \ its parent job.\nOn Update or Reset, this field is used to reference the\
        \ tasks to be updated or reset.\nThe maximum length is 100 characters."
      example: Task_Key
      maxLength: 100
      minLength: 1
      pattern: "^[\\w\\-]+$"
      title: TaskKey
      type: string
    TaskDependencies:
      description: "An optional array of objects specifying the dependency graph of\
        \ the task. All tasks specified in this field must complete successfully before\
        \ executing this task.\nThe key is `task_key`, and the value is the name assigned\
        \ to the dependent task.\nThis field is required when a job consists of more\
        \ than one task."
      example:
      - task_key: Previous_Task_Key
      - task_key: Other_Task_Key
      items:
        $ref: '#/components/schemas/TaskDependencies_inner'
      title: TaskDependencies
      type: array
    TaskDescription:
      description: |-
        An optional description for this task.
        The maximum length is 4096 bytes.
      example: This is the description for this task.
      maxLength: 4096
      title: TaskDescription
      type: string
    JobTaskSettings:
      properties:
        task_key:
          description: "A unique name for the task. This field is used to refer to\
            \ this task from other tasks.\nThis field is required and must be unique\
            \ within its parent job.\nOn Update or Reset, this field is used to reference\
            \ the tasks to be updated or reset.\nThe maximum length is 100 characters."
          example: Task_Key
          maxLength: 100
          minLength: 1
          pattern: "^[\\w\\-]+$"
          title: TaskKey
          type: string
        description:
          description: |-
            An optional description for this task.
            The maximum length is 4096 bytes.
          example: This is the description for this task.
          maxLength: 4096
          title: TaskDescription
          type: string
        depends_on:
          description: "An optional array of objects specifying the dependency graph\
            \ of the task. All tasks specified in this field must complete successfully\
            \ before executing this task.\nThe key is `task_key`, and the value is\
            \ the name assigned to the dependent task.\nThis field is required when\
            \ a job consists of more than one task."
          example:
          - task_key: Previous_Task_Key
          - task_key: Other_Task_Key
          items:
            $ref: '#/components/schemas/TaskDependencies_inner'
          title: TaskDependencies
          type: array
        existing_cluster_id:
          description: "If existing_cluster_id, the ID of an existing cluster that\
            \ is used for all runs of this task. When running tasks on an existing\
            \ cluster, you may need to manually restart the cluster if it stops responding.\
            \ We suggest running jobs on new clusters for greater reliability."
          example: 0923-164208-meows279
          title: existing_cluster_id
          type: string
        new_cluster:
          $ref: '#/components/schemas/NewCluster'
        job_cluster_key:
          description: "If job_cluster_key, this task is executed reusing the cluster\
            \ specified in `job.settings.job_clusters`."
          maxLength: 100
          minLength: 1
          pattern: "^[\\w\\-]+$"
          title: job_cluster_key
          type: string
        notebook_task:
          $ref: '#/components/schemas/NotebookTask'
        spark_jar_task:
          $ref: '#/components/schemas/SparkJarTask'
        spark_python_task:
          $ref: '#/components/schemas/SparkPythonTask'
        spark_submit_task:
          $ref: '#/components/schemas/SparkSubmitTask'
        pipeline_task:
          $ref: '#/components/schemas/PipelineTask'
        python_wheel_task:
          $ref: '#/components/schemas/PythonWheelTask'
        libraries:
          description: An optional list of libraries to be installed on the cluster
            that executes the task. The default value is an empty list.
          items:
            $ref: '#/components/schemas/Library'
          title: libraries
          type: array
        email_notifications:
          $ref: '#/components/schemas/JobEmailNotifications'
        timeout_seconds:
          description: An optional timeout applied to each run of this job task. The
            default behavior is to have no timeout.
          example: 86400
          format: int32
          title: timeout_seconds
          type: integer
        max_retries:
          description: An optional maximum number of times to retry an unsuccessful
            run. A run is considered to be unsuccessful if it completes with the `FAILED`
            result_state or `INTERNAL_ERROR` `life_cycle_state`. The value -1 means
            to retry indefinitely and the value 0 means to never retry. The default
            behavior is to never retry.
          example: 10
          format: int32
          title: max_retries
          type: integer
        min_retry_interval_millis:
          description: An optional minimal interval in milliseconds between the start
            of the failed run and the subsequent retry run. The default behavior is
            that unsuccessful runs are immediately retried.
          example: 2000
          format: int32
          title: min_retry_interval_millis
          type: integer
        retry_on_timeout:
          description: An optional policy to specify whether to retry a task when
            it times out. The default behavior is to not retry on timeout.
          example: true
          title: retry_on_timeout
          type: boolean
      required:
      - task_key
      title: JobTaskSettings
    JobCluster:
      properties:
        job_cluster_key:
          description: |-
            A unique name for the job cluster. This field is required and must be unique within the job.
            `JobTaskSettings` may refer to this field to determine which cluster to launch for the task execution.
          example: auto_scaling_cluster
          maxLength: 100
          minLength: 1
          pattern: "^[\\w\\-]+$"
          title: job_cluster_key
          type: string
        new_cluster:
          $ref: '#/components/schemas/NewCluster'
      required:
      - job_cluster_key
      title: JobCluster
    NewCluster:
      example:
        spark_conf:
          key: ""
        cluster_log_conf:
          s3:
            encryption_type: encryption_type
            enable_encryption: true
            endpoint: endpoint
            canned_acl: canned_acl
            destination: destination
            region: region
            kms_key: kms_key
          dbfs:
            destination: destination
        enable_elastic_disk: true
        policy_id: policy_id
        spark_env_vars:
          key: ""
        node_type_id: node_type_id
        ssh_public_keys:
        - ssh_public_keys
        - ssh_public_keys
        custom_tags:
          key: custom_tags
        num_workers: 0
        autoscale:
          max_workers: 1
          min_workers: 6
        instance_pool_id: instance_pool_id
        aws_attributes:
          ebs_volume_count: 2
          zone_id: zone_id
          ebs_volume_size: 7
          ebs_volume_throughput: 3
          spot_bid_price_percent: 5
          availability: SPOT
          ebs_volume_type: GENERAL_PURPOSE_SSD
          first_on_demand: 5
          ebs_volume_iops: 9
          instance_profile_arn: instance_profile_arn
        spark_version: spark_version
        driver_node_type_id: driver_node_type_id
        init_scripts:
        - S3:
            encryption_type: encryption_type
            enable_encryption: true
            endpoint: endpoint
            canned_acl: canned_acl
            destination: destination
            region: region
            kms_key: kms_key
          file:
            destination: destination
          dbfs:
            destination: destination
        - S3:
            encryption_type: encryption_type
            enable_encryption: true
            endpoint: endpoint
            canned_acl: canned_acl
            destination: destination
            region: region
            kms_key: kms_key
          file:
            destination: destination
          dbfs:
            destination: destination
        driver_instance_pool_id: driver_instance_pool_id
      properties:
        num_workers:
          description: "If num_workers, number of worker nodes that this cluster must\
            \ have. A cluster has one Spark driver and num_workers executors for a\
            \ total of num_workers + 1 Spark nodes. When reading the properties of\
            \ a cluster, this field reflects the desired number of workers rather\
            \ than the actual current number of workers. For example, if a cluster\
            \ is resized from 5 to 10 workers, this field immediately updates to reflect\
            \ the target size of 10 workers, whereas the workers listed in `spark_info`\
            \ gradually increase from 5 to 10 as the new nodes are provisioned."
          format: int32
          title: num_workers
          type: integer
        autoscale:
          $ref: '#/components/schemas/AutoScale'
        spark_version:
          description: "The Spark version of the cluster. A list of available Spark\
            \ versions can be retrieved by using the [Runtime versions](https://docs.databricks.com/dev-tools/api/latest/clusters.html#runtime-versions)\
            \ API call. This field is required."
          title: spark_version
          type: string
        spark_conf:
          additionalProperties: true
          description: An arbitrary object where the object key is a configuration
            propery name and the value is a configuration property value.
          title: SparkConfPair
          type: object
        aws_attributes:
          $ref: '#/components/schemas/AwsAttributes'
        node_type_id:
          description: "This field encodes, through a single value, the resources\
            \ available to each of the Spark nodes in this cluster. For example, the\
            \ Spark nodes can be provisioned and optimized for memory or compute intensive\
            \ workloads A list of available node types can be retrieved by using the\
            \ [List node types](https://docs.databricks.com/dev-tools/api/latest/clusters.html#list-node-types)\
            \ API call. This field is required."
          title: node_type_id
          type: string
        driver_node_type_id:
          description: "The node type of the Spark driver. This field is optional;\
            \ if unset, the driver node type is set as the same value as `node_type_id`\
            \ defined above."
          title: driver_node_type_id
          type: string
        ssh_public_keys:
          description: SSH public key contents that are added to each Spark node in
            this cluster. The corresponding private keys can be used to login with
            the user name `ubuntu` on port `2200`. Up to 10 keys can be specified.
          items:
            type: string
          title: ssh_public_keys
          type: array
        custom_tags:
          additionalProperties:
            type: string
          description: "An object with key value pairs. The key length must be between\
            \ 1 and 127 UTF-8 characters, inclusive. The value length must be less\
            \ than or equal to 255 UTF-8 characters. For a list of all restrictions,\
            \ see AWS Tag Restrictions: <https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html#tag-restrictions>"
          title: ClusterTag
          type: object
        cluster_log_conf:
          $ref: '#/components/schemas/ClusterLogConf'
        init_scripts:
          description: "The configuration for storing init scripts. Any number of\
            \ scripts can be specified. The scripts are executed sequentially in the\
            \ order provided. If `cluster_log_conf` is specified, init script logs\
            \ are sent to `<destination>/<cluster-id>/init_scripts`."
          items:
            $ref: '#/components/schemas/InitScriptInfo'
          title: init_scripts
          type: array
        spark_env_vars:
          additionalProperties: true
          description: An arbitrary object where the object key is an environment
            variable name and the value is an environment variable value.
          title: SparkEnvPair
          type: object
        enable_elastic_disk:
          description: "Autoscaling Local Storage: when enabled, this cluster dynamically\
            \ acquires additional disk space when its Spark workers are running low\
            \ on disk space. This feature requires specific AWS permissions to function\
            \ correctly - refer to [Autoscaling local storage](https://docs.databricks.com/clusters/configure.html#autoscaling-local-storage)\
            \ for details."
          title: enable_elastic_disk
          type: boolean
        driver_instance_pool_id:
          description: "The optional ID of the instance pool to use for the driver\
            \ node. You must also specify `instance_pool_id`. Refer to [Instance Pools\
            \ API](https://docs.databricks.com/dev-tools/api/latest/instance-pools.html)\
            \ for details."
          title: driver_instance_pool_id
          type: string
        instance_pool_id:
          description: "The optional ID of the instance pool to use for cluster nodes.\
            \ If `driver_instance_pool_id` is present, `instance_pool_id` is used\
            \ for worker nodes only. Otherwise, it is used for both the driver node\
            \ and worker nodes. Refer to [Instance Pools API](https://docs.databricks.com/dev-tools/api/latest/instance-pools.html)\
            \ for details."
          title: instance_pool_id
          type: string
        policy_id:
          description: "A [cluster policy](https://docs.databricks.com/dev-tools/api/latest/policies.html)\
            \ ID."
          title: policy_id
          type: string
      required:
      - node_type_id
      - spark_version
      title: NewCluster
    NotebookOutput:
      example:
        result: An arbitrary string passed by calling dbutils.notebook.exit(...)
        truncated: false
      properties:
        result:
          description: "The value passed to [dbutils.notebook.exit()](https://docs.databricks.com/notebooks/notebook-workflows.html#notebook-workflows-exit).\
            \ Databricks restricts this API to return the first 5 MB of the value.\
            \ For a larger result, your job can store the results in a cloud storage\
            \ service. This field is absent if `dbutils.notebook.exit()` was never\
            \ called."
          example: An arbitrary string passed by calling dbutils.notebook.exit(...)
          title: result
          type: string
        truncated:
          description: Whether or not the result was truncated.
          example: false
          title: truncated
          type: boolean
      title: NotebookOutput
    NotebookTask:
      properties:
        notebook_path:
          description: "The path of the notebook to be run in the Databricks workspace\
            \ or remote repository. For notebooks stored in the Databricks workspace,\
            \ the path must be absolute and begin with a slash. For notebooks stored\
            \ in a remote repository, the path must be relative. This field is required."
          example: /Users/user.name@databricks.com/notebook_to_run
          title: notebook_path
          type: string
        base_parameters:
          additionalProperties: true
          description: "Base parameters to be used for each run of this job. If the\
            \ run is initiated by a call to [`run-now`](https://docs.databricks.com/dev-tools/api/latest/jobs.html#operation/JobsRunNow)\
            \ with parameters specified, the two parameters maps are merged. If the\
            \ same key is specified in `base_parameters` and in `run-now`, the value\
            \ from `run-now` is used.\n\nUse [Task parameter variables](https://docs.databricks.com/jobs.html#parameter-variables)\
            \ to set parameters containing information about job runs.\n\nIf the notebook\
            \ takes a parameter that is not specified in the job’s `base_parameters`\
            \ or the `run-now` override parameters, the default value from the notebook\
            \ is used.\n\nRetrieve these parameters in a notebook using [dbutils.widgets.get](https://docs.databricks.com/dev-tools/databricks-utils.html#dbutils-widgets)."
          example:
            name: John Doe
            age: 35
          title: base_parameters
          type: object
      required:
      - notebook_path
      title: NotebookTask
    RunTask:
      properties:
        run_id:
          description: The ID of the task run.
          example: 99887766
          format: int64
          title: run_id
          type: integer
        task_key:
          description: "A unique name for the task. This field is used to refer to\
            \ this task from other tasks.\nThis field is required and must be unique\
            \ within its parent job.\nOn Update or Reset, this field is used to reference\
            \ the tasks to be updated or reset.\nThe maximum length is 100 characters."
          example: Task_Key
          maxLength: 100
          minLength: 1
          pattern: "^[\\w\\-]+$"
          title: TaskKey
          type: string
        description:
          description: |-
            An optional description for this task.
            The maximum length is 4096 bytes.
          example: This is the description for this task.
          maxLength: 4096
          title: TaskDescription
          type: string
        state:
          $ref: '#/components/schemas/RunState'
        depends_on:
          description: "An optional array of objects specifying the dependency graph\
            \ of the task. All tasks specified in this field must complete successfully\
            \ before executing this task.\nThe key is `task_key`, and the value is\
            \ the name assigned to the dependent task.\nThis field is required when\
            \ a job consists of more than one task."
          example:
          - task_key: Previous_Task_Key
          - task_key: Other_Task_Key
          items:
            $ref: '#/components/schemas/TaskDependencies_inner'
          title: TaskDependencies
          type: array
        existing_cluster_id:
          description: "If existing_cluster_id, the ID of an existing cluster that\
            \ is used for all runs of this job. When running jobs on an existing cluster,\
            \ you may need to manually restart the cluster if it stops responding.\
            \ We suggest running jobs on new clusters for greater reliability."
          title: existing_cluster_id
          type: string
        new_cluster:
          $ref: '#/components/schemas/NewCluster'
        libraries:
          description: An optional list of libraries to be installed on the cluster
            that executes the job. The default value is an empty list.
          items:
            $ref: '#/components/schemas/Library'
          title: libraries
          type: array
        notebook_task:
          $ref: '#/components/schemas/NotebookTask'
        spark_jar_task:
          $ref: '#/components/schemas/SparkJarTask'
        spark_python_task:
          $ref: '#/components/schemas/SparkPythonTask'
        spark_submit_task:
          $ref: '#/components/schemas/SparkSubmitTask'
        pipeline_task:
          $ref: '#/components/schemas/PipelineTask'
        python_wheel_task:
          $ref: '#/components/schemas/PythonWheelTask'
        start_time:
          description: "The time at which this run was started in epoch milliseconds\
            \ (milliseconds since 1/1/1970 UTC). This may not be the time when the\
            \ job task starts executing, for example, if the job is scheduled to run\
            \ on a new cluster, this is the time the cluster creation call is issued."
          example: 1625060460483
          format: int64
          title: start_time
          type: integer
        setup_duration:
          description: "The time it took to set up the cluster in milliseconds. For\
            \ runs that run on new clusters this is the cluster creation time, for\
            \ runs that run on existing clusters this time should be very short."
          example: 0
          format: int64
          title: setup_duration
          type: integer
        execution_duration:
          description: "The time in milliseconds it took to execute the commands in\
            \ the JAR or notebook until they completed, failed, timed out, were cancelled,\
            \ or encountered an unexpected error."
          example: 0
          format: int64
          title: execution_duration
          type: integer
        cleanup_duration:
          description: "The time in milliseconds it took to terminate the cluster\
            \ and clean up any associated artifacts. The total duration of the run\
            \ is the sum of the setup_duration, the execution_duration, and the cleanup_duration."
          example: 0
          format: int64
          title: cleanup_duration
          type: integer
        end_time:
          description: The time at which this run ended in epoch milliseconds (milliseconds
            since 1/1/1970 UTC). This field is set to 0 if the job is still running.
          example: 1625060863413
          format: int64
          title: end_time
          type: integer
        attempt_number:
          description: "The sequence number of this run attempt for a triggered job\
            \ run. The initial attempt of a run has an attempt_number of 0\\. If the\
            \ initial run attempt fails, and the job has a retry policy (`max_retries`\
            \ \\> 0), subsequent runs are created with an `original_attempt_run_id`\
            \ of the original attempt’s ID and an incrementing `attempt_number`. Runs\
            \ are retried only until they succeed, and the maximum `attempt_number`\
            \ is the same as the `max_retries` value for the job."
          example: 0
          format: int32
          title: attempt_number
          type: integer
        cluster_instance:
          $ref: '#/components/schemas/ClusterInstance'
        git_source:
          $ref: '#/components/schemas/GitSource'
      title: RunTask
    Run:
      example:
        number_in_job: 455644833
        run_id: 455644833
        original_attempt_run_id: 455644833
        cluster_instance:
          cluster_id: 0923-164208-meows279
          spark_context_id: spark_context_id
        run_page_url: https://my-workspace.cloud.databricks.com/#job/11223344/run/123
        end_time: 1625060863413
        cleanup_duration: 0
        trigger: null
        run_type: JOB_RUN
        git_source:
          git_commit: e0056d01
          git_tag: release-1.0.0
          git_provider: github
          git_branch: main
          git_url: https://github.com/databricks/databricks-cli
          git_snapshot:
            used_commit: 4506fdf41e9fa98090570a34df7a5bce163ff15f
        schedule:
          quartz_cron_expression: 20 30 * * * ?
          timezone_id: Europe/London
          pause_status: PAUSED
        start_time: 1625060460483
        cluster_spec:
          new_cluster:
            spark_conf:
              key: ""
            cluster_log_conf:
              s3:
                encryption_type: encryption_type
                enable_encryption: true
                endpoint: endpoint
                canned_acl: canned_acl
                destination: destination
                region: region
                kms_key: kms_key
              dbfs:
                destination: destination
            enable_elastic_disk: true
            policy_id: policy_id
            spark_env_vars:
              key: ""
            node_type_id: node_type_id
            ssh_public_keys:
            - ssh_public_keys
            - ssh_public_keys
            custom_tags:
              key: custom_tags
            num_workers: 0
            autoscale:
              max_workers: 1
              min_workers: 6
            instance_pool_id: instance_pool_id
            aws_attributes:
              ebs_volume_count: 2
              zone_id: zone_id
              ebs_volume_size: 7
              ebs_volume_throughput: 3
              spot_bid_price_percent: 5
              availability: SPOT
              ebs_volume_type: GENERAL_PURPOSE_SSD
              first_on_demand: 5
              ebs_volume_iops: 9
              instance_profile_arn: instance_profile_arn
            spark_version: spark_version
            driver_node_type_id: driver_node_type_id
            init_scripts:
            - S3:
                encryption_type: encryption_type
                enable_encryption: true
                endpoint: endpoint
                canned_acl: canned_acl
                destination: destination
                region: region
                kms_key: kms_key
              file:
                destination: destination
              dbfs:
                destination: destination
            - S3:
                encryption_type: encryption_type
                enable_encryption: true
                endpoint: endpoint
                canned_acl: canned_acl
                destination: destination
                region: region
                kms_key: kms_key
              file:
                destination: destination
              dbfs:
                destination: destination
            driver_instance_pool_id: driver_instance_pool_id
          libraries:
          - cran:
              package: geojson
              repo: https://my-repo.com
            egg: dbfs:/my/egg
            pypi:
              package: simplejson==3.8.0
              repo: https://my-repo.com
            maven:
              repo: https://my-repo.com
              coordinates: org.jsoup:jsoup:1.7.2
              exclusions:
              - slf4j:slf4j
              - '*:hadoop-client'
            jar: dbfs:/my-jar.jar
            whl: dbfs:/my/whl
          - cran:
              package: geojson
              repo: https://my-repo.com
            egg: dbfs:/my/egg
            pypi:
              package: simplejson==3.8.0
              repo: https://my-repo.com
            maven:
              repo: https://my-repo.com
              coordinates: org.jsoup:jsoup:1.7.2
              exclusions:
              - slf4j:slf4j
              - '*:hadoop-client'
            jar: dbfs:/my-jar.jar
            whl: dbfs:/my/whl
          existing_cluster_id: 0923-164208-meows279
        attempt_number: 0
        creator_user_name: user.name@databricks.com
        setup_duration: 0
        execution_duration: 0
        job_id: 11223344
        job_clusters:
        - job_cluster_key: auto_scaling_cluster
          new_cluster:
            spark_version: 7.3.x-scala2.12
            node_type_id: i3.xlarge
            spark_conf:
              spark.speculation: true
            aws_attributes:
              availability: SPOT
              zone_id: us-west-2a
            autoscale:
              min_workers: 2
              max_workers: 16
        state:
          user_cancelled_or_timedout: false
          life_cycle_state: null
          result_state: null
          state_message: ""
        overriding_parameters:
          python_params:
          - john doe
          - "35"
          python_named_params:
            name: task
            data: dbfs:/path/to/data.json
          notebook_params:
            name: john doe
            age: "35"
          jar_params:
          - john
          - doe
          - "35"
          spark_submit_params:
          - --class
          - org.apache.spark.examples.SparkPi
        run_name: A multitask job run
        tasks:
        - run_id: 2112892
          task_key: Orders_Ingest
          description: Ingests order data
          job_cluster_key: auto_scaling_cluster
          spark_jar_task:
            main_class_name: com.databricks.OrdersIngest
            run_as_repl: true
          libraries:
          - jar: dbfs:/mnt/databricks/OrderIngest.jar
          state:
            life_cycle_state: INTERNAL_ERROR
            result_state: FAILED
            state_message: |-
              Library installation failed for library due to user error. Error messages:
              'Manage' permissions are required to install libraries on a cluster
            user_cancelled_or_timedout: false
          run_page_url: https://my-workspace.cloud.databricks.com/#job/39832/run/20
          start_time: 1629989929660
          setup_duration: 0
          execution_duration: 0
          cleanup_duration: 0
          end_time: 1629989930171
          cluster_instance:
            cluster_id: 0923-164208-meows279
            spark_context_id: "4348585301701786933"
          attempt_number: 0
        - run_id: 2112897
          task_key: Match
          description: Matches orders with user sessions
          depends_on:
          - task_key: Orders_Ingest
          - task_key: Sessionize
          new_cluster:
            spark_version: 7.3.x-scala2.12
            node_type_id: i3.xlarge
            spark_conf:
              spark.speculation: true
            aws_attributes:
              availability: SPOT
              zone_id: us-west-2a
            autoscale:
              min_workers: 2
              max_workers: 16
          notebook_task:
            notebook_path: /Users/user.name@databricks.com/Match
          state:
            life_cycle_state: SKIPPED
            state_message: An upstream task failed.
            user_cancelled_or_timedout: false
          run_page_url: https://my-workspace.cloud.databricks.com/#job/39832/run/21
          start_time: 0
          setup_duration: 0
          execution_duration: 0
          cleanup_duration: 0
          end_time: 1629989930238
          cluster_instance:
            cluster_id: 0923-164208-meows279
          attempt_number: 0
        - run_id: 2112902
          task_key: Sessionize
          description: Extracts session data from events
          existing_cluster_id: 0923-164208-meows279
          spark_jar_task:
            main_class_name: com.databricks.Sessionize
            run_as_repl: true
          libraries:
          - jar: dbfs:/mnt/databricks/Sessionize.jar
          state:
            life_cycle_state: INTERNAL_ERROR
            result_state: FAILED
            state_message: |-
              Library installation failed for library due to user error. Error messages:
              'Manage' permissions are required to install libraries on a cluster
            user_cancelled_or_timedout: false
          run_page_url: https://my-workspace.cloud.databricks.com/#job/39832/run/22
          start_time: 1629989929668
          setup_duration: 0
          execution_duration: 0
          cleanup_duration: 0
          end_time: 1629989930144
          cluster_instance:
            cluster_id: 0923-164208-meows279
            spark_context_id: "4348585301701786933"
          attempt_number: 0
      properties:
        job_id:
          description: The canonical identifier of the job that contains this run.
          example: 11223344
          format: int64
          title: job_id
          type: integer
        run_id:
          description: The canonical identifier of the run. This ID is unique across
            all runs of all jobs.
          example: 455644833
          format: int64
          title: run_id
          type: integer
        number_in_job:
          deprecated: true
          description: A unique identifier for this job run. This is set to the same
            value as `run_id`.
          example: 455644833
          format: int64
          title: number_in_job
          type: integer
        creator_user_name:
          description: The creator user name. This field won’t be included in the
            response if the user has already been deleted.
          example: user.name@databricks.com
          title: creator_user_name
          type: string
        original_attempt_run_id:
          description: "If this run is a retry of a prior run attempt, this field\
            \ contains the run_id of the original attempt; otherwise, it is the same\
            \ as the run_id."
          example: 455644833
          format: int64
          title: original_attempt_run_id
          type: integer
        state:
          $ref: '#/components/schemas/RunState'
        schedule:
          $ref: '#/components/schemas/CronSchedule'
        tasks:
          description: The list of tasks performed by the run. Each task has its own
            `run_id` which you can use to call `JobsGetOutput` to retrieve the run
            resutls.
          example:
          - run_id: 2112892
            task_key: Orders_Ingest
            description: Ingests order data
            job_cluster_key: auto_scaling_cluster
            spark_jar_task:
              main_class_name: com.databricks.OrdersIngest
              run_as_repl: true
            libraries:
            - jar: dbfs:/mnt/databricks/OrderIngest.jar
            state:
              life_cycle_state: INTERNAL_ERROR
              result_state: FAILED
              state_message: |-
                Library installation failed for library due to user error. Error messages:
                'Manage' permissions are required to install libraries on a cluster
              user_cancelled_or_timedout: false
            run_page_url: https://my-workspace.cloud.databricks.com/#job/39832/run/20
            start_time: 1629989929660
            setup_duration: 0
            execution_duration: 0
            cleanup_duration: 0
            end_time: 1629989930171
            cluster_instance:
              cluster_id: 0923-164208-meows279
              spark_context_id: "4348585301701786933"
            attempt_number: 0
          - run_id: 2112897
            task_key: Match
            description: Matches orders with user sessions
            depends_on:
            - task_key: Orders_Ingest
            - task_key: Sessionize
            new_cluster:
              spark_version: 7.3.x-scala2.12
              node_type_id: i3.xlarge
              spark_conf:
                spark.speculation: true
              aws_attributes:
                availability: SPOT
                zone_id: us-west-2a
              autoscale:
                min_workers: 2
                max_workers: 16
            notebook_task:
              notebook_path: /Users/user.name@databricks.com/Match
            state:
              life_cycle_state: SKIPPED
              state_message: An upstream task failed.
              user_cancelled_or_timedout: false
            run_page_url: https://my-workspace.cloud.databricks.com/#job/39832/run/21
            start_time: 0
            setup_duration: 0
            execution_duration: 0
            cleanup_duration: 0
            end_time: 1629989930238
            cluster_instance:
              cluster_id: 0923-164208-meows279
            attempt_number: 0
          - run_id: 2112902
            task_key: Sessionize
            description: Extracts session data from events
            existing_cluster_id: 0923-164208-meows279
            spark_jar_task:
              main_class_name: com.databricks.Sessionize
              run_as_repl: true
            libraries:
            - jar: dbfs:/mnt/databricks/Sessionize.jar
            state:
              life_cycle_state: INTERNAL_ERROR
              result_state: FAILED
              state_message: |-
                Library installation failed for library due to user error. Error messages:
                'Manage' permissions are required to install libraries on a cluster
              user_cancelled_or_timedout: false
            run_page_url: https://my-workspace.cloud.databricks.com/#job/39832/run/22
            start_time: 1629989929668
            setup_duration: 0
            execution_duration: 0
            cleanup_duration: 0
            end_time: 1629989930144
            cluster_instance:
              cluster_id: 0923-164208-meows279
              spark_context_id: "4348585301701786933"
            attempt_number: 0
          items:
            $ref: '#/components/schemas/RunTask'
          maxItems: 100
          title: tasks
          type: array
        job_clusters:
          description: A list of job cluster specifications that can be shared and
            reused by tasks of this job. Libraries cannot be declared in a shared
            job cluster. You must declare dependent libraries in task settings.
          example:
          - job_cluster_key: auto_scaling_cluster
            new_cluster:
              spark_version: 7.3.x-scala2.12
              node_type_id: i3.xlarge
              spark_conf:
                spark.speculation: true
              aws_attributes:
                availability: SPOT
                zone_id: us-west-2a
              autoscale:
                min_workers: 2
                max_workers: 16
          items:
            $ref: '#/components/schemas/JobCluster'
          maxItems: 100
          title: job_clusters
          type: array
        cluster_spec:
          $ref: '#/components/schemas/ClusterSpec'
        cluster_instance:
          $ref: '#/components/schemas/ClusterInstance'
        git_source:
          $ref: '#/components/schemas/GitSource'
        overriding_parameters:
          $ref: '#/components/schemas/RunParameters'
        start_time:
          description: "The time at which this run was started in epoch milliseconds\
            \ (milliseconds since 1/1/1970 UTC). This may not be the time when the\
            \ job task starts executing, for example, if the job is scheduled to run\
            \ on a new cluster, this is the time the cluster creation call is issued."
          example: 1625060460483
          format: int64
          title: start_time
          type: integer
        setup_duration:
          description: "The time it took to set up the cluster in milliseconds. For\
            \ runs that run on new clusters this is the cluster creation time, for\
            \ runs that run on existing clusters this time should be very short."
          example: 0
          format: int64
          title: setup_duration
          type: integer
        execution_duration:
          description: "The time in milliseconds it took to execute the commands in\
            \ the JAR or notebook until they completed, failed, timed out, were cancelled,\
            \ or encountered an unexpected error."
          example: 0
          format: int64
          title: execution_duration
          type: integer
        cleanup_duration:
          description: "The time in milliseconds it took to terminate the cluster\
            \ and clean up any associated artifacts. The total duration of the run\
            \ is the sum of the setup_duration, the execution_duration, and the cleanup_duration."
          example: 0
          format: int64
          title: cleanup_duration
          type: integer
        end_time:
          description: The time at which this run ended in epoch milliseconds (milliseconds
            since 1/1/1970 UTC). This field is set to 0 if the job is still running.
          example: 1625060863413
          format: int64
          title: end_time
          type: integer
        trigger:
          $ref: '#/components/schemas/TriggerType'
        run_name:
          default: Untitled
          description: An optional name for the run. The maximum allowed length is
            4096 bytes in UTF-8 encoding.
          example: A multitask job run
          title: run_name
          type: string
        run_page_url:
          description: The URL to the detail page of the run.
          example: https://my-workspace.cloud.databricks.com/#job/11223344/run/123
          title: run_page_url
          type: string
        run_type:
          $ref: '#/components/schemas/RunType'
        attempt_number:
          description: "The sequence number of this run attempt for a triggered job\
            \ run. The initial attempt of a run has an attempt_number of 0\\. If the\
            \ initial run attempt fails, and the job has a retry policy (`max_retries`\
            \ \\> 0), subsequent runs are created with an `original_attempt_run_id`\
            \ of the original attempt’s ID and an incrementing `attempt_number`. Runs\
            \ are retried only until they succeed, and the maximum `attempt_number`\
            \ is the same as the `max_retries` value for the job."
          example: 0
          format: int32
          title: attempt_number
          type: integer
      title: Run
    RunType:
      description: "The type of the run.\n* `JOB_RUN` \\- Normal job run. A run created\
        \ with [Run now](https://docs.databricks.com/dev-tools/api/latest/jobs.html#operation/JobsRunNow).\
        \  \n* `WORKFLOW_RUN` \\- Workflow run. A run created with [dbutils.notebook.run](https://docs.databricks.com/dev-tools/databricks-utils.html#dbutils-workflow).\n\
        * `SUBMIT_RUN` \\- Submit run. A run created with [Run now](https://docs.databricks.com/dev-tools/api/latest/jobs.html#operation/JobsRunNow)."
      enum:
      - JOB_RUN
      - WORKFLOW_RUN
      - SUBMIT_RUN
      example: JOB_RUN
      title: RunType
      type: string
    RunParameters:
      example:
        python_params:
        - john doe
        - "35"
        python_named_params:
          name: task
          data: dbfs:/path/to/data.json
        notebook_params:
          name: john doe
          age: "35"
        jar_params:
        - john
        - doe
        - "35"
        spark_submit_params:
        - --class
        - org.apache.spark.examples.SparkPi
      properties:
        jar_params:
          description: "A list of parameters for jobs with Spark JAR tasks, for example\
            \ `\"jar_params\": [\"john doe\", \"35\"]`. The parameters are used to\
            \ invoke the main function of the main class specified in the Spark JAR\
            \ task. If not specified upon `run-now`, it defaults to an empty list.\
            \ jar_params cannot be specified in conjunction with notebook_params.\
            \ The JSON representation of this field (for example `{\"jar_params\"\
            :[\"john doe\",\"35\"]}`) cannot exceed 10,000 bytes.\n\nUse [Task parameter\
            \ variables](https://docs.databricks.com/jobs.html#parameter-variables)\
            \ to set parameters containing information about job runs."
          example:
          - john
          - doe
          - "35"
          items:
            type: string
          title: jar_params
          type: array
        notebook_params:
          additionalProperties: true
          description: "A map from keys to values for jobs with notebook task, for\
            \ example `\"notebook_params\": {\"name\": \"john doe\", \"age\": \"35\"\
            }`. The map is passed to the notebook and is accessible through the [dbutils.widgets.get](https://docs.databricks.com/dev-tools/databricks-utils.html#dbutils-widgets)\
            \ function.\n\nIf not specified upon `run-now`, the triggered run uses\
            \ the job’s base parameters.\n\nnotebook_params cannot be specified in\
            \ conjunction with jar_params.\n\nUse [Task parameter variables](https://docs.databricks.com/jobs.html#parameter-variables)\
            \ to set parameters containing information about job runs.\n\nThe JSON\
            \ representation of this field (for example `{\"notebook_params\":{\"\
            name\":\"john doe\",\"age\":\"35\"}}`) cannot exceed 10,000 bytes."
          example:
            name: john doe
            age: "35"
          title: notebook_params
          type: object
        python_params:
          description: "A list of parameters for jobs with Python tasks, for example\
            \ `\"python_params\": [\"john doe\", \"35\"]`. The parameters are passed\
            \ to Python file as command-line parameters. If specified upon `run-now`,\
            \ it would overwrite the parameters specified in job setting. The JSON\
            \ representation of this field (for example `{\"python_params\":[\"john\
            \ doe\",\"35\"]}`) cannot exceed 10,000 bytes.\n\nUse [Task parameter\
            \ variables](https://docs.databricks.com/jobs.html#parameter-variables)\
            \ to set parameters containing information about job runs.\n\nImportant\n\
            \nThese parameters accept only Latin characters (ASCII character set).\
            \ Using non-ASCII characters returns an error. Examples of invalid, non-ASCII\
            \ characters are Chinese, Japanese kanjis, and emojis."
          example:
          - john doe
          - "35"
          items:
            type: string
          title: python_params
          type: array
        spark_submit_params:
          description: "A list of parameters for jobs with spark submit task, for\
            \ example `\"spark_submit_params\": [\"--class\", \"org.apache.spark.examples.SparkPi\"\
            ]`. The parameters are passed to spark-submit script as command-line parameters.\
            \ If specified upon `run-now`, it would overwrite the parameters specified\
            \ in job setting. The JSON representation of this field (for example `{\"\
            python_params\":[\"john doe\",\"35\"]}`) cannot exceed 10,000 bytes.\n\
            \nUse [Task parameter variables](https://docs.databricks.com/jobs.html#parameter-variables)\
            \ to set parameters containing information about job runs.\n\nImportant\n\
            \nThese parameters accept only Latin characters (ASCII character set).\
            \ Using non-ASCII characters returns an error. Examples of invalid, non-ASCII\
            \ characters are Chinese, Japanese kanjis, and emojis."
          example:
          - --class
          - org.apache.spark.examples.SparkPi
          items:
            type: string
          title: spark_submit_params
          type: array
        python_named_params:
          description: "A map from keys to values for jobs with Python wheel task,\
            \ for example `\"python_named_params\": {\"name\": \"task\", \"data\"\
            : \"dbfs:/path/to/data.json\"}`."
          example:
            name: task
            data: dbfs:/path/to/data.json
          title: python_named_params
          type: object
      title: RunParameters
    RunState:
      description: The result and lifecycle state of the run.
      example:
        user_cancelled_or_timedout: false
        life_cycle_state: null
        result_state: null
        state_message: ""
      properties:
        life_cycle_state:
          $ref: '#/components/schemas/RunLifeCycleState'
        result_state:
          $ref: '#/components/schemas/RunResultState'
        user_cancelled_or_timedout:
          description: Whether a run was canceled manually by a user or by the scheduler
            because the run timed out.
          example: false
          title: user_cancelled_or_timedout
          type: boolean
        state_message:
          description: "A descriptive message for the current state. This field is\
            \ unstructured, and its exact format is subject to change."
          example: ""
          title: state_message
          type: string
      title: RunState
    SparkJarTask:
      properties:
        main_class_name:
          description: "The full name of the class containing the main method to be\
            \ executed. This class must be contained in a JAR provided as a library.\n\
            \nThe code must use `SparkContext.getOrCreate` to obtain a Spark context;\
            \ otherwise, runs of the job fail."
          example: com.databricks.ComputeModels
          title: main_class_name
          type: string
        parameters:
          description: "Parameters passed to the main method.\n\nUse [Task parameter\
            \ variables](https://docs.databricks.com/jobs.html#parameter-variables)\
            \ to set parameters containing information about job runs."
          example:
          - --data
          - dbfs:/path/to/data.json
          items:
            type: string
          title: parameters
          type: array
        jar_uri:
          deprecated: true
          description: "Deprecated since 04/2016\\. Provide a `jar` through the `libraries`\
            \ field instead. For an example, see [Create](https://docs.databricks.com/dev-tools/api/latest/jobs.html#operation/JobsCreate)."
          title: jar_uri
          type: string
      title: SparkJarTask
    SparkPythonTask:
      properties:
        python_file:
          description: The URI of the Python file to be executed. DBFS and S3 paths
            are supported. This field is required.
          example: dbfs:/path/to/file.py
          title: python_file
          type: string
        parameters:
          description: "Command line parameters passed to the Python file.\n\nUse\
            \ [Task parameter variables](https://docs.databricks.com/jobs.html#parameter-variables)\
            \ to set parameters containing information about job runs."
          example:
          - --data
          - dbfs:/path/to/data.json
          items:
            type: string
          title: parameters
          type: array
      required:
      - python_file
      title: SparkPythonTask
    SparkSubmitTask:
      properties:
        parameters:
          description: "Command-line parameters passed to spark submit.\n\nUse [Task\
            \ parameter variables](https://docs.databricks.com/jobs.html#parameter-variables)\
            \ to set parameters containing information about job runs."
          example:
          - --class
          - org.apache.spark.examples.SparkPi
          - dbfs:/path/to/examples.jar
          - "10"
          items:
            type: string
          title: parameters
          type: array
      title: SparkSubmitTask
    PipelineTask:
      properties:
        pipeline_id:
          description: The full name of the pipeline task to execute.
          example: a12cd3e4-0ab1-1abc-1a2b-1a2bcd3e4fg5
          title: pipeline_id
          type: string
      title: PipelineTask
    PythonWheelTask:
      properties:
        package_name:
          description: Name of the package to execute
          title: package_name
          type: string
        entry_point:
          description: "Named entry point to use, if it does not exist in the metadata\
            \ of the package it executes the function from the package directly using\
            \ `$packageName.$entryPoint()`"
          title: entry_point
          type: string
        parameters:
          description: Command-line parameters passed to Python wheel task. Leave
            it empty if `named_parameters` is not null.
          example:
          - --name=task
          - one
          - two
          items:
            type: string
          title: parameters
          type: array
        named_parameters:
          description: "Command-line parameters passed to Python wheel task in the\
            \ form of `[\"--name=task\", \"--data=dbfs:/path/to/data.json\"]`. Leave\
            \ it empty if `parameters` is not null."
          example:
            name: task
            data: dbfs:/path/to/data.json
          title: named_parameters
          type: object
      title: PythonWheelTask
    ViewItem:
      example:
        name: name
        type: null
        content: content
      properties:
        content:
          description: Content of the view.
          title: content
          type: string
        name:
          description: "Name of the view item. In the case of code view, it would\
            \ be the notebook’s name. In the case of dashboard view, it would be the\
            \ dashboard’s name."
          title: name
          type: string
        type:
          $ref: '#/components/schemas/ViewType'
      title: ViewItem
    RunLifeCycleState:
      description: "* `PENDING`: The run has been triggered. If there is not already\
        \ an active run of the same job, the cluster and execution context are being\
        \ prepared. If there is already an active run of the same job, the run immediately\
        \ transitions into the `SKIPPED` state without preparing any resources.\n\
        * `RUNNING`: The task of this run is being executed.\n* `TERMINATING`: The\
        \ task of this run has completed, and the cluster and execution context are\
        \ being cleaned up.\n* `TERMINATED`: The task of this run has completed, and\
        \ the cluster and execution context have been cleaned up. This state is terminal.\n\
        * `SKIPPED`: This run was aborted because a previous run of the same job was\
        \ already active. This state is terminal.\n* `INTERNAL_ERROR`: An exceptional\
        \ state that indicates a failure in the Jobs service, such as network failure\
        \ over a long period. If a run on a new cluster ends in the `INTERNAL_ERROR`\
        \ state, the Jobs service terminates the cluster as soon as possible. This\
        \ state is terminal."
      enum:
      - PENDING
      - RUNNING
      - TERMINATING
      - TERMINATED
      - SKIPPED
      - INTERNAL_ERROR
      title: RunLifeCycleState
      type: string
    RunResultState:
      description: "* `SUCCESS`: The task completed successfully.\n* `FAILED`: The\
        \ task completed with an error.\n* `TIMEDOUT`: The run was stopped after reaching\
        \ the timeout.\n* `CANCELED`: The run was canceled at user request."
      enum:
      - SUCCESS
      - FAILED
      - TIMEDOUT
      - CANCELED
      title: RunResultState
      type: string
    TriggerType:
      description: "* `PERIODIC`: Schedules that periodically trigger runs, such as\
        \ a cron scheduler.\n* `ONE_TIME`: One time triggers that fire a single run.\
        \ This occurs you triggered a single run on demand through the UI or the API.\n\
        * `RETRY`: Indicates a run that is triggered as a retry of a previously failed\
        \ run. This occurs when you request to re-run the job in case of failures."
      enum:
      - PERIODIC
      - ONE_TIME
      - RETRY
      title: TriggerType
      type: string
    ViewType:
      description: "* `NOTEBOOK`: Notebook view item.\n* `DASHBOARD`: Dashboard view\
        \ item."
      enum:
      - NOTEBOOK
      - DASHBOARD
      title: ViewType
      type: string
    ViewsToExport:
      default: CODE
      description: "* `CODE`: Code view of the notebook.\n* `DASHBOARDS`: All dashboard\
        \ views of the notebook.\n* `ALL`: All views of the notebook."
      enum:
      - CODE
      - DASHBOARDS
      - ALL
      title: ViewsToExport
      type: string
    AutoScale:
      example:
        max_workers: 1
        min_workers: 6
      properties:
        min_workers:
          description: The minimum number of workers to which the cluster can scale
            down when underutilized. It is also the initial number of workers the
            cluster has after creation.
          format: int32
          title: min_workers
          type: integer
        max_workers:
          description: The maximum number of workers to which the cluster can scale
            up when overloaded. max_workers must be strictly greater than min_workers.
          format: int32
          title: max_workers
          type: integer
      title: AutoScale
    ClusterInfo:
      properties:
        num_workers:
          description: "If num_workers, number of worker nodes that this cluster must\
            \ have. A cluster has one Spark driver and num_workers executors for a\
            \ total of num_workers + 1 Spark nodes. **Note:** When reading the properties\
            \ of a cluster, this field reflects the desired number of workers rather\
            \ than the actual number of workers. For instance, if a cluster is resized\
            \ from 5 to 10 workers, this field is immediately updated to reflect the\
            \ target size of 10 workers, whereas the workers listed in `executors`\
            \ gradually increase from 5 to 10 as the new nodes are provisioned."
          format: int32
          type: integer
        autoscale:
          $ref: '#/components/schemas/AutoScale'
        cluster_id:
          description: "Canonical identifier for the cluster. This ID is retained\
            \ during cluster restarts and resizes, while each new cluster has a globally\
            \ unique ID."
          type: string
        creator_user_name:
          description: Creator user name. The field won’t be included in the response
            if the user has already been deleted.
          type: string
        driver:
          $ref: '#/components/schemas/SparkNode'
        executors:
          description: Nodes on which the Spark executors reside.
          items:
            $ref: '#/components/schemas/SparkNode'
          type: array
        spark_context_id:
          description: "A canonical SparkContext identifier. This value _does_ change\
            \ when the Spark driver restarts. The pair `(cluster_id, spark_context_id)`\
            \ is a globally unique identifier over all Spark contexts."
          format: int64
          type: integer
        jdbc_port:
          description: Port on which Spark JDBC server is listening in the driver
            node. No service listens on this port in executor nodes.
          format: int32
          type: integer
        cluster_name:
          description: "Cluster name requested by the user. This doesn’t have to be\
            \ unique. If not specified at creation, the cluster name is an empty string."
          type: string
        spark_version:
          description: "The runtime version of the cluster. You can retrieve a list\
            \ of available runtime versions by using the [Runtime versions](https://docs.databricks.com/dev-tools/api/latest/clusters.html#runtime-versions)\
            \ API call."
          type: string
        spark_conf:
          additionalProperties: true
          description: An arbitrary object where the object key is a configuration
            propery name and the value is a configuration property value.
          title: SparkConfPair
          type: object
        aws_attributes:
          $ref: '#/components/schemas/AwsAttributes'
        node_type_id:
          description: "This field encodes, through a single value, the resources\
            \ available to each of the Spark nodes in this cluster. For example, the\
            \ Spark nodes can be provisioned and optimized for memory or compute intensive\
            \ workloads. A list of available node types can be retrieved by using\
            \ the [List node types](https://docs.databricks.com/dev-tools/api/latest/clusters.html#list-node-types)\
            \ API call."
          type: string
        driver_node_type_id:
          description: "The node type of the Spark driver. This field is optional;\
            \ if unset, the driver node type is set as the same value as `node_type_id`\
            \ defined above."
          type: string
        ssh_public_keys:
          description: SSH public key contents that are added to each Spark node in
            this cluster. The corresponding private keys can be used to login with
            the user name `ubuntu` on port `2200`. Up to 10 keys can be specified.
          items:
            type: string
          type: array
        custom_tags:
          description: |-
            An object containing a set of tags for cluster resources. Databricks tags all cluster resources (such as AWS instances and EBS volumes) with these tags in addition to default_tags.

            **Note**:

            * Tags are not supported on legacy node types such as compute-optimized and memory-optimized
            * Databricks allows at most 45 custom tags
          items:
            $ref: '#/components/schemas/ClusterTag'
          type: array
        cluster_log_conf:
          $ref: '#/components/schemas/ClusterLogConf'
        init_scripts:
          description: "The configuration for storing init scripts. Any number of\
            \ destinations can be specified. The scripts are executed sequentially\
            \ in the order provided. If `cluster_log_conf` is specified, init script\
            \ logs are sent to `<destination>/<cluster-ID>/init_scripts`."
          items:
            $ref: '#/components/schemas/InitScriptInfo'
          type: array
        docker_image:
          $ref: '#/components/schemas/DockerImage'
        spark_env_vars:
          additionalProperties: true
          description: An arbitrary object where the object key is an environment
            variable name and the value is an environment variable value.
          title: SparkEnvPair
          type: object
        autotermination_minutes:
          description: "Automatically terminates the cluster after it is inactive\
            \ for this time in minutes. If not set, this cluster is not be automatically\
            \ terminated. If specified, the threshold must be between 10 and 10000\
            \ minutes. You can also set this value to 0 to explicitly disable automatic\
            \ termination."
          format: int32
          type: integer
        enable_elastic_disk:
          description: "Autoscaling Local Storage: when enabled, this cluster dynamically\
            \ acquires additional disk space when its Spark workers are running low\
            \ on disk space. This feature requires specific AWS permissions to function\
            \ correctly - refer to [Autoscaling local storage](https://docs.databricks.com/clusters/configure.html#autoscaling-local-storage)\
            \ for details."
          type: boolean
        instance_pool_id:
          description: "The optional ID of the instance pool to which the cluster\
            \ belongs. Refer to [Pools](https://docs.databricks.com/clusters/instance-pools/index.html)\
            \ for details."
          type: string
        cluster_source:
          $ref: '#/components/schemas/ClusterSource'
        state:
          $ref: '#/components/schemas/ClusterState'
        state_message:
          description: "A message associated with the most recent state transition\
            \ (for example, the reason why the cluster entered a `TERMINATED` state).\
            \ This field is unstructured, and its exact format is subject to change."
          type: string
        start_time:
          description: Time (in epoch milliseconds) when the cluster creation request
            was received (when the cluster entered a `PENDING` state).
          format: int64
          type: integer
        terminated_time:
          description: "Time (in epoch milliseconds) when the cluster was terminated,\
            \ if applicable."
          format: int64
          type: integer
        last_state_loss_time:
          description: Time when the cluster driver last lost its state (due to a
            restart or driver failure).
          format: int64
          type: integer
        last_activity_time:
          description: "Time (in epoch milliseconds) when the cluster was last active.\
            \ A cluster is active if there is at least one command that has not finished\
            \ on the cluster. This field is available after the cluster has reached\
            \ a `RUNNING` state. Updates to this field are made as best-effort attempts.\
            \ Certain versions of Spark do not support reporting of cluster activity.\
            \ Refer to [Automatic termination](https://docs.databricks.com/clusters/clusters-manage.html#automatic-termination)\
            \ for details."
          format: int64
          type: integer
        cluster_memory_mb:
          description: "Total amount of cluster memory, in megabytes."
          format: int64
          type: integer
        cluster_cores:
          description: Number of CPU cores available for this cluster. This can be
            fractional since certain node types are configured to share cores between
            Spark nodes on the same instance.
          format: float
          type: number
        default_tags:
          additionalProperties:
            type: string
          description: "An object with key value pairs. The key length must be between\
            \ 1 and 127 UTF-8 characters, inclusive. The value length must be less\
            \ than or equal to 255 UTF-8 characters. For a list of all restrictions,\
            \ see AWS Tag Restrictions: <https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html#tag-restrictions>"
          title: ClusterTag
          type: object
        cluster_log_status:
          $ref: '#/components/schemas/LogSyncStatus'
        termination_reason:
          $ref: '#/components/schemas/TerminationReason'
    ClusterEvent:
      properties:
        cluster_id:
          description: Canonical identifier for the cluster. This field is required.
          type: string
        timestamp:
          description: "The timestamp when the event occurred, stored as the number\
            \ of milliseconds since the unix epoch. Assigned by the Timeline service."
          format: int64
          type: integer
        type:
          $ref: '#/components/schemas/ClusterEventType'
        details:
          $ref: '#/components/schemas/EventDetails'
      required:
      - cluster_id
      - details
      - type
    ClusterEventType:
      description: "* `CREATING`: Indicates that the cluster is being created.\n*\
        \ `DID_NOT_EXPAND_DISK`: Indicates that a disk is low on space, but adding\
        \ disks would put it over the max capacity.\n* `EXPANDED_DISK`: Indicates\
        \ that a disk was low on space and the disks were expanded.\n* `FAILED_TO_EXPAND_DISK`:\
        \ Indicates that a disk was low on space and disk space could not be expanded.\n\
        * `INIT_SCRIPTS_STARTING`: Indicates that the cluster scoped init script has\
        \ started.\n* `INIT_SCRIPTS_FINISHED`: Indicates that the cluster scoped init\
        \ script has finished.\n* `STARTING`: Indicates that the cluster is being\
        \ started.\n* `RESTARTING`: Indicates that the cluster is being started.\n\
        * `TERMINATING`: Indicates that the cluster is being terminated.\n* `EDITED`:\
        \ Indicates that the cluster has been edited.\n* `RUNNING`: Indicates the\
        \ cluster has finished being created. Includes the number of nodes in the\
        \ cluster and a failure reason if some nodes could not be acquired.\n* `RESIZING`:\
        \ Indicates a change in the target size of the cluster (upsize or downsize).\n\
        * `UPSIZE_COMPLETED`: Indicates that nodes finished being added to the cluster.\
        \ Includes the number of nodes in the cluster and a failure reason if some\
        \ nodes could not be acquired.\n* `NODES_LOST`: Indicates that some nodes\
        \ were lost from the cluster.\n* `DRIVER_HEALTHY`: Indicates that the driver\
        \ is healthy and the cluster is ready for use.\n* `DRIVER_UNAVAILABLE`: Indicates\
        \ that the driver is unavailable.\n* `SPARK_EXCEPTION`: Indicates that a Spark\
        \ exception was thrown from the driver.\n* `DRIVER_NOT_RESPONDING`: Indicates\
        \ that the driver is up but is not responsive, likely due to GC.\n* `DBFS_DOWN`:\
        \ Indicates that the driver is up but DBFS is down.\n* `METASTORE_DOWN`: Indicates\
        \ that the driver is up but the metastore is down.\n* `NODE_BLACKLISTED`:\
        \ Indicates that a node is not allowed by Spark.\n* `PINNED`: Indicates that\
        \ the cluster was pinned.\n* `UNPINNED`: Indicates that the cluster was unpinned."
      enum:
      - CREATING
      - DID_NOT_EXPAND_DISK
      - EXPANDED_DISK
      - FAILED_TO_EXPAND_DISK
      - INIT_SCRIPTS_STARTING
      - INIT_SCRIPTS_FINISHED
      - STARTING
      - RESTARTING
      - TERMINATING
      - EDITED
      - RUNNING
      - RESIZING
      - UPSIZE_COMPLETED
      - NODES_LOST
      - DRIVER_HEALTHY
      - DRIVER_UNAVAILABLE
      - SPARK_EXCEPTION
      - DRIVER_NOT_RESPONDING
      - DBFS_DOWN
      - METASTORE_DOWN
      - NODE_BLACKLISTED
      - PINNED
      - UNPINNED
      title: ClusterEventType
      type: string
    EventDetails:
      properties:
        current_num_workers:
          description: The number of nodes in the cluster.
          format: int32
          title: current_num_workers
          type: integer
        target_num_workers:
          description: The targeted number of nodes in the cluster.
          format: int32
          title: target_num_workers
          type: integer
        previous_attributes:
          $ref: '#/components/schemas/AwsAttributes'
        attributes:
          $ref: '#/components/schemas/AwsAttributes'
        previous_cluster_size:
          $ref: '#/components/schemas/ClusterSize'
        cluster_size:
          $ref: '#/components/schemas/ClusterSize'
        cause:
          $ref: '#/components/schemas/ResizeCause'
        reason:
          $ref: '#/components/schemas/TerminationReason'
        user:
          description: The user that caused the event to occur. (Empty if it was done
            by Databricks.)
          title: user
          type: string
      title: EventDetails
    AwsAttributes:
      example:
        ebs_volume_count: 2
        zone_id: zone_id
        ebs_volume_size: 7
        ebs_volume_throughput: 3
        spot_bid_price_percent: 5
        availability: SPOT
        ebs_volume_type: GENERAL_PURPOSE_SSD
        first_on_demand: 5
        ebs_volume_iops: 9
        instance_profile_arn: instance_profile_arn
      properties:
        first_on_demand:
          description: "The first first_on_demand nodes of the cluster are placed\
            \ on on-demand instances. If this value is greater than 0, the cluster\
            \ driver node is placed on an on-demand instance. If this value is greater\
            \ than or equal to the current cluster size, all nodes are placed on on-demand\
            \ instances. If this value is less than the current cluster size, first_on_demand\
            \ nodes are placed on on-demand instances and the remainder are placed\
            \ on `availability` instances. This value does not affect cluster size\
            \ and cannot be mutated over the lifetime of a cluster."
          format: int32
          title: first_on_demand
          type: integer
        availability:
          description: "Availability type used for all subsequent nodes past the `first_on_demand`\
            \ ones. **Note:** If `first_on_demand` is zero, this availability type\
            \ is used for the entire cluster.\n\n`SPOT`: use spot instances.\n`ON_DEMAND`:\
            \ use on-demand instances.\n`SPOT_WITH_FALLBACK`: preferably use spot\
            \ instances, but fall back to on-demand instances if spot instances cannot\
            \ be acquired (for example, if AWS spot prices are too high)."
          enum:
          - SPOT
          - ON_DEMAND
          - SPOT_WITH_FALLBACK
          title: availability
          type: string
        zone_id:
          description: "Identifier for the availability zone/datacenter in which the\
            \ cluster resides. You have three options:\n\n**Specify an availability\
            \ zone as a string**, for example: “us-west-2a”. The provided availability\
            \ zone must be in the same region as the Databricks deployment. For example,\
            \ “us-west-2a” is not a valid zone ID if the Databricks deployment resides\
            \ in the “us-east-1” region.\n\n**Enable automatic availability zone selection\
            \ (“Auto-AZ”)**, by setting the value “auto”. Databricks selects the AZ\
            \ based on available IPs in the workspace subnets and retries in other\
            \ availability zones if AWS returns insufficient capacity errors.\n\n\
            **Do not specify a value**. If not specified, a default zone is used.\n\
            \nThe list of available zones as well as the default value can be found\
            \ by using the [List zones](https://docs.databricks.com/dev-tools/api/latest/clusters.html#list-zones)\
            \ API."
          title: zone_id
          type: string
        instance_profile_arn:
          description: "Nodes for this cluster are only be placed on AWS instances\
            \ with this instance profile. If omitted, nodes are placed on instances\
            \ without an instance profile. The instance profile must have previously\
            \ been added to the Databricks environment by an account administrator.\n\
            \nThis feature may only be available to certain customer plans."
          title: instance_profile_arn
          type: string
        spot_bid_price_percent:
          description: "The max price for AWS spot instances, as a percentage of the\
            \ corresponding instance type’s on-demand price. For example, if this\
            \ field is set to 50, and the cluster needs a new `i3.xlarge` spot instance,\
            \ then the max price is half of the price of on-demand `i3.xlarge` instances.\
            \ Similarly, if this field is set to 200, the max price is twice the price\
            \ of on-demand `i3.xlarge` instances. If not specified, the default value\
            \ is 100\\. When spot instances are requested for this cluster, only spot\
            \ instances whose max price percentage matches this field is considered.\
            \ For safety, we enforce this field to be no more than 10000."
          format: int32
          title: spot_bid_price_percent
          type: integer
        ebs_volume_type:
          description: "The type of EBS volume that is launched with this cluster.\n\
            \n`GENERAL_PURPOSE_SSD`: provision extra storage using AWS gp2 EBS volumes.\n\
            `THROUGHPUT_OPTIMIZED_HDD`: provision extra storage using AWS st1 volumes."
          enum:
          - GENERAL_PURPOSE_SSD
          - THROUGHPUT_OPTIMIZED_HDD
          title: ebs_volume_type
          type: string
        ebs_volume_count:
          description: "The number of volumes launched for each instance. You can\
            \ choose up to 10 volumes. This feature is only enabled for supported\
            \ node types. Legacy node types cannot specify custom EBS volumes. For\
            \ node types with no instance store, at least one EBS volume needs to\
            \ be specified; otherwise, cluster creation fails.\n\nThese EBS volumes\
            \ are mounted at `/ebs0`, `/ebs1`, and etc. Instance store volumes are\
            \ mounted at `/local_disk0`, `/local_disk1`, and etc.\n\nIf EBS volumes\
            \ are attached, Databricks configures Spark to use only the EBS volumes\
            \ for scratch storage because heterogeneously sized scratch devices can\
            \ lead to inefficient disk utilization. If no EBS volumes are attached,\
            \ Databricks configures Spark to use instance store volumes.\n\nIf EBS\
            \ volumes are specified, then the Spark configuration `spark.local.dir`\
            \ is overridden."
          format: int32
          title: ebs_volume_count
          type: integer
        ebs_volume_size:
          description: "The size of each EBS volume (in GiB) launched for each instance.\
            \ For general purpose SSD, this value must be within the range 100 - 4096\\\
            . For throughput optimized HDD, this value must be within the range 500\
            \ - 4096\\. Custom EBS volumes cannot be specified for the legacy node\
            \ types (_memory-optimized_ and _compute-optimized_)."
          format: int32
          title: ebs_volume_size
          type: integer
        ebs_volume_iops:
          description: "The number of IOPS per EBS gp3 volume.\n\nThis value must\
            \ be between 3000 and 16000.\n\nThe value of IOPS and throughput is calculated\
            \ based on AWS documentation to match the maximum performance of a gp2\
            \ volume with the same volume size.\n\nFor more information, see the [EBS\
            \ volume limit calculator](https://github.com/awslabs/aws-support-tools/tree/master/EBS/VolumeLimitCalculator)."
          format: int32
          title: ebs_volume_iops
          type: integer
        ebs_volume_throughput:
          description: "The throughput per EBS gp3 volume, in MiB per second.\n\n\
            This value must be between 125 and 1000."
          format: int32
          title: ebs_volume_throughput
          type: integer
      title: AwsAttributes
    ClusterAttributes:
      properties:
        cluster_name:
          description: "Cluster name requested by the user. This doesn’t have to be\
            \ unique. If not specified at creation, the cluster name is an empty string."
          type: string
        spark_version:
          description: "The runtime version of the cluster, for example “5.0.x-scala2.11”\
            . You can retrieve a list of available runtime versions by using the [Runtime\
            \ versions](https://docs.databricks.com/dev-tools/api/latest/clusters.html#runtime-versions)\
            \ API call."
          type: string
        spark_conf:
          additionalProperties: true
          description: An arbitrary object where the object key is a configuration
            propery name and the value is a configuration property value.
          title: SparkConfPair
          type: object
        aws_attributes:
          $ref: '#/components/schemas/AwsAttributes'
        node_type_id:
          description: "This field encodes, through a single value, the resources\
            \ available to each of the Spark nodes in this cluster. For example, the\
            \ Spark nodes can be provisioned and optimized for memory or compute intensive\
            \ workloads A list of available node types can be retrieved by using the\
            \ [List node types](https://docs.databricks.com/dev-tools/api/latest/clusters.html#list-node-types)\
            \ API call."
          type: string
        driver_node_type_id:
          description: "The node type of the Spark driver. This field is optional;\
            \ if unset, the driver node type is set as the same value as `node_type_id`\
            \ defined above."
          type: string
        ssh_public_keys:
          description: SSH public key contents that is added to each Spark node in
            this cluster. The corresponding private keys can be used to login with
            the user name `ubuntu` on port `2200`. Up to 10 keys can be specified.
          items:
            type: string
          type: array
        custom_tags:
          additionalProperties:
            type: string
          description: "An object with key value pairs. The key length must be between\
            \ 1 and 127 UTF-8 characters, inclusive. The value length must be less\
            \ than or equal to 255 UTF-8 characters. For a list of all restrictions,\
            \ see AWS Tag Restrictions: <https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html#tag-restrictions>"
          title: ClusterTag
          type: object
        cluster_log_conf:
          $ref: '#/components/schemas/ClusterLogConf'
        init_scripts:
          description: "The configuration for storing init scripts. Any number of\
            \ destinations can be specified. The scripts are executed sequentially\
            \ in the order provided. If `cluster_log_conf` is specified, init script\
            \ logs are sent to `<destination>/<cluster-ID>/init_scripts`."
          items:
            $ref: '#/components/schemas/InitScriptInfo'
          type: array
        docker_image:
          $ref: '#/components/schemas/DockerImage'
        spark_env_vars:
          additionalProperties: true
          description: An arbitrary object where the object key is an environment
            variable name and the value is an environment variable value.
          title: SparkEnvPair
          type: object
        autotermination_minutes:
          description: "Automatically terminates the cluster after it is inactive\
            \ for this time in minutes. If not set, this cluster is not be automatically\
            \ terminated. If specified, the threshold must be between 10 and 10000\
            \ minutes. You can also set this value to 0 to explicitly disable automatic\
            \ termination."
          format: int32
          type: integer
        enable_elastic_disk:
          description: "Autoscaling Local Storage: when enabled, this cluster dynamically\
            \ acquires additional disk space when its Spark workers are running low\
            \ on disk space. This feature requires specific AWS permissions to function\
            \ correctly. Refer to [Autoscaling local storage](https://docs.databricks.com/clusters/configure.html#autoscaling-local-storage)\
            \ for details."
          type: boolean
        instance_pool_id:
          description: "The optional ID of the instance pool to which the cluster\
            \ belongs. Refer to [Pools](https://docs.databricks.com/clusters/instance-pools/index.html)\
            \ for details."
          type: string
        cluster_source:
          $ref: '#/components/schemas/ClusterSource'
        policy_id:
          description: "A [cluster policy](https://docs.databricks.com/dev-tools/api/latest/policies.html)\
            \ ID."
          type: string
    ClusterSize:
      properties:
        num_workers:
          description: "If num_workers, number of worker nodes that this cluster must\
            \ have. A cluster has one Spark driver and num_workers executors for a\
            \ total of num_workers + 1 Spark nodes. When reading the properties of\
            \ a cluster, this field reflects the desired number of workers rather\
            \ than the actual number of workers. For instance, if a cluster is resized\
            \ from 5 to 10 workers, this field is updated to reflect the target size\
            \ of 10 workers, whereas the workers listed in executors gradually increase\
            \ from 5 to 10 as the new nodes are provisioned."
          format: int32
          title: num_workers
          type: integer
        autoscale:
          $ref: '#/components/schemas/AutoScale'
      title: ClusterSize
    ListOrder:
      description: "* `DESC`: Descending order.\n* `ASC`: Ascending order."
      enum:
      - DESC
      - ASC
      type: string
    ResizeCause:
      description: "* `AUTOSCALE`: Automatically resized based on load.\n* `USER_REQUEST`:\
        \ User requested a new size.\n* `AUTORECOVERY`: Autorecovery monitor resized\
        \ the cluster after it lost a node."
      enum:
      - AUTOSCALE
      - USER_REQUEST
      - AUTORECOVERY
      title: ResizeCause
      type: string
    ClusterLogConf:
      example:
        s3:
          encryption_type: encryption_type
          enable_encryption: true
          endpoint: endpoint
          canned_acl: canned_acl
          destination: destination
          region: region
          kms_key: kms_key
        dbfs:
          destination: destination
      properties:
        dbfs:
          $ref: '#/components/schemas/DbfsStorageInfo'
        s3:
          $ref: '#/components/schemas/S3StorageInfo'
      title: ClusterLogConf
    InitScriptInfo:
      example:
        S3:
          encryption_type: encryption_type
          enable_encryption: true
          endpoint: endpoint
          canned_acl: canned_acl
          destination: destination
          region: region
          kms_key: kms_key
        file:
          destination: destination
        dbfs:
          destination: destination
      properties:
        dbfs:
          $ref: '#/components/schemas/DbfsStorageInfo'
        file:
          $ref: '#/components/schemas/FileStorageInfo'
        S3:
          $ref: '#/components/schemas/S3StorageInfo'
      title: InitScriptInfo
    ClusterTag:
      additionalProperties:
        type: string
      description: "An object with key value pairs. The key length must be between\
        \ 1 and 127 UTF-8 characters, inclusive. The value length must be less than\
        \ or equal to 255 UTF-8 characters. For a list of all restrictions, see AWS\
        \ Tag Restrictions: <https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html#tag-restrictions>"
      title: ClusterTag
      type: object
    DbfsStorageInfo:
      example:
        destination: destination
      properties:
        destination:
          description: "DBFS destination. Example: `dbfs:/my/path`"
          title: destination
          type: string
      title: DbfsStorageInfo
    FileStorageInfo:
      example:
        destination: destination
      properties:
        destination:
          description: "File destination. Example: `file:/my/file.sh`"
          title: destination
          type: string
      title: FileStorageInfo
    DockerImage:
      properties:
        url:
          description: URL for the Docker image.
          title: url
          type: string
        basic_auth:
          $ref: '#/components/schemas/DockerBasicAuth'
      title: DockerImage
    DockerBasicAuth:
      properties:
        username:
          description: User name for the Docker repository.
          title: username
          type: string
        password:
          description: Password for the Docker repository.
          title: password
          type: string
      title: DockerBasicAuth
    LogSyncStatus:
      properties:
        last_attempted:
          description: "The timestamp of last attempt. If the last attempt fails,\
            \ last_exception contains the exception in the last attempt."
          format: int64
          title: last_attempted
          type: integer
        last_exception:
          description: "The exception thrown in the last attempt, it would be null\
            \ (omitted in the response) if there is no exception in last attempted."
          title: last_exception
          type: string
      title: LogSyncStatus
    NodeType:
      properties:
        node_type_id:
          description: Unique identifier for this node type. This field is required.
          type: string
        memory_mb:
          description: Memory (in MB) available for this node type. This field is
            required.
          format: int32
          type: integer
        num_cores:
          description: Number of CPU cores available for this node type. This can
            be fractional if the number of cores on a machine instance is not divisible
            by the number of Spark nodes on that machine. This field is required.
          format: float
          type: number
        description:
          description: A string description associated with this node type. This field
            is required.
          type: string
        instance_type_id:
          description: An identifier for the type of hardware that this node runs
            on. This field is required.
          type: string
        is_deprecated:
          description: Whether the node type is deprecated. Non-deprecated node types
            offer greater performance.
          type: boolean
        node_info:
          $ref: '#/components/schemas/ClusterCloudProviderNodeInfo'
      required:
      - description
      - instance_type_id
      - memory_mb
      - node_type_id
    ClusterCloudProviderNodeInfo:
      properties:
        status:
          $ref: '#/components/schemas/ClusterCloudProviderNodeStatus'
        available_core_quota:
          description: Available CPU core quota.
          format: int32
          title: available_core_quota
          type: integer
        total_core_quota:
          description: Total CPU core quota.
          format: int32
          title: total_core_quota
          type: integer
      title: ClusterCloudProviderNodeInfo
    ClusterCloudProviderNodeStatus:
      description: "* NotEnabledOnSubscription: Node type not available for subscription.\n\
        * NotAvailableInRegion: Node type not available in region.\n"
      enum:
      - NotEnabledOnSubscription
      - NotAvailableInRegion
      title: ClusterCloudProviderNodeStatus
      type: string
    ParameterPair:
      additionalProperties: true
      description: An object with additional information about why a cluster was terminated.
        The object keys are one of `TerminationParameter` and the value is the termination
        information.
      title: ParameterPair
      type: object
    SparkConfPair:
      additionalProperties: true
      description: An arbitrary object where the object key is a configuration propery
        name and the value is a configuration property value.
      title: SparkConfPair
      type: object
    SparkEnvPair:
      additionalProperties: true
      description: An arbitrary object where the object key is an environment variable
        name and the value is an environment variable value.
      title: SparkEnvPair
      type: object
    SparkNode:
      properties:
        private_ip:
          description: Private IP address (typically a 10.x.x.x address) of the Spark
            node. This is different from the private IP address of the host instance.
          title: private_ip
          type: string
        public_dns:
          description: "Public DNS address of this node. This address can be used\
            \ to access the Spark JDBC server on the driver node. To communicate with\
            \ the JDBC server, traffic must be manually authorized by adding security\
            \ group rules to the “worker-unmanaged” security group via the AWS console."
          title: public_dns
          type: string
        node_id:
          description: Globally unique identifier for this node.
          title: node_id
          type: string
        instance_id:
          description: Globally unique identifier for the host instance from the cloud
            provider.
          title: instance_id
          type: string
        start_timestamp:
          description: The timestamp (in millisecond) when the Spark node is launched.
          format: int64
          title: start_timestamp
          type: integer
        node_aws_attributes:
          $ref: '#/components/schemas/SparkNodeAwsAttributes'
        host_private_ip:
          description: The private IP address of the host instance.
          title: host_private_ip
          type: string
      title: SparkNode
    SparkVersion:
      properties:
        key:
          description: "[Databricks Runtime version](https://docs.databricks.com/dev-tools/api/latest/index.html#programmatic-version)\
            \ key, for example `7.3.x-scala2.12`. The value that must be provided\
            \ as the `spark_version` when creating a new cluster. The exact runtime\
            \ version may change over time for a “wildcard” version (that is, `7.3.x-scala2.12`\
            \ is a “wildcard” version) with minor bug fixes."
          type: string
        name:
          description: "A descriptive name for the runtime version, for example “\
            Databricks Runtime 7.3 LTS”."
          type: string
    TerminationReason:
      properties:
        code:
          $ref: '#/components/schemas/TerminationCode'
        type:
          $ref: '#/components/schemas/TerminationType'
        parameters:
          additionalProperties: true
          description: An object with additional information about why a cluster was
            terminated. The object keys are one of `TerminationParameter` and the
            value is the termination information.
          title: ParameterPair
          type: object
      title: TerminationReason
    PoolClusterTerminationCode:
      description: "* INSTANCE_POOL_MAX_CAPACITY_FAILURE: The pool max capacity has\
        \ been reached.\n* INSTANCE_POOL_NOT_FOUND_FAILURE: The pool specified by\
        \ the cluster is no longer active or doesn’t exist."
      enum:
      - INSTANCE_POOL_MAX_CAPACITY_FAILURE
      - INSTANCE_POOL_NOT_FOUND_FAILURE
      type: string
    ClusterSource:
      description: "* UI: Cluster created through the UI.\n* JOB: Cluster created\
        \ by the Databricks job scheduler.\n* API: Cluster created through an API\
        \ call.\n"
      enum:
      - UI
      - JOB
      - API
      title: ClusterSource
      type: string
    ClusterState:
      description: "* PENDING: Indicates that a cluster is in the process of being\
        \ created.\n* RUNNING: Indicates that a cluster has been started and is ready\
        \ for use.\n* RESTARTING: Indicates that a cluster is in the process of restarting.\n\
        * RESIZING: Indicates that a cluster is in the process of adding or removing\
        \ nodes.\n* TERMINATING: Indicates that a cluster is in the process of being\
        \ destroyed.\n* TERMINATED: Indicates that a cluster has been successfully\
        \ destroyed.\n* ERROR: This state is no longer used. It was used to indicate\
        \ a cluster that failed to be created. `TERMINATING` and `TERMINATED` are\
        \ used instead.\n* UNKNOWN: Indicates that a cluster is in an unknown state.\
        \ A cluster should never be in this state.\n"
      enum:
      - PENDING
      - RUNNING
      - RESTARTING
      - RESIZING
      - TERMINATING
      - TERMINATED
      - ERROR
      - UNKNOWN
      title: ClusterState
      type: string
    TerminationCode:
      description: "* USER_REQUEST: A user terminated the cluster directly. Parameters\
        \ should include a `username` field that indicates the specific user who terminated\
        \ the cluster.\n* JOB_FINISHED: The cluster was launched by a job, and terminated\
        \ when the job completed.\n* INACTIVITY: The cluster was terminated since\
        \ it was idle.\n* CLOUD_PROVIDER_SHUTDOWN: The instance that hosted the Spark\
        \ driver was terminated by the cloud provider. In AWS, for example, AWS may\
        \ retire instances and directly shut them down. Parameters should include\
        \ an `aws_instance_state_reason` field indicating the AWS-provided reason\
        \ why the instance was terminated.\n* COMMUNICATION_LOST: Databricks lost\
        \ connection to services on the driver instance. For example, this can happen\
        \ when problems arise in cloud networking infrastructure, or when the instance\
        \ itself becomes unhealthy.\n* CLOUD_PROVIDER_LAUNCH_FAILURE: Databricks experienced\
        \ a cloud provider failure when requesting instances to launch clusters. For\
        \ example, AWS limits the number of running instances and EBS volumes. If\
        \ you ask Databricks to launch a cluster that requires instances or EBS volumes\
        \ that exceed your AWS limit, the cluster fails with this status code. Parameters\
        \ should include one of `aws_api_error_code`, `aws_instance_state_reason`,\
        \ or `aws_spot_request_status` to indicate the AWS-provided reason why Databricks\
        \ could not request the required instances for the cluster.\n* SPARK_STARTUP_FAILURE:\
        \ The cluster failed to initialize. Possible reasons may include failure to\
        \ create the environment for Spark or issues launching the Spark master and\
        \ worker processes.\n* INVALID_ARGUMENT: Cannot launch the cluster because\
        \ the user specified an invalid argument. For example, the user might specify\
        \ an invalid runtime version for the cluster.\n* UNEXPECTED_LAUNCH_FAILURE:\
        \ While launching this cluster, Databricks failed to complete critical setup\
        \ steps, terminating the cluster.\n* INTERNAL_ERROR: Databricks encountered\
        \ an unexpected error that forced the running cluster to be terminated. Contact\
        \ Databricks support for additional details.\n* SPARK_ERROR: The Spark driver\
        \ failed to start. Possible reasons may include incompatible libraries and\
        \ initialization scripts that corrupted the Spark container.\n* METASTORE_COMPONENT_UNHEALTHY:\
        \ The cluster failed to start because the external metastore could not be\
        \ reached. Refer to [Troubleshooting](https://docs.databricks.com/data/metastores/external-hive-metastore.html#troubleshooting).\n\
        * DBFS_COMPONENT_UNHEALTHY: The cluster failed to start because Databricks\
        \ File System (DBFS) could not be reached. \n* DRIVER_UNREACHABLE: Databricks\
        \ was not able to access the Spark driver, because it was not reachable.\n\
        * DRIVER_UNRESPONSIVE: Databricks was not able to access the Spark driver,\
        \ because it was unresponsive.\n* INSTANCE_UNREACHABLE: Databricks was not\
        \ able to access instances in order to start the cluster. This can be a transient\
        \ networking issue. If the problem persists, this usually indicates a networking\
        \ environment misconfiguration.\n* CONTAINER_LAUNCH_FAILURE: Databricks was\
        \ unable to launch containers on worker nodes for the cluster. Have your admin\
        \ check your network configuration.\n* INSTANCE_POOL_CLUSTER_FAILURE: Pool\
        \ backed cluster specific failure. Refer to [Pools](https://docs.databricks.com/clusters/instance-pools/index.html)\
        \ for details.\n* REQUEST_REJECTED: Databricks cannot handle the request at\
        \ this moment. Try again later and contact Databricks if the problem persists.\n\
        * INIT_SCRIPT_FAILURE: Databricks cannot load and run a cluster-scoped init\
        \ script on one of the cluster’s nodes, or the init script terminates with\
        \ a non-zero exit code. Refer to [Init script logs](https://docs.databricks.com/clusters/init-scripts.html#init-script-log).\n\
        * TRIAL_EXPIRED: The Databricks trial subscription expired."
      enum:
      - USER_REQUEST
      - JOB_FINISHED
      - INACTIVITY
      - CLOUD_PROVIDER_SHUTDOWN
      - COMMUNICATION_LOST
      - CLOUD_PROVIDER_LAUNCH_FAILURE
      - SPARK_STARTUP_FAILURE
      - INVALID_ARGUMENT
      - UNEXPECTED_LAUNCH_FAILURE
      - INTERNAL_ERROR
      - SPARK_ERROR
      - METASTORE_COMPONENT_UNHEALTHY
      - DBFS_COMPONENT_UNHEALTHY
      - DRIVER_UNREACHABLE
      - DRIVER_UNRESPONSIVE
      - INSTANCE_UNREACHABLE
      - CONTAINER_LAUNCH_FAILURE
      - INSTANCE_POOL_CLUSTER_FAILURE
      - REQUEST_REJECTED
      - INIT_SCRIPT_FAILURE
      - TRIAL_EXPIRED
      title: TerminationCode
      type: string
    TerminationType:
      description: "* SUCCESS: Termination succeeded.\n* CLIENT_ERROR: Non-retriable.\
        \ Client must fix parameters before reattempting the cluster creation.\n*\
        \ SERVICE_FAULT: Databricks service issue. Client can retry.\n* CLOUD_FAILURECloud\
        \ provider infrastructure issue. Client can retry after the underlying issue\
        \ is resolved.\n"
      enum:
      - SUCCESS
      - CLIENT_ERROR
      - SERVICE_FAULT
      - CLOUD_FAILURE
      title: TerminationType
      type: string
    TerminationParameter:
      properties:
        username:
          description: The username of the user who terminated the cluster.
          type: string
        aws_api_error_code:
          description: "The AWS provided error code describing why cluster nodes could\
            \ not be provisioned. For example, `InstanceLimitExceeded` indicates that\
            \ the limit of EC2 instances for a specific instance type has been exceeded.\
            \ For reference, see: <https://docs.aws.amazon.com/AWSEC2/latest/APIReference/query-api-troubleshooting.html>."
          type: string
        aws_instance_state_reason:
          description: "The AWS provided state reason describing why the driver node\
            \ was terminated. For example, `Client.VolumeLimitExceeded` indicates\
            \ that the limit of EBS volumes or total EBS volume storage has been exceeded.\
            \ For reference, see <https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_StateReason.html>."
          type: string
        aws_spot_request_status:
          description: "Describes why a spot request could not be fulfilled. For example,\
            \ `price-too-low` indicates that the max price was lower than the current\
            \ spot price. For reference, see: <https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-bid-status.html#spot-instance-bid-status-understand>."
          type: string
        aws_spot_request_fault_code:
          description: "Provides additional details when a spot request fails. For\
            \ example `InsufficientFreeAddressesInSubnet` indicates the subnet does\
            \ not have free IP addresses to accommodate the new instance. For reference,\
            \ see <https://docs.aws.amazon.com/cli/latest/reference/ec2/describe-spot-instance-requests.html>."
          type: string
        aws_impaired_status_details:
          description: "The AWS provided status check which failed and induced a node\
            \ loss. This status may correspond to a failed instance or system check.\
            \ For reference, see <https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-system-instance-status-check.html>."
          type: string
        aws_instance_status_event:
          description: "The AWS provided scheduled event (for example reboot) which\
            \ induced a node loss. For reference, see <https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-instances-status-check_sched.html>."
          type: string
        aws_error_message:
          description: "Human-readable context of various failures from AWS. This\
            \ field is unstructured, and its exact format is subject to change."
          type: string
        databricks_error_message:
          description: "Additional context that may explain the reason for cluster\
            \ termination. This field is unstructured, and its exact format is subject\
            \ to change."
          type: string
        inactivity_duration_min:
          description: An idle cluster was shut down after being inactive for this
            duration.
          type: string
        instance_id:
          description: The ID of the instance that was hosting the Spark driver.
          type: string
        instance_pool_id:
          description: The ID of the instance pool the cluster is using.
          type: string
        instance_pool_error_code:
          description: "The [error code](https://docs.databricks.com/dev-tools/api/latest/clusters.html#clusterterminationreasonpoolclusterterminationcode)\
            \ for cluster failures specific to a pool."
          type: string
    S3StorageInfo:
      example:
        encryption_type: encryption_type
        enable_encryption: true
        endpoint: endpoint
        canned_acl: canned_acl
        destination: destination
        region: region
        kms_key: kms_key
      properties:
        destination:
          description: "S3 destination. For example: `s3://my-bucket/some-prefix`\
            \ You must configure the cluster with an instance profile and the instance\
            \ profile must have write access to the destination. You _cannot_ use\
            \ AWS keys."
          title: destination
          type: string
        region:
          description: "S3 region. For example: `us-west-2`. Either region or endpoint\
            \ must be set. If both are set, endpoint is used."
          title: region
          type: string
        endpoint:
          description: "S3 endpoint. For example: `https://s3-us-west-2.amazonaws.com`.\
            \ Either region or endpoint must be set. If both are set, endpoint is\
            \ used."
          title: endpoint
          type: string
        enable_encryption:
          description: "(Optional)Enable server side encryption, `false` by default."
          title: enable_encryption
          type: boolean
        encryption_type:
          description: "(Optional) The encryption type, it could be `sse-s3` or `sse-kms`.\
            \ It is used only when encryption is enabled and the default type is `sse-s3`."
          title: encryption_type
          type: string
        kms_key:
          description: (Optional) KMS key used if encryption is enabled and encryption
            type is set to `sse-kms`.
          title: kms_key
          type: string
        canned_acl:
          description: "(Optional) Set canned access control list. For example: `bucket-owner-full-control`.\
            \ If canned_acl is set, the cluster instance profile must have `s3:PutObjectAcl`\
            \ permission on the destination bucket and prefix. The full list of possible\
            \ canned ACLs can be found at <https://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html#canned-acl>.\
            \ By default only the object owner gets full control. If you are using\
            \ cross account role for writing data, you may want to set `bucket-owner-full-control`\
            \ to make bucket owner able to read the logs."
          title: canned_acl
          type: string
      title: S3StorageInfo
    SparkNodeAwsAttributes:
      properties:
        is_spot:
          description: Whether this node is on an Amazon spot instance.
          title: is_spot
          type: boolean
      title: SparkNodeAwsAttributes
    ClusterLibraryStatuses:
      properties:
        cluster_id:
          description: Unique identifier for the cluster.
          type: string
        library_statuses:
          description: Status of all libraries on the cluster.
          items:
            $ref: '#/components/schemas/LibraryFullStatus'
          type: array
    Library:
      example:
        cran:
          package: geojson
          repo: https://my-repo.com
        egg: dbfs:/my/egg
        pypi:
          package: simplejson==3.8.0
          repo: https://my-repo.com
        maven:
          repo: https://my-repo.com
          coordinates: org.jsoup:jsoup:1.7.2
          exclusions:
          - slf4j:slf4j
          - '*:hadoop-client'
        jar: dbfs:/my-jar.jar
        whl: dbfs:/my/whl
      properties:
        jar:
          description: "If jar, URI of the JAR to be installed. DBFS and S3 URIs are\
            \ supported. For example: `{ \"jar\": \"dbfs:/mnt/databricks/library.jar\"\
            \ }` or `{ \"jar\": \"s3://my-bucket/library.jar\" }`. If S3 is used,\
            \ make sure the cluster has read access on the library. You may need to\
            \ launch the cluster with an instance profile to access the S3 URI."
          example: dbfs:/my-jar.jar
          title: jar
          type: string
        egg:
          description: "If egg, URI of the egg to be installed. DBFS and S3 URIs are\
            \ supported. For example: `{ \"egg\": \"dbfs:/my/egg\" }` or `{ \"egg\"\
            : \"s3://my-bucket/egg\" }`. If S3 is used, make sure the cluster has\
            \ read access on the library. You may need to launch the cluster with\
            \ an instance profile to access the S3 URI."
          example: dbfs:/my/egg
          title: egg
          type: string
        whl:
          description: "If whl, URI of the wheel or zipped wheels to be installed.\
            \ DBFS and S3 URIs are supported. For example: `{ \"whl\": \"dbfs:/my/whl\"\
            \ }` or `{ \"whl\": \"s3://my-bucket/whl\" }`. If S3 is used, make sure\
            \ the cluster has read access on the library. You may need to launch the\
            \ cluster with an instance profile to access the S3 URI. Also the wheel\
            \ file name needs to use the [correct convention](https://www.python.org/dev/peps/pep-0427/#file-format).\
            \ If zipped wheels are to be installed, the file name suffix should be\
            \ `.wheelhouse.zip`."
          example: dbfs:/my/whl
          title: whl
          type: string
        pypi:
          $ref: '#/components/schemas/PythonPyPiLibrary'
        maven:
          $ref: '#/components/schemas/MavenLibrary'
        cran:
          $ref: '#/components/schemas/RCranLibrary'
      title: Library
    LibraryFullStatus:
      properties:
        library:
          $ref: '#/components/schemas/Library'
        status:
          $ref: '#/components/schemas/LibraryInstallStatus'
        messages:
          description: All the info and warning messages that have occurred so far
            for this library.
          items:
            type: string
          title: messages
          type: array
        is_library_for_all_clusters:
          description: Whether the library was set to be installed on all clusters
            via the libraries UI.
          title: is_library_for_all_clusters
          type: boolean
      title: LibraryFullStatus
    MavenLibrary:
      example:
        repo: https://my-repo.com
        coordinates: org.jsoup:jsoup:1.7.2
        exclusions:
        - slf4j:slf4j
        - '*:hadoop-client'
      properties:
        coordinates:
          description: "Gradle-style Maven coordinates. For example: `org.jsoup:jsoup:1.7.2`.\
            \ This field is required."
          example: org.jsoup:jsoup:1.7.2
          title: coordinates
          type: string
        repo:
          description: "Maven repo to install the Maven package from. If omitted,\
            \ both Maven Central Repository and Spark Packages are searched."
          example: https://my-repo.com
          title: repo
          type: string
        exclusions:
          description: "List of dependences to exclude. For example: `[\"slf4j:slf4j\"\
            , \"*:hadoop-client\"]`.\n\nMaven dependency exclusions: <https://maven.apache.org/guides/introduction/introduction-to-optional-and-excludes-dependencies.html>."
          example:
          - slf4j:slf4j
          - '*:hadoop-client'
          items:
            type: string
          title: exclusions
          type: array
      required:
      - coordinates
      title: MavenLibrary
    PythonPyPiLibrary:
      example:
        package: simplejson==3.8.0
        repo: https://my-repo.com
      properties:
        package:
          description: "The name of the PyPI package to install. An optional exact\
            \ version specification is also supported. Examples: `simplejson` and\
            \ `simplejson==3.8.0`. This field is required."
          example: simplejson==3.8.0
          title: package
          type: string
        repo:
          description: "The repository where the package can be found. If not specified,\
            \ the default pip index is used."
          example: https://my-repo.com
          title: repo
          type: string
      required:
      - package
      title: PythonPyPiLibrary
    RCranLibrary:
      example:
        package: geojson
        repo: https://my-repo.com
      properties:
        package:
          description: The name of the CRAN package to install. This field is required.
          example: geojson
          title: package
          type: string
        repo:
          description: "The repository where the package can be found. If not specified,\
            \ the default CRAN repo is used."
          example: https://my-repo.com
          title: repo
          type: string
      required:
      - package
      title: RCranLibrary
    LibraryInstallStatus:
      description: "* `PENDING`: No action has yet been taken to install the library.\
        \ This state should be very short lived.\n* `RESOLVING`: Metadata necessary\
        \ to install the library is being retrieved from the provided repository.\
        \ For Jar, Egg, and Whl libraries, this step is a no-op.\n* `INSTALLING`:\
        \ The library is actively being installed, either by adding resources to Spark\
        \ or executing system commands inside the Spark nodes.\n* `INSTALLED`: The\
        \ library has been successfully instally.\n* `SKIPPED`: Installation on a\
        \ Databricks Runtime 7.0 or above cluster was skipped due to Scala version\
        \ incompatibility.\n* `FAILED`: Some step in installation failed. More information\
        \ can be found in the messages field.\n* `UNINSTALL_ON_RESTART`: The library\
        \ has been marked for removal. Libraries can be removed only when clusters\
        \ are restarted, so libraries that enter this state remains until the cluster\
        \ is restarted."
      enum:
      - PENDING
      - RESOLVING
      - INSTALLING
      - INSTALLED
      - SKIPPED
      - FAILED
      - UNINSTALL_ON_RESTART
      title: LibraryInstallStatus
      type: string
    Error:
      properties:
        error_code:
          description: Error code
          example: INTERNAL_ERROR
          title: error_code
          type: string
        message:
          description: Human-readable error message that describes the cause of the
            error.
          example: Unexpected error.
          title: message
          type: string
      title: Error
      type: object
    AccessControlList:
      properties:
        access_control_list:
          description: List of permissions to set on the job.
          items:
            $ref: '#/components/schemas/AccessControlRequest'
          title: access_control_list
          type: array
      title: AccessControlList
      type: object
    AccessControlRequest:
      oneOf:
      - $ref: '#/components/schemas/AccessControlRequestForUser'
      - $ref: '#/components/schemas/AccessControlRequestForGroup'
      title: AccessControlRequest
    AccessControlRequestForUser:
      properties:
        user_name:
          description: Email address for the user.
          example: jsmith@example.com
          title: UserName
          type: string
        permission_level:
          $ref: '#/components/schemas/PermissionLevel'
      title: AccessControlRequestForUser
      type: object
    AccessControlRequestForGroup:
      properties:
        group_name:
          description: "Group name. There are two built-in groups: `users` for all\
            \ users, and `admins` for administrators."
          example: users
          title: GroupName
          type: string
        permission_level:
          $ref: '#/components/schemas/PermissionLevelForGroup'
      title: AccessControlRequestForGroup
      type: object
    AccessControlRequestForServicePrincipal:
      properties:
        service_principal_name:
          description: Name of an Azure service principal.
          example: 9f0621ee-b52b-11ea-b3de-0242ac130004
          type: string
        permission_level:
          $ref: '#/components/schemas/PermissionLevel'
      type: object
    UserName:
      description: Email address for the user.
      example: jsmith@example.com
      title: UserName
      type: string
    GroupName:
      description: "Group name. There are two built-in groups: `users` for all users,\
        \ and `admins` for administrators."
      example: users
      title: GroupName
      type: string
    ServicePrincipalName:
      description: Name of an Azure service principal.
      example: 9f0621ee-b52b-11ea-b3de-0242ac130004
      type: string
    PermissionLevel:
      description: Permission level to grant.
      oneOf:
      - $ref: '#/components/schemas/CanManage'
      - $ref: '#/components/schemas/CanManageRun'
      - $ref: '#/components/schemas/CanView'
      - $ref: '#/components/schemas/IsOwner'
      title: PermissionLevel
    PermissionLevelForGroup:
      description: Permission level to grant.
      oneOf:
      - $ref: '#/components/schemas/CanManage'
      - $ref: '#/components/schemas/CanManageRun'
      - $ref: '#/components/schemas/CanView'
      title: PermissionLevelForGroup
    CanManage:
      description: Permission to manage the job.
      enum:
      - CAN_MANAGE
      title: CanManage
      type: string
    CanManageRun:
      description: Permission to run and/or manage runs for the job.
      enum:
      - CAN_MANAGE_RUN
      title: CanManageRun
      type: string
    CanView:
      description: Permission to view the settings of the job.
      enum:
      - CAN_VIEW
      title: CanView
      type: string
    IsOwner:
      description: Perimssion that represents ownership of the job.
      enum:
      - IS_OWNER
      title: IsOwner
      type: string
    RunSubmitTaskSettings:
      properties:
        task_key:
          description: "A unique name for the task. This field is used to refer to\
            \ this task from other tasks.\nThis field is required and must be unique\
            \ within its parent job.\nOn Update or Reset, this field is used to reference\
            \ the tasks to be updated or reset.\nThe maximum length is 100 characters."
          example: Task_Key
          maxLength: 100
          minLength: 1
          pattern: "^[\\w\\-]+$"
          title: TaskKey
          type: string
        depends_on:
          description: "An optional array of objects specifying the dependency graph\
            \ of the task. All tasks specified in this field must complete successfully\
            \ before executing this task.\nThe key is `task_key`, and the value is\
            \ the name assigned to the dependent task.\nThis field is required when\
            \ a job consists of more than one task."
          example:
          - task_key: Previous_Task_Key
          - task_key: Other_Task_Key
          items:
            $ref: '#/components/schemas/TaskDependencies_inner'
          title: TaskDependencies
          type: array
        existing_cluster_id:
          description: "If existing_cluster_id, the ID of an existing cluster that\
            \ is used for all runs of this task. When running tasks on an existing\
            \ cluster, you may need to manually restart the cluster if it stops responding.\
            \ We suggest running jobs on new clusters for greater reliability."
          example: 0923-164208-meows279
          title: existing_cluster_id
          type: string
        new_cluster:
          $ref: '#/components/schemas/NewCluster'
        notebook_task:
          $ref: '#/components/schemas/NotebookTask'
        spark_jar_task:
          $ref: '#/components/schemas/SparkJarTask'
        spark_python_task:
          $ref: '#/components/schemas/SparkPythonTask'
        spark_submit_task:
          $ref: '#/components/schemas/SparkSubmitTask'
        pipeline_task:
          $ref: '#/components/schemas/PipelineTask'
        python_wheel_task:
          $ref: '#/components/schemas/PythonWheelTask'
        libraries:
          description: An optional list of libraries to be installed on the cluster
            that executes the task. The default value is an empty list.
          items:
            $ref: '#/components/schemas/Library'
          title: libraries
          type: array
        timeout_seconds:
          description: An optional timeout applied to each run of this job task. The
            default behavior is to have no timeout.
          example: 86400
          format: int32
          title: timeout_seconds
          type: integer
      required:
      - task_key
      title: RunSubmitTaskSettings
    RunSubmitSettings:
      properties:
        tasks:
          example:
          - task_key: Sessionize
            description: Extracts session data from events
            depends_on: []
            existing_cluster_id: 0923-164208-meows279
            spark_jar_task:
              main_class_name: com.databricks.Sessionize
              parameters:
              - --data
              - dbfs:/path/to/data.json
            libraries:
            - jar: dbfs:/mnt/databricks/Sessionize.jar
            timeout_seconds: 86400
          - task_key: Orders_Ingest
            description: Ingests order data
            depends_on: []
            existing_cluster_id: 0923-164208-meows279
            spark_jar_task:
              main_class_name: com.databricks.OrdersIngest
              parameters:
              - --data
              - dbfs:/path/to/order-data.json
            libraries:
            - jar: dbfs:/mnt/databricks/OrderIngest.jar
            timeout_seconds: 86400
          - task_key: Match
            description: Matches orders with user sessions
            depends_on:
            - task_key: Orders_Ingest
            - task_key: Sessionize
            new_cluster:
              spark_version: 7.3.x-scala2.12
              node_type_id: i3.xlarge
              spark_conf:
                spark.speculation: true
              aws_attributes:
                availability: SPOT
                zone_id: us-west-2a
              autoscale:
                min_workers: 2
                max_workers: 16
            notebook_task:
              notebook_path: /Users/user.name@databricks.com/Match
              base_parameters:
                name: John Doe
                age: "35"
            timeout_seconds: 86400
          items:
            $ref: '#/components/schemas/RunSubmitTaskSettings'
          maxItems: 100
          title: tasks
          type: array
        run_name:
          description: An optional name for the run. The default value is `Untitled`.
          example: A multitask job run
          title: run_name
          type: string
        git_source:
          $ref: '#/components/schemas/GitSource'
        timeout_seconds:
          description: An optional timeout applied to each run of this job. The default
            behavior is to have no timeout.
          example: 86400
          format: int32
          title: timeout_seconds
          type: integer
        idempotency_token:
          description: "An optional token that can be used to guarantee the idempotency\
            \ of job run requests. If a run with the provided token already exists,\
            \ the request does not create a new run but returns the ID of the existing\
            \ run instead.\n\nIf you specify the idempotency token, upon failure you\
            \ can retry until the request succeeds. Databricks guarantees that exactly\
            \ one run is launched with that idempotency token.\n\nThis token must\
            \ have at most 64 characters.\n\nFor more information, see [How to ensure\
            \ idempotency for jobs](https://kb.databricks.com/jobs/jobs-idempotency.html)."
          example: 8f018174-4792-40d5-bcbc-3e6a527352c8
          title: idempotency_token
          type: string
      title: RunSubmitSettings
      type: object
    RunNowInput:
      properties:
        job_id:
          description: The ID of the job to be executed
          example: 11223344
          format: int64
          title: job_id
          type: integer
        idempotency_token:
          description: "An optional token to guarantee the idempotency of job run\
            \ requests. If a run with the provided token already exists, the request\
            \ does not create a new run but returns the ID of the existing run instead.\n\
            \nIf you specify the idempotency token, upon failure you can retry until\
            \ the request succeeds. Databricks guarantees that exactly one run is\
            \ launched with that idempotency token.\n\nThis token must have at most\
            \ 64 characters.\n\nFor more information, see [How to ensure idempotency\
            \ for jobs](https://kb.databricks.com/jobs/jobs-idempotency.html)."
          example: 8f018174-4792-40d5-bcbc-3e6a527352c8
          title: idempotency_token
          type: string
      title: RunNowInput
      type: object
    RepairRunInput:
      properties:
        run_id:
          description: The job run ID of the run to repair. The run must not be in
            progress.
          example: 455644833
          format: int64
          title: run_id
          type: integer
        rerun_tasks:
          description: The task keys of the task runs to repair.
          example:
          - task0
          - task1
          items:
            type: string
          title: rerun_tasks
          type: array
        latest_repair_id:
          description: "The ID of the latest repair. This parameter is not required\
            \ when repairing a run for the first time, but must be provided on subsequent\
            \ requests to repair the same run."
          example: 734650698524280
          format: int64
          title: latest_repair_id
          type: integer
      title: RepairRunInput
      type: object
    RepairHistory:
      properties:
        repair_history:
          description: The repair history of the run.
          items:
            $ref: '#/components/schemas/RepairHistoryItem'
          title: repair_history
          type: array
      title: RepairHistory
      type: object
    RepairHistoryItem:
      properties:
        type:
          description: The repair history item type. Indicates whether a run is the
            original run or a repair run.
          enum:
          - ORIGINAL
          - REPAIR
          title: type
          type: string
        start_time:
          description: The start time of the (repaired) run.
          example: 1625060460483
          format: int64
          title: start_time
          type: integer
        end_time:
          description: The end time of the (repaired) run.
          example: 1625060863413
          format: int64
          title: end_time
          type: integer
        state:
          $ref: '#/components/schemas/RunState'
        id:
          description: The ID of the repair. Only returned for the items that represent
            a repair in `repair_history`.
          example: 734650698524280
          format: int64
          title: id
          type: integer
        task_run_ids:
          description: The run IDs of the task runs that ran as part of this repair
            history item.
          example:
          - 1106460542112844
          - 988297789683452
          items:
            format: int64
            type: integer
          title: task_run_ids
          type: array
      title: RepairHistoryItem
      type: object
    JobsCreate_request:
      allOf:
      - $ref: '#/components/schemas/JobSettings'
      - $ref: '#/components/schemas/AccessControlList'
      title: JobsCreate_request
    JobsCreate_200_response:
      example:
        job_id: 11223344
      properties:
        job_id:
          description: The canonical identifier for the newly created job.
          example: 11223344
          format: int64
          title: job_id
          type: integer
      title: JobsCreate_200_response
      type: object
    JobsList_200_response:
      example:
        jobs:
        - settings:
            schedule:
              quartz_cron_expression: 20 30 * * * ?
              timezone_id: Europe/London
              pause_status: PAUSED
            max_concurrent_runs: 10
            name: A multitask job
            format: MULTI_TASK
            email_notifications:
              on_failure:
              - user.name@databricks.com
              no_alert_for_skipped_runs: false
              on_start:
              - user.name@databricks.com
              on_success:
              - user.name@databricks.com
            job_clusters:
            - job_cluster_key: auto_scaling_cluster
              new_cluster:
                spark_version: 7.3.x-scala2.12
                node_type_id: i3.xlarge
                spark_conf:
                  spark.speculation: true
                aws_attributes:
                  availability: SPOT
                  zone_id: us-west-2a
                autoscale:
                  min_workers: 2
                  max_workers: 16
            timeout_seconds: 86400
            tasks:
            - task_key: Sessionize
              description: Extracts session data from events
              depends_on: []
              existing_cluster_id: 0923-164208-meows279
              spark_jar_task:
                main_class_name: com.databricks.Sessionize
                parameters:
                - --data
                - dbfs:/path/to/data.json
              libraries:
              - jar: dbfs:/mnt/databricks/Sessionize.jar
              timeout_seconds: 86400
              max_retries: 3
              min_retry_interval_millis: 2000
              retry_on_timeout: false
            - task_key: Orders_Ingest
              description: Ingests order data
              depends_on: []
              job_cluster_key: auto_scaling_cluster
              spark_jar_task:
                main_class_name: com.databricks.OrdersIngest
                parameters:
                - --data
                - dbfs:/path/to/order-data.json
              libraries:
              - jar: dbfs:/mnt/databricks/OrderIngest.jar
              timeout_seconds: 86400
              max_retries: 3
              min_retry_interval_millis: 2000
              retry_on_timeout: false
            - task_key: Match
              description: Matches orders with user sessions
              depends_on:
              - task_key: Orders_Ingest
              - task_key: Sessionize
              new_cluster:
                spark_version: 7.3.x-scala2.12
                node_type_id: i3.xlarge
                spark_conf:
                  spark.speculation: true
                aws_attributes:
                  availability: SPOT
                  zone_id: us-west-2a
                autoscale:
                  min_workers: 2
                  max_workers: 16
              notebook_task:
                notebook_path: /Users/user.name@databricks.com/Match
                base_parameters:
                  name: John Doe
                  age: "35"
              timeout_seconds: 86400
              max_retries: 3
              min_retry_interval_millis: 2000
              retry_on_timeout: false
            git_source:
              git_commit: e0056d01
              git_tag: release-1.0.0
              git_provider: github
              git_branch: main
              git_url: https://github.com/databricks/databricks-cli
              git_snapshot:
                used_commit: 4506fdf41e9fa98090570a34df7a5bce163ff15f
            tags:
              cost-center: engineering
              team: jobs
          created_time: 1601370337343
          creator_user_name: user.name@databricks.com
          job_id: 11223344
          run_as_user_name: user.name@databricks.com
        - settings:
            schedule:
              quartz_cron_expression: 20 30 * * * ?
              timezone_id: Europe/London
              pause_status: PAUSED
            max_concurrent_runs: 10
            name: A multitask job
            format: MULTI_TASK
            email_notifications:
              on_failure:
              - user.name@databricks.com
              no_alert_for_skipped_runs: false
              on_start:
              - user.name@databricks.com
              on_success:
              - user.name@databricks.com
            job_clusters:
            - job_cluster_key: auto_scaling_cluster
              new_cluster:
                spark_version: 7.3.x-scala2.12
                node_type_id: i3.xlarge
                spark_conf:
                  spark.speculation: true
                aws_attributes:
                  availability: SPOT
                  zone_id: us-west-2a
                autoscale:
                  min_workers: 2
                  max_workers: 16
            timeout_seconds: 86400
            tasks:
            - task_key: Sessionize
              description: Extracts session data from events
              depends_on: []
              existing_cluster_id: 0923-164208-meows279
              spark_jar_task:
                main_class_name: com.databricks.Sessionize
                parameters:
                - --data
                - dbfs:/path/to/data.json
              libraries:
              - jar: dbfs:/mnt/databricks/Sessionize.jar
              timeout_seconds: 86400
              max_retries: 3
              min_retry_interval_millis: 2000
              retry_on_timeout: false
            - task_key: Orders_Ingest
              description: Ingests order data
              depends_on: []
              job_cluster_key: auto_scaling_cluster
              spark_jar_task:
                main_class_name: com.databricks.OrdersIngest
                parameters:
                - --data
                - dbfs:/path/to/order-data.json
              libraries:
              - jar: dbfs:/mnt/databricks/OrderIngest.jar
              timeout_seconds: 86400
              max_retries: 3
              min_retry_interval_millis: 2000
              retry_on_timeout: false
            - task_key: Match
              description: Matches orders with user sessions
              depends_on:
              - task_key: Orders_Ingest
              - task_key: Sessionize
              new_cluster:
                spark_version: 7.3.x-scala2.12
                node_type_id: i3.xlarge
                spark_conf:
                  spark.speculation: true
                aws_attributes:
                  availability: SPOT
                  zone_id: us-west-2a
                autoscale:
                  min_workers: 2
                  max_workers: 16
              notebook_task:
                notebook_path: /Users/user.name@databricks.com/Match
                base_parameters:
                  name: John Doe
                  age: "35"
              timeout_seconds: 86400
              max_retries: 3
              min_retry_interval_millis: 2000
              retry_on_timeout: false
            git_source:
              git_commit: e0056d01
              git_tag: release-1.0.0
              git_provider: github
              git_branch: main
              git_url: https://github.com/databricks/databricks-cli
              git_snapshot:
                used_commit: 4506fdf41e9fa98090570a34df7a5bce163ff15f
            tags:
              cost-center: engineering
              team: jobs
          created_time: 1601370337343
          creator_user_name: user.name@databricks.com
          job_id: 11223344
          run_as_user_name: user.name@databricks.com
        has_more: false
      properties:
        jobs:
          description: The list of jobs.
          items:
            $ref: '#/components/schemas/Job'
          title: jobs
          type: array
        has_more:
          example: false
          title: has_more
          type: boolean
      title: JobsList_200_response
      type: object
    JobsGet_200_response:
      example:
        settings:
          schedule:
            quartz_cron_expression: 20 30 * * * ?
            timezone_id: Europe/London
            pause_status: PAUSED
          max_concurrent_runs: 10
          name: A multitask job
          format: MULTI_TASK
          email_notifications:
            on_failure:
            - user.name@databricks.com
            no_alert_for_skipped_runs: false
            on_start:
            - user.name@databricks.com
            on_success:
            - user.name@databricks.com
          job_clusters:
          - job_cluster_key: auto_scaling_cluster
            new_cluster:
              spark_version: 7.3.x-scala2.12
              node_type_id: i3.xlarge
              spark_conf:
                spark.speculation: true
              aws_attributes:
                availability: SPOT
                zone_id: us-west-2a
              autoscale:
                min_workers: 2
                max_workers: 16
          timeout_seconds: 86400
          tasks:
          - task_key: Sessionize
            description: Extracts session data from events
            depends_on: []
            existing_cluster_id: 0923-164208-meows279
            spark_jar_task:
              main_class_name: com.databricks.Sessionize
              parameters:
              - --data
              - dbfs:/path/to/data.json
            libraries:
            - jar: dbfs:/mnt/databricks/Sessionize.jar
            timeout_seconds: 86400
            max_retries: 3
            min_retry_interval_millis: 2000
            retry_on_timeout: false
          - task_key: Orders_Ingest
            description: Ingests order data
            depends_on: []
            job_cluster_key: auto_scaling_cluster
            spark_jar_task:
              main_class_name: com.databricks.OrdersIngest
              parameters:
              - --data
              - dbfs:/path/to/order-data.json
            libraries:
            - jar: dbfs:/mnt/databricks/OrderIngest.jar
            timeout_seconds: 86400
            max_retries: 3
            min_retry_interval_millis: 2000
            retry_on_timeout: false
          - task_key: Match
            description: Matches orders with user sessions
            depends_on:
            - task_key: Orders_Ingest
            - task_key: Sessionize
            new_cluster:
              spark_version: 7.3.x-scala2.12
              node_type_id: i3.xlarge
              spark_conf:
                spark.speculation: true
              aws_attributes:
                availability: SPOT
                zone_id: us-west-2a
              autoscale:
                min_workers: 2
                max_workers: 16
            notebook_task:
              notebook_path: /Users/user.name@databricks.com/Match
              base_parameters:
                name: John Doe
                age: "35"
            timeout_seconds: 86400
            max_retries: 3
            min_retry_interval_millis: 2000
            retry_on_timeout: false
          git_source:
            git_commit: e0056d01
            git_tag: release-1.0.0
            git_provider: github
            git_branch: main
            git_url: https://github.com/databricks/databricks-cli
            git_snapshot:
              used_commit: 4506fdf41e9fa98090570a34df7a5bce163ff15f
          tags:
            cost-center: engineering
            team: jobs
        created_time: 1601370337343
        creator_user_name: user.name@databricks.com
        job_id: 11223344
        run_as_user_name: user.name@databricks.com
      properties:
        job_id:
          description: The canonical identifier for this job.
          example: 11223344
          format: int64
          title: job_id
          type: integer
        creator_user_name:
          description: The creator user name. This field won’t be included in the
            response if the user has been deleted.
          example: user.name@databricks.com
          title: creator_user_name
          type: string
        run_as_user_name:
          description: "The user name that the job runs as. `run_as_user_name` is\
            \ based on the current job settings, and is set to the creator of the\
            \ job if job access control is disabled, or the `is_owner` permission\
            \ if job access control is enabled."
          example: user.name@databricks.com
          title: run_as_user_name
          type: string
        settings:
          $ref: '#/components/schemas/JobSettings'
        created_time:
          description: The time at which this job was created in epoch milliseconds
            (milliseconds since 1/1/1970 UTC).
          example: 1601370337343
          format: int64
          title: created_time
          type: integer
      title: JobsGet_200_response
      type: object
    JobsReset_request:
      properties:
        job_id:
          description: The canonical identifier of the job to reset. This field is
            required.
          example: 11223344
          format: int64
          title: job_id
          type: integer
        new_settings:
          $ref: '#/components/schemas/JobSettings'
      required:
      - job_id
      title: JobsReset_request
      type: object
    JobsUpdate_request:
      properties:
        job_id:
          description: The canonical identifier of the job to update. This field is
            required.
          example: 11223344
          format: int64
          title: job_id
          type: integer
        new_settings:
          $ref: '#/components/schemas/JobSettings'
        fields_to_remove:
          description: Remove top-level fields in the job settings. Removing nested
            fields is not supported. This field is optional.
          example:
          - libraries
          - schedule
          items:
            type: string
          title: fields_to_remove
          type: array
      required:
      - job_id
      title: JobsUpdate_request
      type: object
    JobsDelete_request:
      properties:
        job_id:
          description: The canonical identifier of the job to delete. This field is
            required.
          example: 11223344
          format: int64
          title: job_id
          type: integer
      required:
      - job_id
      title: JobsDelete_request
      type: object
    JobsRunNow_request:
      allOf:
      - $ref: '#/components/schemas/RunNowInput'
      - $ref: '#/components/schemas/RunParameters'
      title: JobsRunNow_request
    JobsRunNow_200_response:
      example:
        number_in_job: 455644833
        run_id: 455644833
      properties:
        run_id:
          description: The globally unique ID of the newly triggered run.
          example: 455644833
          format: int64
          title: run_id
          type: integer
        number_in_job:
          deprecated: true
          description: A unique identifier for this job run. This is set to the same
            value as `run_id`.
          example: 455644833
          format: int64
          title: number_in_job
          type: integer
      title: JobsRunNow_200_response
      type: object
    JobsRunsSubmit_request:
      allOf:
      - $ref: '#/components/schemas/RunSubmitSettings'
      - $ref: '#/components/schemas/AccessControlList'
      title: JobsRunsSubmit_request
    JobsRunsSubmit_200_response:
      example:
        run_id: 455644833
      properties:
        run_id:
          description: The canonical identifier for the newly submitted run.
          example: 455644833
          format: int64
          title: run_id
          type: integer
      title: JobsRunsSubmit_200_response
      type: object
    JobsRunsList_200_response:
      example:
        has_more: true
        runs:
        - number_in_job: 455644833
          run_id: 455644833
          original_attempt_run_id: 455644833
          cluster_instance:
            cluster_id: 0923-164208-meows279
            spark_context_id: spark_context_id
          run_page_url: https://my-workspace.cloud.databricks.com/#job/11223344/run/123
          end_time: 1625060863413
          cleanup_duration: 0
          trigger: null
          run_type: JOB_RUN
          git_source:
            git_commit: e0056d01
            git_tag: release-1.0.0
            git_provider: github
            git_branch: main
            git_url: https://github.com/databricks/databricks-cli
            git_snapshot:
              used_commit: 4506fdf41e9fa98090570a34df7a5bce163ff15f
          schedule:
            quartz_cron_expression: 20 30 * * * ?
            timezone_id: Europe/London
            pause_status: PAUSED
          start_time: 1625060460483
          cluster_spec:
            new_cluster:
              spark_conf:
                key: ""
              cluster_log_conf:
                s3:
                  encryption_type: encryption_type
                  enable_encryption: true
                  endpoint: endpoint
                  canned_acl: canned_acl
                  destination: destination
                  region: region
                  kms_key: kms_key
                dbfs:
                  destination: destination
              enable_elastic_disk: true
              policy_id: policy_id
              spark_env_vars:
                key: ""
              node_type_id: node_type_id
              ssh_public_keys:
              - ssh_public_keys
              - ssh_public_keys
              custom_tags:
                key: custom_tags
              num_workers: 0
              autoscale:
                max_workers: 1
                min_workers: 6
              instance_pool_id: instance_pool_id
              aws_attributes:
                ebs_volume_count: 2
                zone_id: zone_id
                ebs_volume_size: 7
                ebs_volume_throughput: 3
                spot_bid_price_percent: 5
                availability: SPOT
                ebs_volume_type: GENERAL_PURPOSE_SSD
                first_on_demand: 5
                ebs_volume_iops: 9
                instance_profile_arn: instance_profile_arn
              spark_version: spark_version
              driver_node_type_id: driver_node_type_id
              init_scripts:
              - S3:
                  encryption_type: encryption_type
                  enable_encryption: true
                  endpoint: endpoint
                  canned_acl: canned_acl
                  destination: destination
                  region: region
                  kms_key: kms_key
                file:
                  destination: destination
                dbfs:
                  destination: destination
              - S3:
                  encryption_type: encryption_type
                  enable_encryption: true
                  endpoint: endpoint
                  canned_acl: canned_acl
                  destination: destination
                  region: region
                  kms_key: kms_key
                file:
                  destination: destination
                dbfs:
                  destination: destination
              driver_instance_pool_id: driver_instance_pool_id
            libraries:
            - cran:
                package: geojson
                repo: https://my-repo.com
              egg: dbfs:/my/egg
              pypi:
                package: simplejson==3.8.0
                repo: https://my-repo.com
              maven:
                repo: https://my-repo.com
                coordinates: org.jsoup:jsoup:1.7.2
                exclusions:
                - slf4j:slf4j
                - '*:hadoop-client'
              jar: dbfs:/my-jar.jar
              whl: dbfs:/my/whl
            - cran:
                package: geojson
                repo: https://my-repo.com
              egg: dbfs:/my/egg
              pypi:
                package: simplejson==3.8.0
                repo: https://my-repo.com
              maven:
                repo: https://my-repo.com
                coordinates: org.jsoup:jsoup:1.7.2
                exclusions:
                - slf4j:slf4j
                - '*:hadoop-client'
              jar: dbfs:/my-jar.jar
              whl: dbfs:/my/whl
            existing_cluster_id: 0923-164208-meows279
          attempt_number: 0
          creator_user_name: user.name@databricks.com
          setup_duration: 0
          execution_duration: 0
          job_id: 11223344
          job_clusters:
          - job_cluster_key: auto_scaling_cluster
            new_cluster:
              spark_version: 7.3.x-scala2.12
              node_type_id: i3.xlarge
              spark_conf:
                spark.speculation: true
              aws_attributes:
                availability: SPOT
                zone_id: us-west-2a
              autoscale:
                min_workers: 2
                max_workers: 16
          state:
            user_cancelled_or_timedout: false
            life_cycle_state: null
            result_state: null
            state_message: ""
          overriding_parameters:
            python_params:
            - john doe
            - "35"
            python_named_params:
              name: task
              data: dbfs:/path/to/data.json
            notebook_params:
              name: john doe
              age: "35"
            jar_params:
            - john
            - doe
            - "35"
            spark_submit_params:
            - --class
            - org.apache.spark.examples.SparkPi
          run_name: A multitask job run
          tasks:
          - run_id: 2112892
            task_key: Orders_Ingest
            description: Ingests order data
            job_cluster_key: auto_scaling_cluster
            spark_jar_task:
              main_class_name: com.databricks.OrdersIngest
              run_as_repl: true
            libraries:
            - jar: dbfs:/mnt/databricks/OrderIngest.jar
            state:
              life_cycle_state: INTERNAL_ERROR
              result_state: FAILED
              state_message: |-
                Library installation failed for library due to user error. Error messages:
                'Manage' permissions are required to install libraries on a cluster
              user_cancelled_or_timedout: false
            run_page_url: https://my-workspace.cloud.databricks.com/#job/39832/run/20
            start_time: 1629989929660
            setup_duration: 0
            execution_duration: 0
            cleanup_duration: 0
            end_time: 1629989930171
            cluster_instance:
              cluster_id: 0923-164208-meows279
              spark_context_id: "4348585301701786933"
            attempt_number: 0
          - run_id: 2112897
            task_key: Match
            description: Matches orders with user sessions
            depends_on:
            - task_key: Orders_Ingest
            - task_key: Sessionize
            new_cluster:
              spark_version: 7.3.x-scala2.12
              node_type_id: i3.xlarge
              spark_conf:
                spark.speculation: true
              aws_attributes:
                availability: SPOT
                zone_id: us-west-2a
              autoscale:
                min_workers: 2
                max_workers: 16
            notebook_task:
              notebook_path: /Users/user.name@databricks.com/Match
            state:
              life_cycle_state: SKIPPED
              state_message: An upstream task failed.
              user_cancelled_or_timedout: false
            run_page_url: https://my-workspace.cloud.databricks.com/#job/39832/run/21
            start_time: 0
            setup_duration: 0
            execution_duration: 0
            cleanup_duration: 0
            end_time: 1629989930238
            cluster_instance:
              cluster_id: 0923-164208-meows279
            attempt_number: 0
          - run_id: 2112902
            task_key: Sessionize
            description: Extracts session data from events
            existing_cluster_id: 0923-164208-meows279
            spark_jar_task:
              main_class_name: com.databricks.Sessionize
              run_as_repl: true
            libraries:
            - jar: dbfs:/mnt/databricks/Sessionize.jar
            state:
              life_cycle_state: INTERNAL_ERROR
              result_state: FAILED
              state_message: |-
                Library installation failed for library due to user error. Error messages:
                'Manage' permissions are required to install libraries on a cluster
              user_cancelled_or_timedout: false
            run_page_url: https://my-workspace.cloud.databricks.com/#job/39832/run/22
            start_time: 1629989929668
            setup_duration: 0
            execution_duration: 0
            cleanup_duration: 0
            end_time: 1629989930144
            cluster_instance:
              cluster_id: 0923-164208-meows279
              spark_context_id: "4348585301701786933"
            attempt_number: 0
        - number_in_job: 455644833
          run_id: 455644833
          original_attempt_run_id: 455644833
          cluster_instance:
            cluster_id: 0923-164208-meows279
            spark_context_id: spark_context_id
          run_page_url: https://my-workspace.cloud.databricks.com/#job/11223344/run/123
          end_time: 1625060863413
          cleanup_duration: 0
          trigger: null
          run_type: JOB_RUN
          git_source:
            git_commit: e0056d01
            git_tag: release-1.0.0
            git_provider: github
            git_branch: main
            git_url: https://github.com/databricks/databricks-cli
            git_snapshot:
              used_commit: 4506fdf41e9fa98090570a34df7a5bce163ff15f
          schedule:
            quartz_cron_expression: 20 30 * * * ?
            timezone_id: Europe/London
            pause_status: PAUSED
          start_time: 1625060460483
          cluster_spec:
            new_cluster:
              spark_conf:
                key: ""
              cluster_log_conf:
                s3:
                  encryption_type: encryption_type
                  enable_encryption: true
                  endpoint: endpoint
                  canned_acl: canned_acl
                  destination: destination
                  region: region
                  kms_key: kms_key
                dbfs:
                  destination: destination
              enable_elastic_disk: true
              policy_id: policy_id
              spark_env_vars:
                key: ""
              node_type_id: node_type_id
              ssh_public_keys:
              - ssh_public_keys
              - ssh_public_keys
              custom_tags:
                key: custom_tags
              num_workers: 0
              autoscale:
                max_workers: 1
                min_workers: 6
              instance_pool_id: instance_pool_id
              aws_attributes:
                ebs_volume_count: 2
                zone_id: zone_id
                ebs_volume_size: 7
                ebs_volume_throughput: 3
                spot_bid_price_percent: 5
                availability: SPOT
                ebs_volume_type: GENERAL_PURPOSE_SSD
                first_on_demand: 5
                ebs_volume_iops: 9
                instance_profile_arn: instance_profile_arn
              spark_version: spark_version
              driver_node_type_id: driver_node_type_id
              init_scripts:
              - S3:
                  encryption_type: encryption_type
                  enable_encryption: true
                  endpoint: endpoint
                  canned_acl: canned_acl
                  destination: destination
                  region: region
                  kms_key: kms_key
                file:
                  destination: destination
                dbfs:
                  destination: destination
              - S3:
                  encryption_type: encryption_type
                  enable_encryption: true
                  endpoint: endpoint
                  canned_acl: canned_acl
                  destination: destination
                  region: region
                  kms_key: kms_key
                file:
                  destination: destination
                dbfs:
                  destination: destination
              driver_instance_pool_id: driver_instance_pool_id
            libraries:
            - cran:
                package: geojson
                repo: https://my-repo.com
              egg: dbfs:/my/egg
              pypi:
                package: simplejson==3.8.0
                repo: https://my-repo.com
              maven:
                repo: https://my-repo.com
                coordinates: org.jsoup:jsoup:1.7.2
                exclusions:
                - slf4j:slf4j
                - '*:hadoop-client'
              jar: dbfs:/my-jar.jar
              whl: dbfs:/my/whl
            - cran:
                package: geojson
                repo: https://my-repo.com
              egg: dbfs:/my/egg
              pypi:
                package: simplejson==3.8.0
                repo: https://my-repo.com
              maven:
                repo: https://my-repo.com
                coordinates: org.jsoup:jsoup:1.7.2
                exclusions:
                - slf4j:slf4j
                - '*:hadoop-client'
              jar: dbfs:/my-jar.jar
              whl: dbfs:/my/whl
            existing_cluster_id: 0923-164208-meows279
          attempt_number: 0
          creator_user_name: user.name@databricks.com
          setup_duration: 0
          execution_duration: 0
          job_id: 11223344
          job_clusters:
          - job_cluster_key: auto_scaling_cluster
            new_cluster:
              spark_version: 7.3.x-scala2.12
              node_type_id: i3.xlarge
              spark_conf:
                spark.speculation: true
              aws_attributes:
                availability: SPOT
                zone_id: us-west-2a
              autoscale:
                min_workers: 2
                max_workers: 16
          state:
            user_cancelled_or_timedout: false
            life_cycle_state: null
            result_state: null
            state_message: ""
          overriding_parameters:
            python_params:
            - john doe
            - "35"
            python_named_params:
              name: task
              data: dbfs:/path/to/data.json
            notebook_params:
              name: john doe
              age: "35"
            jar_params:
            - john
            - doe
            - "35"
            spark_submit_params:
            - --class
            - org.apache.spark.examples.SparkPi
          run_name: A multitask job run
          tasks:
          - run_id: 2112892
            task_key: Orders_Ingest
            description: Ingests order data
            job_cluster_key: auto_scaling_cluster
            spark_jar_task:
              main_class_name: com.databricks.OrdersIngest
              run_as_repl: true
            libraries:
            - jar: dbfs:/mnt/databricks/OrderIngest.jar
            state:
              life_cycle_state: INTERNAL_ERROR
              result_state: FAILED
              state_message: |-
                Library installation failed for library due to user error. Error messages:
                'Manage' permissions are required to install libraries on a cluster
              user_cancelled_or_timedout: false
            run_page_url: https://my-workspace.cloud.databricks.com/#job/39832/run/20
            start_time: 1629989929660
            setup_duration: 0
            execution_duration: 0
            cleanup_duration: 0
            end_time: 1629989930171
            cluster_instance:
              cluster_id: 0923-164208-meows279
              spark_context_id: "4348585301701786933"
            attempt_number: 0
          - run_id: 2112897
            task_key: Match
            description: Matches orders with user sessions
            depends_on:
            - task_key: Orders_Ingest
            - task_key: Sessionize
            new_cluster:
              spark_version: 7.3.x-scala2.12
              node_type_id: i3.xlarge
              spark_conf:
                spark.speculation: true
              aws_attributes:
                availability: SPOT
                zone_id: us-west-2a
              autoscale:
                min_workers: 2
                max_workers: 16
            notebook_task:
              notebook_path: /Users/user.name@databricks.com/Match
            state:
              life_cycle_state: SKIPPED
              state_message: An upstream task failed.
              user_cancelled_or_timedout: false
            run_page_url: https://my-workspace.cloud.databricks.com/#job/39832/run/21
            start_time: 0
            setup_duration: 0
            execution_duration: 0
            cleanup_duration: 0
            end_time: 1629989930238
            cluster_instance:
              cluster_id: 0923-164208-meows279
            attempt_number: 0
          - run_id: 2112902
            task_key: Sessionize
            description: Extracts session data from events
            existing_cluster_id: 0923-164208-meows279
            spark_jar_task:
              main_class_name: com.databricks.Sessionize
              run_as_repl: true
            libraries:
            - jar: dbfs:/mnt/databricks/Sessionize.jar
            state:
              life_cycle_state: INTERNAL_ERROR
              result_state: FAILED
              state_message: |-
                Library installation failed for library due to user error. Error messages:
                'Manage' permissions are required to install libraries on a cluster
              user_cancelled_or_timedout: false
            run_page_url: https://my-workspace.cloud.databricks.com/#job/39832/run/22
            start_time: 1629989929668
            setup_duration: 0
            execution_duration: 0
            cleanup_duration: 0
            end_time: 1629989930144
            cluster_instance:
              cluster_id: 0923-164208-meows279
              spark_context_id: "4348585301701786933"
            attempt_number: 0
      properties:
        runs:
          description: "A list of runs, from most recently started to least."
          items:
            $ref: '#/components/schemas/Run'
          title: runs
          type: array
        has_more:
          description: "If true, additional runs matching the provided filter are\
            \ available for listing."
          title: has_more
          type: boolean
      title: JobsRunsList_200_response
      type: object
    JobsRunsGet_200_response:
      allOf:
      - $ref: '#/components/schemas/Run'
      - $ref: '#/components/schemas/RepairHistory'
      title: JobsRunsGet_200_response
    JobsRunsExport_200_response:
      example:
        views:
        - name: name
          type: null
          content: content
        - name: name
          type: null
          content: content
      properties:
        views:
          description: The exported content in HTML format (one for every view item).
          items:
            $ref: '#/components/schemas/ViewItem'
          title: views
          type: array
      title: JobsRunsExport_200_response
      type: object
    JobsRunsCancel_request:
      properties:
        run_id:
          description: This field is required.
          example: 455644833
          format: int64
          title: run_id
          type: integer
      required:
      - run_id
      title: JobsRunsCancel_request
      type: object
    JobsRunsGetOutput_200_response:
      example:
        logs_truncated: true
        metadata:
          number_in_job: 455644833
          run_id: 455644833
          original_attempt_run_id: 455644833
          cluster_instance:
            cluster_id: 0923-164208-meows279
            spark_context_id: spark_context_id
          run_page_url: https://my-workspace.cloud.databricks.com/#job/11223344/run/123
          end_time: 1625060863413
          cleanup_duration: 0
          trigger: null
          run_type: JOB_RUN
          git_source:
            git_commit: e0056d01
            git_tag: release-1.0.0
            git_provider: github
            git_branch: main
            git_url: https://github.com/databricks/databricks-cli
            git_snapshot:
              used_commit: 4506fdf41e9fa98090570a34df7a5bce163ff15f
          schedule:
            quartz_cron_expression: 20 30 * * * ?
            timezone_id: Europe/London
            pause_status: PAUSED
          start_time: 1625060460483
          cluster_spec:
            new_cluster:
              spark_conf:
                key: ""
              cluster_log_conf:
                s3:
                  encryption_type: encryption_type
                  enable_encryption: true
                  endpoint: endpoint
                  canned_acl: canned_acl
                  destination: destination
                  region: region
                  kms_key: kms_key
                dbfs:
                  destination: destination
              enable_elastic_disk: true
              policy_id: policy_id
              spark_env_vars:
                key: ""
              node_type_id: node_type_id
              ssh_public_keys:
              - ssh_public_keys
              - ssh_public_keys
              custom_tags:
                key: custom_tags
              num_workers: 0
              autoscale:
                max_workers: 1
                min_workers: 6
              instance_pool_id: instance_pool_id
              aws_attributes:
                ebs_volume_count: 2
                zone_id: zone_id
                ebs_volume_size: 7
                ebs_volume_throughput: 3
                spot_bid_price_percent: 5
                availability: SPOT
                ebs_volume_type: GENERAL_PURPOSE_SSD
                first_on_demand: 5
                ebs_volume_iops: 9
                instance_profile_arn: instance_profile_arn
              spark_version: spark_version
              driver_node_type_id: driver_node_type_id
              init_scripts:
              - S3:
                  encryption_type: encryption_type
                  enable_encryption: true
                  endpoint: endpoint
                  canned_acl: canned_acl
                  destination: destination
                  region: region
                  kms_key: kms_key
                file:
                  destination: destination
                dbfs:
                  destination: destination
              - S3:
                  encryption_type: encryption_type
                  enable_encryption: true
                  endpoint: endpoint
                  canned_acl: canned_acl
                  destination: destination
                  region: region
                  kms_key: kms_key
                file:
                  destination: destination
                dbfs:
                  destination: destination
              driver_instance_pool_id: driver_instance_pool_id
            libraries:
            - cran:
                package: geojson
                repo: https://my-repo.com
              egg: dbfs:/my/egg
              pypi:
                package: simplejson==3.8.0
                repo: https://my-repo.com
              maven:
                repo: https://my-repo.com
                coordinates: org.jsoup:jsoup:1.7.2
                exclusions:
                - slf4j:slf4j
                - '*:hadoop-client'
              jar: dbfs:/my-jar.jar
              whl: dbfs:/my/whl
            - cran:
                package: geojson
                repo: https://my-repo.com
              egg: dbfs:/my/egg
              pypi:
                package: simplejson==3.8.0
                repo: https://my-repo.com
              maven:
                repo: https://my-repo.com
                coordinates: org.jsoup:jsoup:1.7.2
                exclusions:
                - slf4j:slf4j
                - '*:hadoop-client'
              jar: dbfs:/my-jar.jar
              whl: dbfs:/my/whl
            existing_cluster_id: 0923-164208-meows279
          attempt_number: 0
          creator_user_name: user.name@databricks.com
          setup_duration: 0
          execution_duration: 0
          job_id: 11223344
          job_clusters:
          - job_cluster_key: auto_scaling_cluster
            new_cluster:
              spark_version: 7.3.x-scala2.12
              node_type_id: i3.xlarge
              spark_conf:
                spark.speculation: true
              aws_attributes:
                availability: SPOT
                zone_id: us-west-2a
              autoscale:
                min_workers: 2
                max_workers: 16
          state:
            user_cancelled_or_timedout: false
            life_cycle_state: null
            result_state: null
            state_message: ""
          overriding_parameters:
            python_params:
            - john doe
            - "35"
            python_named_params:
              name: task
              data: dbfs:/path/to/data.json
            notebook_params:
              name: john doe
              age: "35"
            jar_params:
            - john
            - doe
            - "35"
            spark_submit_params:
            - --class
            - org.apache.spark.examples.SparkPi
          run_name: A multitask job run
          tasks:
          - run_id: 2112892
            task_key: Orders_Ingest
            description: Ingests order data
            job_cluster_key: auto_scaling_cluster
            spark_jar_task:
              main_class_name: com.databricks.OrdersIngest
              run_as_repl: true
            libraries:
            - jar: dbfs:/mnt/databricks/OrderIngest.jar
            state:
              life_cycle_state: INTERNAL_ERROR
              result_state: FAILED
              state_message: |-
                Library installation failed for library due to user error. Error messages:
                'Manage' permissions are required to install libraries on a cluster
              user_cancelled_or_timedout: false
            run_page_url: https://my-workspace.cloud.databricks.com/#job/39832/run/20
            start_time: 1629989929660
            setup_duration: 0
            execution_duration: 0
            cleanup_duration: 0
            end_time: 1629989930171
            cluster_instance:
              cluster_id: 0923-164208-meows279
              spark_context_id: "4348585301701786933"
            attempt_number: 0
          - run_id: 2112897
            task_key: Match
            description: Matches orders with user sessions
            depends_on:
            - task_key: Orders_Ingest
            - task_key: Sessionize
            new_cluster:
              spark_version: 7.3.x-scala2.12
              node_type_id: i3.xlarge
              spark_conf:
                spark.speculation: true
              aws_attributes:
                availability: SPOT
                zone_id: us-west-2a
              autoscale:
                min_workers: 2
                max_workers: 16
            notebook_task:
              notebook_path: /Users/user.name@databricks.com/Match
            state:
              life_cycle_state: SKIPPED
              state_message: An upstream task failed.
              user_cancelled_or_timedout: false
            run_page_url: https://my-workspace.cloud.databricks.com/#job/39832/run/21
            start_time: 0
            setup_duration: 0
            execution_duration: 0
            cleanup_duration: 0
            end_time: 1629989930238
            cluster_instance:
              cluster_id: 0923-164208-meows279
            attempt_number: 0
          - run_id: 2112902
            task_key: Sessionize
            description: Extracts session data from events
            existing_cluster_id: 0923-164208-meows279
            spark_jar_task:
              main_class_name: com.databricks.Sessionize
              run_as_repl: true
            libraries:
            - jar: dbfs:/mnt/databricks/Sessionize.jar
            state:
              life_cycle_state: INTERNAL_ERROR
              result_state: FAILED
              state_message: |-
                Library installation failed for library due to user error. Error messages:
                'Manage' permissions are required to install libraries on a cluster
              user_cancelled_or_timedout: false
            run_page_url: https://my-workspace.cloud.databricks.com/#job/39832/run/22
            start_time: 1629989929668
            setup_duration: 0
            execution_duration: 0
            cleanup_duration: 0
            end_time: 1629989930144
            cluster_instance:
              cluster_id: 0923-164208-meows279
              spark_context_id: "4348585301701786933"
            attempt_number: 0
        notebook_output:
          result: An arbitrary string passed by calling dbutils.notebook.exit(...)
          truncated: false
        error_trace: "---------------------------------------------------------------------------\n\
          Exception                                 Traceback (most recent call last)\n\
          \      1 numerator = 42\n      2 denominator = 0\n----> 3 return numerator\
          \ / denominator\n\nZeroDivisionError: integer division or modulo by zero"
        error: "ZeroDivisionError: integer division or modulo by zero"
        logs: Hello World!
      properties:
        notebook_output:
          $ref: '#/components/schemas/NotebookOutput'
        logs:
          description: "The output from tasks that write to cluster logs such as [SparkJarTask](https://docs.databricks.com/dev-tools/api/latest/jobs.html#/components/schemas/SparkJarTask)\
            \ or [SparkPythonTask](https://docs.databricks.com/dev-tools/api/latest/jobs.html#/components/schemas/SparkPythonTask.\
            \ Databricks restricts this API to return the last 5 MB of these logs.\
            \ To return a larger result, use the [ClusterLogConf](https://docs.databricks.com/dev-tools/api/latest/clusters.html#clusterlogconf)\
            \ field to configure log storage for the job cluster."
          example: Hello World!
          title: logs
          type: string
        logs_truncated:
          description: Whether the logs are truncated.
          example: true
          title: logs_truncated
          type: boolean
        error:
          description: "An error message indicating why a task failed or why output\
            \ is not available. The message is unstructured, and its exact format\
            \ is subject to change."
          example: "ZeroDivisionError: integer division or modulo by zero"
          title: error
          type: string
        error_trace:
          description: "If there was an error executing the run, this field contains\
            \ any available stack traces."
          example: "---------------------------------------------------------------------------\n\
            Exception                                 Traceback (most recent call\
            \ last)\n      1 numerator = 42\n      2 denominator = 0\n----> 3 return\
            \ numerator / denominator\n\nZeroDivisionError: integer division or modulo\
            \ by zero"
          title: error_trace
          type: string
        metadata:
          $ref: '#/components/schemas/Run'
      title: JobsRunsGetOutput_200_response
      type: object
    JobsRunsDelete_request:
      properties:
        run_id:
          description: The canonical identifier of the run for which to retrieve the
            metadata.
          example: 455644833
          format: int64
          title: run_id
          type: integer
      title: JobsRunsDelete_request
      type: object
    JobsRunsRepair_request:
      allOf:
      - $ref: '#/components/schemas/RepairRunInput'
      - $ref: '#/components/schemas/RunParameters'
      title: JobsRunsRepair_request
    JobsRunsRepair_200_response:
      example:
        repair_id: 734650698524280
      properties:
        repair_id:
          description: The ID of the repair.
          example: 734650698524280
          format: int64
          title: repair_id
          type: integer
      title: JobsRunsRepair_200_response
      type: object
    TaskDependencies_inner:
      properties:
        task_key:
          title: task_key
          type: string
      title: TaskDependencies_inner
      type: object
  securitySchemes:
    bearerAuth:
      bearerFormat: api_token
      scheme: bearer
      type: http
      x-bearerInfoFunc: openapi_server.controllers.security_controller_.info_from_bearerAuth
