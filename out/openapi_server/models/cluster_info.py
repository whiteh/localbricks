# coding: utf-8

from __future__ import absolute_import
from datetime import date, datetime  # noqa: F401

from typing import List, Dict  # noqa: F401

from openapi_server.models.base_model_ import Model
from openapi_server.models.auto_scale import AutoScale
from openapi_server.models.aws_attributes import AwsAttributes
from openapi_server.models.cluster_log_conf import ClusterLogConf
from openapi_server.models.cluster_source import ClusterSource
from openapi_server.models.cluster_state import ClusterState
from openapi_server.models.docker_image import DockerImage
from openapi_server.models.init_script_info import InitScriptInfo
from openapi_server.models.log_sync_status import LogSyncStatus
from openapi_server.models.spark_node import SparkNode
from openapi_server.models.termination_reason import TerminationReason
from openapi_server import util

from openapi_server.models.auto_scale import AutoScale  # noqa: E501
from openapi_server.models.aws_attributes import AwsAttributes  # noqa: E501
from openapi_server.models.cluster_log_conf import ClusterLogConf  # noqa: E501
from openapi_server.models.cluster_source import ClusterSource  # noqa: E501
from openapi_server.models.cluster_state import ClusterState  # noqa: E501
from openapi_server.models.docker_image import DockerImage  # noqa: E501
from openapi_server.models.init_script_info import InitScriptInfo  # noqa: E501
from openapi_server.models.log_sync_status import LogSyncStatus  # noqa: E501
from openapi_server.models.spark_node import SparkNode  # noqa: E501
from openapi_server.models.termination_reason import TerminationReason  # noqa: E501

class ClusterInfo(Model):
    """NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).

    Do not edit the class manually.
    """

    def __init__(self, num_workers=None, autoscale=None, cluster_id=None, creator_user_name=None, driver=None, executors=None, spark_context_id=None, jdbc_port=None, cluster_name=None, spark_version=None, spark_conf=None, aws_attributes=None, node_type_id=None, driver_node_type_id=None, ssh_public_keys=None, custom_tags=None, cluster_log_conf=None, init_scripts=None, docker_image=None, spark_env_vars=None, autotermination_minutes=None, enable_elastic_disk=None, instance_pool_id=None, cluster_source=None, state=None, state_message=None, start_time=None, terminated_time=None, last_state_loss_time=None, last_activity_time=None, cluster_memory_mb=None, cluster_cores=None, default_tags=None, cluster_log_status=None, termination_reason=None):  # noqa: E501
        """ClusterInfo - a model defined in OpenAPI

        :param num_workers: The num_workers of this ClusterInfo.  # noqa: E501
        :type num_workers: int
        :param autoscale: The autoscale of this ClusterInfo.  # noqa: E501
        :type autoscale: AutoScale
        :param cluster_id: The cluster_id of this ClusterInfo.  # noqa: E501
        :type cluster_id: str
        :param creator_user_name: The creator_user_name of this ClusterInfo.  # noqa: E501
        :type creator_user_name: str
        :param driver: The driver of this ClusterInfo.  # noqa: E501
        :type driver: SparkNode
        :param executors: The executors of this ClusterInfo.  # noqa: E501
        :type executors: List[SparkNode]
        :param spark_context_id: The spark_context_id of this ClusterInfo.  # noqa: E501
        :type spark_context_id: int
        :param jdbc_port: The jdbc_port of this ClusterInfo.  # noqa: E501
        :type jdbc_port: int
        :param cluster_name: The cluster_name of this ClusterInfo.  # noqa: E501
        :type cluster_name: str
        :param spark_version: The spark_version of this ClusterInfo.  # noqa: E501
        :type spark_version: str
        :param spark_conf: The spark_conf of this ClusterInfo.  # noqa: E501
        :type spark_conf: Dict[str, object]
        :param aws_attributes: The aws_attributes of this ClusterInfo.  # noqa: E501
        :type aws_attributes: AwsAttributes
        :param node_type_id: The node_type_id of this ClusterInfo.  # noqa: E501
        :type node_type_id: str
        :param driver_node_type_id: The driver_node_type_id of this ClusterInfo.  # noqa: E501
        :type driver_node_type_id: str
        :param ssh_public_keys: The ssh_public_keys of this ClusterInfo.  # noqa: E501
        :type ssh_public_keys: List[str]
        :param custom_tags: The custom_tags of this ClusterInfo.  # noqa: E501
        :type custom_tags: List[Dict]
        :param cluster_log_conf: The cluster_log_conf of this ClusterInfo.  # noqa: E501
        :type cluster_log_conf: ClusterLogConf
        :param init_scripts: The init_scripts of this ClusterInfo.  # noqa: E501
        :type init_scripts: List[InitScriptInfo]
        :param docker_image: The docker_image of this ClusterInfo.  # noqa: E501
        :type docker_image: DockerImage
        :param spark_env_vars: The spark_env_vars of this ClusterInfo.  # noqa: E501
        :type spark_env_vars: Dict[str, object]
        :param autotermination_minutes: The autotermination_minutes of this ClusterInfo.  # noqa: E501
        :type autotermination_minutes: int
        :param enable_elastic_disk: The enable_elastic_disk of this ClusterInfo.  # noqa: E501
        :type enable_elastic_disk: bool
        :param instance_pool_id: The instance_pool_id of this ClusterInfo.  # noqa: E501
        :type instance_pool_id: str
        :param cluster_source: The cluster_source of this ClusterInfo.  # noqa: E501
        :type cluster_source: ClusterSource
        :param state: The state of this ClusterInfo.  # noqa: E501
        :type state: ClusterState
        :param state_message: The state_message of this ClusterInfo.  # noqa: E501
        :type state_message: str
        :param start_time: The start_time of this ClusterInfo.  # noqa: E501
        :type start_time: int
        :param terminated_time: The terminated_time of this ClusterInfo.  # noqa: E501
        :type terminated_time: int
        :param last_state_loss_time: The last_state_loss_time of this ClusterInfo.  # noqa: E501
        :type last_state_loss_time: int
        :param last_activity_time: The last_activity_time of this ClusterInfo.  # noqa: E501
        :type last_activity_time: int
        :param cluster_memory_mb: The cluster_memory_mb of this ClusterInfo.  # noqa: E501
        :type cluster_memory_mb: int
        :param cluster_cores: The cluster_cores of this ClusterInfo.  # noqa: E501
        :type cluster_cores: float
        :param default_tags: The default_tags of this ClusterInfo.  # noqa: E501
        :type default_tags: Dict[str, str]
        :param cluster_log_status: The cluster_log_status of this ClusterInfo.  # noqa: E501
        :type cluster_log_status: LogSyncStatus
        :param termination_reason: The termination_reason of this ClusterInfo.  # noqa: E501
        :type termination_reason: TerminationReason
        """
        self.openapi_types = {
            'num_workers': int,
            'autoscale': AutoScale,
            'cluster_id': str,
            'creator_user_name': str,
            'driver': SparkNode,
            'executors': List[SparkNode],
            'spark_context_id': int,
            'jdbc_port': int,
            'cluster_name': str,
            'spark_version': str,
            'spark_conf': Dict[str, object],
            'aws_attributes': AwsAttributes,
            'node_type_id': str,
            'driver_node_type_id': str,
            'ssh_public_keys': List[str],
            'custom_tags': List[Dict],
            'cluster_log_conf': ClusterLogConf,
            'init_scripts': List[InitScriptInfo],
            'docker_image': DockerImage,
            'spark_env_vars': Dict[str, object],
            'autotermination_minutes': int,
            'enable_elastic_disk': bool,
            'instance_pool_id': str,
            'cluster_source': ClusterSource,
            'state': ClusterState,
            'state_message': str,
            'start_time': int,
            'terminated_time': int,
            'last_state_loss_time': int,
            'last_activity_time': int,
            'cluster_memory_mb': int,
            'cluster_cores': float,
            'default_tags': Dict[str, str],
            'cluster_log_status': LogSyncStatus,
            'termination_reason': TerminationReason
        }

        self.attribute_map = {
            'num_workers': 'num_workers',
            'autoscale': 'autoscale',
            'cluster_id': 'cluster_id',
            'creator_user_name': 'creator_user_name',
            'driver': 'driver',
            'executors': 'executors',
            'spark_context_id': 'spark_context_id',
            'jdbc_port': 'jdbc_port',
            'cluster_name': 'cluster_name',
            'spark_version': 'spark_version',
            'spark_conf': 'spark_conf',
            'aws_attributes': 'aws_attributes',
            'node_type_id': 'node_type_id',
            'driver_node_type_id': 'driver_node_type_id',
            'ssh_public_keys': 'ssh_public_keys',
            'custom_tags': 'custom_tags',
            'cluster_log_conf': 'cluster_log_conf',
            'init_scripts': 'init_scripts',
            'docker_image': 'docker_image',
            'spark_env_vars': 'spark_env_vars',
            'autotermination_minutes': 'autotermination_minutes',
            'enable_elastic_disk': 'enable_elastic_disk',
            'instance_pool_id': 'instance_pool_id',
            'cluster_source': 'cluster_source',
            'state': 'state',
            'state_message': 'state_message',
            'start_time': 'start_time',
            'terminated_time': 'terminated_time',
            'last_state_loss_time': 'last_state_loss_time',
            'last_activity_time': 'last_activity_time',
            'cluster_memory_mb': 'cluster_memory_mb',
            'cluster_cores': 'cluster_cores',
            'default_tags': 'default_tags',
            'cluster_log_status': 'cluster_log_status',
            'termination_reason': 'termination_reason'
        }

        self._num_workers = num_workers
        self._autoscale = autoscale
        self._cluster_id = cluster_id
        self._creator_user_name = creator_user_name
        self._driver = driver
        self._executors = executors
        self._spark_context_id = spark_context_id
        self._jdbc_port = jdbc_port
        self._cluster_name = cluster_name
        self._spark_version = spark_version
        self._spark_conf = spark_conf
        self._aws_attributes = aws_attributes
        self._node_type_id = node_type_id
        self._driver_node_type_id = driver_node_type_id
        self._ssh_public_keys = ssh_public_keys
        self._custom_tags = custom_tags
        self._cluster_log_conf = cluster_log_conf
        self._init_scripts = init_scripts
        self._docker_image = docker_image
        self._spark_env_vars = spark_env_vars
        self._autotermination_minutes = autotermination_minutes
        self._enable_elastic_disk = enable_elastic_disk
        self._instance_pool_id = instance_pool_id
        self._cluster_source = cluster_source
        self._state = state
        self._state_message = state_message
        self._start_time = start_time
        self._terminated_time = terminated_time
        self._last_state_loss_time = last_state_loss_time
        self._last_activity_time = last_activity_time
        self._cluster_memory_mb = cluster_memory_mb
        self._cluster_cores = cluster_cores
        self._default_tags = default_tags
        self._cluster_log_status = cluster_log_status
        self._termination_reason = termination_reason

    @classmethod
    def from_dict(cls, dikt) -> 'ClusterInfo':
        """Returns the dict as a model

        :param dikt: A dict.
        :type: dict
        :return: The ClusterInfo of this ClusterInfo.  # noqa: E501
        :rtype: ClusterInfo
        """
        return util.deserialize_model(dikt, cls)

    @property
    def num_workers(self):
        """Gets the num_workers of this ClusterInfo.

        If num_workers, number of worker nodes that this cluster must have. A cluster has one Spark driver and num_workers executors for a total of num_workers + 1 Spark nodes. **Note:** When reading the properties of a cluster, this field reflects the desired number of workers rather than the actual number of workers. For instance, if a cluster is resized from 5 to 10 workers, this field is immediately updated to reflect the target size of 10 workers, whereas the workers listed in `executors` gradually increase from 5 to 10 as the new nodes are provisioned.  # noqa: E501

        :return: The num_workers of this ClusterInfo.
        :rtype: int
        """
        return self._num_workers

    @num_workers.setter
    def num_workers(self, num_workers):
        """Sets the num_workers of this ClusterInfo.

        If num_workers, number of worker nodes that this cluster must have. A cluster has one Spark driver and num_workers executors for a total of num_workers + 1 Spark nodes. **Note:** When reading the properties of a cluster, this field reflects the desired number of workers rather than the actual number of workers. For instance, if a cluster is resized from 5 to 10 workers, this field is immediately updated to reflect the target size of 10 workers, whereas the workers listed in `executors` gradually increase from 5 to 10 as the new nodes are provisioned.  # noqa: E501

        :param num_workers: The num_workers of this ClusterInfo.
        :type num_workers: int
        """

        self._num_workers = num_workers

    @property
    def autoscale(self):
        """Gets the autoscale of this ClusterInfo.


        :return: The autoscale of this ClusterInfo.
        :rtype: AutoScale
        """
        return self._autoscale

    @autoscale.setter
    def autoscale(self, autoscale):
        """Sets the autoscale of this ClusterInfo.


        :param autoscale: The autoscale of this ClusterInfo.
        :type autoscale: AutoScale
        """

        self._autoscale = autoscale

    @property
    def cluster_id(self):
        """Gets the cluster_id of this ClusterInfo.

        Canonical identifier for the cluster. This ID is retained during cluster restarts and resizes, while each new cluster has a globally unique ID.  # noqa: E501

        :return: The cluster_id of this ClusterInfo.
        :rtype: str
        """
        return self._cluster_id

    @cluster_id.setter
    def cluster_id(self, cluster_id):
        """Sets the cluster_id of this ClusterInfo.

        Canonical identifier for the cluster. This ID is retained during cluster restarts and resizes, while each new cluster has a globally unique ID.  # noqa: E501

        :param cluster_id: The cluster_id of this ClusterInfo.
        :type cluster_id: str
        """

        self._cluster_id = cluster_id

    @property
    def creator_user_name(self):
        """Gets the creator_user_name of this ClusterInfo.

        Creator user name. The field won’t be included in the response if the user has already been deleted.  # noqa: E501

        :return: The creator_user_name of this ClusterInfo.
        :rtype: str
        """
        return self._creator_user_name

    @creator_user_name.setter
    def creator_user_name(self, creator_user_name):
        """Sets the creator_user_name of this ClusterInfo.

        Creator user name. The field won’t be included in the response if the user has already been deleted.  # noqa: E501

        :param creator_user_name: The creator_user_name of this ClusterInfo.
        :type creator_user_name: str
        """

        self._creator_user_name = creator_user_name

    @property
    def driver(self):
        """Gets the driver of this ClusterInfo.


        :return: The driver of this ClusterInfo.
        :rtype: SparkNode
        """
        return self._driver

    @driver.setter
    def driver(self, driver):
        """Sets the driver of this ClusterInfo.


        :param driver: The driver of this ClusterInfo.
        :type driver: SparkNode
        """

        self._driver = driver

    @property
    def executors(self):
        """Gets the executors of this ClusterInfo.

        Nodes on which the Spark executors reside.  # noqa: E501

        :return: The executors of this ClusterInfo.
        :rtype: List[SparkNode]
        """
        return self._executors

    @executors.setter
    def executors(self, executors):
        """Sets the executors of this ClusterInfo.

        Nodes on which the Spark executors reside.  # noqa: E501

        :param executors: The executors of this ClusterInfo.
        :type executors: List[SparkNode]
        """

        self._executors = executors

    @property
    def spark_context_id(self):
        """Gets the spark_context_id of this ClusterInfo.

        A canonical SparkContext identifier. This value _does_ change when the Spark driver restarts. The pair `(cluster_id, spark_context_id)` is a globally unique identifier over all Spark contexts.  # noqa: E501

        :return: The spark_context_id of this ClusterInfo.
        :rtype: int
        """
        return self._spark_context_id

    @spark_context_id.setter
    def spark_context_id(self, spark_context_id):
        """Sets the spark_context_id of this ClusterInfo.

        A canonical SparkContext identifier. This value _does_ change when the Spark driver restarts. The pair `(cluster_id, spark_context_id)` is a globally unique identifier over all Spark contexts.  # noqa: E501

        :param spark_context_id: The spark_context_id of this ClusterInfo.
        :type spark_context_id: int
        """

        self._spark_context_id = spark_context_id

    @property
    def jdbc_port(self):
        """Gets the jdbc_port of this ClusterInfo.

        Port on which Spark JDBC server is listening in the driver node. No service listens on this port in executor nodes.  # noqa: E501

        :return: The jdbc_port of this ClusterInfo.
        :rtype: int
        """
        return self._jdbc_port

    @jdbc_port.setter
    def jdbc_port(self, jdbc_port):
        """Sets the jdbc_port of this ClusterInfo.

        Port on which Spark JDBC server is listening in the driver node. No service listens on this port in executor nodes.  # noqa: E501

        :param jdbc_port: The jdbc_port of this ClusterInfo.
        :type jdbc_port: int
        """

        self._jdbc_port = jdbc_port

    @property
    def cluster_name(self):
        """Gets the cluster_name of this ClusterInfo.

        Cluster name requested by the user. This doesn’t have to be unique. If not specified at creation, the cluster name is an empty string.  # noqa: E501

        :return: The cluster_name of this ClusterInfo.
        :rtype: str
        """
        return self._cluster_name

    @cluster_name.setter
    def cluster_name(self, cluster_name):
        """Sets the cluster_name of this ClusterInfo.

        Cluster name requested by the user. This doesn’t have to be unique. If not specified at creation, the cluster name is an empty string.  # noqa: E501

        :param cluster_name: The cluster_name of this ClusterInfo.
        :type cluster_name: str
        """

        self._cluster_name = cluster_name

    @property
    def spark_version(self):
        """Gets the spark_version of this ClusterInfo.

        The runtime version of the cluster. You can retrieve a list of available runtime versions by using the [Runtime versions](https://docs.databricks.com/dev-tools/api/latest/clusters.html#runtime-versions) API call.  # noqa: E501

        :return: The spark_version of this ClusterInfo.
        :rtype: str
        """
        return self._spark_version

    @spark_version.setter
    def spark_version(self, spark_version):
        """Sets the spark_version of this ClusterInfo.

        The runtime version of the cluster. You can retrieve a list of available runtime versions by using the [Runtime versions](https://docs.databricks.com/dev-tools/api/latest/clusters.html#runtime-versions) API call.  # noqa: E501

        :param spark_version: The spark_version of this ClusterInfo.
        :type spark_version: str
        """

        self._spark_version = spark_version

    @property
    def spark_conf(self):
        """Gets the spark_conf of this ClusterInfo.

        An arbitrary object where the object key is a configuration propery name and the value is a configuration property value.  # noqa: E501

        :return: The spark_conf of this ClusterInfo.
        :rtype: Dict[str, object]
        """
        return self._spark_conf

    @spark_conf.setter
    def spark_conf(self, spark_conf):
        """Sets the spark_conf of this ClusterInfo.

        An arbitrary object where the object key is a configuration propery name and the value is a configuration property value.  # noqa: E501

        :param spark_conf: The spark_conf of this ClusterInfo.
        :type spark_conf: Dict[str, object]
        """

        self._spark_conf = spark_conf

    @property
    def aws_attributes(self):
        """Gets the aws_attributes of this ClusterInfo.


        :return: The aws_attributes of this ClusterInfo.
        :rtype: AwsAttributes
        """
        return self._aws_attributes

    @aws_attributes.setter
    def aws_attributes(self, aws_attributes):
        """Sets the aws_attributes of this ClusterInfo.


        :param aws_attributes: The aws_attributes of this ClusterInfo.
        :type aws_attributes: AwsAttributes
        """

        self._aws_attributes = aws_attributes

    @property
    def node_type_id(self):
        """Gets the node_type_id of this ClusterInfo.

        This field encodes, through a single value, the resources available to each of the Spark nodes in this cluster. For example, the Spark nodes can be provisioned and optimized for memory or compute intensive workloads. A list of available node types can be retrieved by using the [List node types](https://docs.databricks.com/dev-tools/api/latest/clusters.html#list-node-types) API call.  # noqa: E501

        :return: The node_type_id of this ClusterInfo.
        :rtype: str
        """
        return self._node_type_id

    @node_type_id.setter
    def node_type_id(self, node_type_id):
        """Sets the node_type_id of this ClusterInfo.

        This field encodes, through a single value, the resources available to each of the Spark nodes in this cluster. For example, the Spark nodes can be provisioned and optimized for memory or compute intensive workloads. A list of available node types can be retrieved by using the [List node types](https://docs.databricks.com/dev-tools/api/latest/clusters.html#list-node-types) API call.  # noqa: E501

        :param node_type_id: The node_type_id of this ClusterInfo.
        :type node_type_id: str
        """

        self._node_type_id = node_type_id

    @property
    def driver_node_type_id(self):
        """Gets the driver_node_type_id of this ClusterInfo.

        The node type of the Spark driver. This field is optional; if unset, the driver node type is set as the same value as `node_type_id` defined above.  # noqa: E501

        :return: The driver_node_type_id of this ClusterInfo.
        :rtype: str
        """
        return self._driver_node_type_id

    @driver_node_type_id.setter
    def driver_node_type_id(self, driver_node_type_id):
        """Sets the driver_node_type_id of this ClusterInfo.

        The node type of the Spark driver. This field is optional; if unset, the driver node type is set as the same value as `node_type_id` defined above.  # noqa: E501

        :param driver_node_type_id: The driver_node_type_id of this ClusterInfo.
        :type driver_node_type_id: str
        """

        self._driver_node_type_id = driver_node_type_id

    @property
    def ssh_public_keys(self):
        """Gets the ssh_public_keys of this ClusterInfo.

        SSH public key contents that are added to each Spark node in this cluster. The corresponding private keys can be used to login with the user name `ubuntu` on port `2200`. Up to 10 keys can be specified.  # noqa: E501

        :return: The ssh_public_keys of this ClusterInfo.
        :rtype: List[str]
        """
        return self._ssh_public_keys

    @ssh_public_keys.setter
    def ssh_public_keys(self, ssh_public_keys):
        """Sets the ssh_public_keys of this ClusterInfo.

        SSH public key contents that are added to each Spark node in this cluster. The corresponding private keys can be used to login with the user name `ubuntu` on port `2200`. Up to 10 keys can be specified.  # noqa: E501

        :param ssh_public_keys: The ssh_public_keys of this ClusterInfo.
        :type ssh_public_keys: List[str]
        """

        self._ssh_public_keys = ssh_public_keys

    @property
    def custom_tags(self):
        """Gets the custom_tags of this ClusterInfo.

        An object containing a set of tags for cluster resources. Databricks tags all cluster resources (such as AWS instances and EBS volumes) with these tags in addition to default_tags.  **Note**:  * Tags are not supported on legacy node types such as compute-optimized and memory-optimized * Databricks allows at most 45 custom tags  # noqa: E501

        :return: The custom_tags of this ClusterInfo.
        :rtype: List[Dict]
        """
        return self._custom_tags

    @custom_tags.setter
    def custom_tags(self, custom_tags):
        """Sets the custom_tags of this ClusterInfo.

        An object containing a set of tags for cluster resources. Databricks tags all cluster resources (such as AWS instances and EBS volumes) with these tags in addition to default_tags.  **Note**:  * Tags are not supported on legacy node types such as compute-optimized and memory-optimized * Databricks allows at most 45 custom tags  # noqa: E501

        :param custom_tags: The custom_tags of this ClusterInfo.
        :type custom_tags: List[Dict]
        """

        self._custom_tags = custom_tags

    @property
    def cluster_log_conf(self):
        """Gets the cluster_log_conf of this ClusterInfo.


        :return: The cluster_log_conf of this ClusterInfo.
        :rtype: ClusterLogConf
        """
        return self._cluster_log_conf

    @cluster_log_conf.setter
    def cluster_log_conf(self, cluster_log_conf):
        """Sets the cluster_log_conf of this ClusterInfo.


        :param cluster_log_conf: The cluster_log_conf of this ClusterInfo.
        :type cluster_log_conf: ClusterLogConf
        """

        self._cluster_log_conf = cluster_log_conf

    @property
    def init_scripts(self):
        """Gets the init_scripts of this ClusterInfo.

        The configuration for storing init scripts. Any number of destinations can be specified. The scripts are executed sequentially in the order provided. If `cluster_log_conf` is specified, init script logs are sent to `<destination>/<cluster-ID>/init_scripts`.  # noqa: E501

        :return: The init_scripts of this ClusterInfo.
        :rtype: List[InitScriptInfo]
        """
        return self._init_scripts

    @init_scripts.setter
    def init_scripts(self, init_scripts):
        """Sets the init_scripts of this ClusterInfo.

        The configuration for storing init scripts. Any number of destinations can be specified. The scripts are executed sequentially in the order provided. If `cluster_log_conf` is specified, init script logs are sent to `<destination>/<cluster-ID>/init_scripts`.  # noqa: E501

        :param init_scripts: The init_scripts of this ClusterInfo.
        :type init_scripts: List[InitScriptInfo]
        """

        self._init_scripts = init_scripts

    @property
    def docker_image(self):
        """Gets the docker_image of this ClusterInfo.


        :return: The docker_image of this ClusterInfo.
        :rtype: DockerImage
        """
        return self._docker_image

    @docker_image.setter
    def docker_image(self, docker_image):
        """Sets the docker_image of this ClusterInfo.


        :param docker_image: The docker_image of this ClusterInfo.
        :type docker_image: DockerImage
        """

        self._docker_image = docker_image

    @property
    def spark_env_vars(self):
        """Gets the spark_env_vars of this ClusterInfo.

        An arbitrary object where the object key is an environment variable name and the value is an environment variable value.  # noqa: E501

        :return: The spark_env_vars of this ClusterInfo.
        :rtype: Dict[str, object]
        """
        return self._spark_env_vars

    @spark_env_vars.setter
    def spark_env_vars(self, spark_env_vars):
        """Sets the spark_env_vars of this ClusterInfo.

        An arbitrary object where the object key is an environment variable name and the value is an environment variable value.  # noqa: E501

        :param spark_env_vars: The spark_env_vars of this ClusterInfo.
        :type spark_env_vars: Dict[str, object]
        """

        self._spark_env_vars = spark_env_vars

    @property
    def autotermination_minutes(self):
        """Gets the autotermination_minutes of this ClusterInfo.

        Automatically terminates the cluster after it is inactive for this time in minutes. If not set, this cluster is not be automatically terminated. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination.  # noqa: E501

        :return: The autotermination_minutes of this ClusterInfo.
        :rtype: int
        """
        return self._autotermination_minutes

    @autotermination_minutes.setter
    def autotermination_minutes(self, autotermination_minutes):
        """Sets the autotermination_minutes of this ClusterInfo.

        Automatically terminates the cluster after it is inactive for this time in minutes. If not set, this cluster is not be automatically terminated. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination.  # noqa: E501

        :param autotermination_minutes: The autotermination_minutes of this ClusterInfo.
        :type autotermination_minutes: int
        """

        self._autotermination_minutes = autotermination_minutes

    @property
    def enable_elastic_disk(self):
        """Gets the enable_elastic_disk of this ClusterInfo.

        Autoscaling Local Storage: when enabled, this cluster dynamically acquires additional disk space when its Spark workers are running low on disk space. This feature requires specific AWS permissions to function correctly - refer to [Autoscaling local storage](https://docs.databricks.com/clusters/configure.html#autoscaling-local-storage) for details.  # noqa: E501

        :return: The enable_elastic_disk of this ClusterInfo.
        :rtype: bool
        """
        return self._enable_elastic_disk

    @enable_elastic_disk.setter
    def enable_elastic_disk(self, enable_elastic_disk):
        """Sets the enable_elastic_disk of this ClusterInfo.

        Autoscaling Local Storage: when enabled, this cluster dynamically acquires additional disk space when its Spark workers are running low on disk space. This feature requires specific AWS permissions to function correctly - refer to [Autoscaling local storage](https://docs.databricks.com/clusters/configure.html#autoscaling-local-storage) for details.  # noqa: E501

        :param enable_elastic_disk: The enable_elastic_disk of this ClusterInfo.
        :type enable_elastic_disk: bool
        """

        self._enable_elastic_disk = enable_elastic_disk

    @property
    def instance_pool_id(self):
        """Gets the instance_pool_id of this ClusterInfo.

        The optional ID of the instance pool to which the cluster belongs. Refer to [Pools](https://docs.databricks.com/clusters/instance-pools/index.html) for details.  # noqa: E501

        :return: The instance_pool_id of this ClusterInfo.
        :rtype: str
        """
        return self._instance_pool_id

    @instance_pool_id.setter
    def instance_pool_id(self, instance_pool_id):
        """Sets the instance_pool_id of this ClusterInfo.

        The optional ID of the instance pool to which the cluster belongs. Refer to [Pools](https://docs.databricks.com/clusters/instance-pools/index.html) for details.  # noqa: E501

        :param instance_pool_id: The instance_pool_id of this ClusterInfo.
        :type instance_pool_id: str
        """

        self._instance_pool_id = instance_pool_id

    @property
    def cluster_source(self):
        """Gets the cluster_source of this ClusterInfo.


        :return: The cluster_source of this ClusterInfo.
        :rtype: ClusterSource
        """
        return self._cluster_source

    @cluster_source.setter
    def cluster_source(self, cluster_source):
        """Sets the cluster_source of this ClusterInfo.


        :param cluster_source: The cluster_source of this ClusterInfo.
        :type cluster_source: ClusterSource
        """

        self._cluster_source = cluster_source

    @property
    def state(self):
        """Gets the state of this ClusterInfo.


        :return: The state of this ClusterInfo.
        :rtype: ClusterState
        """
        return self._state

    @state.setter
    def state(self, state):
        """Sets the state of this ClusterInfo.


        :param state: The state of this ClusterInfo.
        :type state: ClusterState
        """

        self._state = state

    @property
    def state_message(self):
        """Gets the state_message of this ClusterInfo.

        A message associated with the most recent state transition (for example, the reason why the cluster entered a `TERMINATED` state). This field is unstructured, and its exact format is subject to change.  # noqa: E501

        :return: The state_message of this ClusterInfo.
        :rtype: str
        """
        return self._state_message

    @state_message.setter
    def state_message(self, state_message):
        """Sets the state_message of this ClusterInfo.

        A message associated with the most recent state transition (for example, the reason why the cluster entered a `TERMINATED` state). This field is unstructured, and its exact format is subject to change.  # noqa: E501

        :param state_message: The state_message of this ClusterInfo.
        :type state_message: str
        """

        self._state_message = state_message

    @property
    def start_time(self):
        """Gets the start_time of this ClusterInfo.

        Time (in epoch milliseconds) when the cluster creation request was received (when the cluster entered a `PENDING` state).  # noqa: E501

        :return: The start_time of this ClusterInfo.
        :rtype: int
        """
        return self._start_time

    @start_time.setter
    def start_time(self, start_time):
        """Sets the start_time of this ClusterInfo.

        Time (in epoch milliseconds) when the cluster creation request was received (when the cluster entered a `PENDING` state).  # noqa: E501

        :param start_time: The start_time of this ClusterInfo.
        :type start_time: int
        """

        self._start_time = start_time

    @property
    def terminated_time(self):
        """Gets the terminated_time of this ClusterInfo.

        Time (in epoch milliseconds) when the cluster was terminated, if applicable.  # noqa: E501

        :return: The terminated_time of this ClusterInfo.
        :rtype: int
        """
        return self._terminated_time

    @terminated_time.setter
    def terminated_time(self, terminated_time):
        """Sets the terminated_time of this ClusterInfo.

        Time (in epoch milliseconds) when the cluster was terminated, if applicable.  # noqa: E501

        :param terminated_time: The terminated_time of this ClusterInfo.
        :type terminated_time: int
        """

        self._terminated_time = terminated_time

    @property
    def last_state_loss_time(self):
        """Gets the last_state_loss_time of this ClusterInfo.

        Time when the cluster driver last lost its state (due to a restart or driver failure).  # noqa: E501

        :return: The last_state_loss_time of this ClusterInfo.
        :rtype: int
        """
        return self._last_state_loss_time

    @last_state_loss_time.setter
    def last_state_loss_time(self, last_state_loss_time):
        """Sets the last_state_loss_time of this ClusterInfo.

        Time when the cluster driver last lost its state (due to a restart or driver failure).  # noqa: E501

        :param last_state_loss_time: The last_state_loss_time of this ClusterInfo.
        :type last_state_loss_time: int
        """

        self._last_state_loss_time = last_state_loss_time

    @property
    def last_activity_time(self):
        """Gets the last_activity_time of this ClusterInfo.

        Time (in epoch milliseconds) when the cluster was last active. A cluster is active if there is at least one command that has not finished on the cluster. This field is available after the cluster has reached a `RUNNING` state. Updates to this field are made as best-effort attempts. Certain versions of Spark do not support reporting of cluster activity. Refer to [Automatic termination](https://docs.databricks.com/clusters/clusters-manage.html#automatic-termination) for details.  # noqa: E501

        :return: The last_activity_time of this ClusterInfo.
        :rtype: int
        """
        return self._last_activity_time

    @last_activity_time.setter
    def last_activity_time(self, last_activity_time):
        """Sets the last_activity_time of this ClusterInfo.

        Time (in epoch milliseconds) when the cluster was last active. A cluster is active if there is at least one command that has not finished on the cluster. This field is available after the cluster has reached a `RUNNING` state. Updates to this field are made as best-effort attempts. Certain versions of Spark do not support reporting of cluster activity. Refer to [Automatic termination](https://docs.databricks.com/clusters/clusters-manage.html#automatic-termination) for details.  # noqa: E501

        :param last_activity_time: The last_activity_time of this ClusterInfo.
        :type last_activity_time: int
        """

        self._last_activity_time = last_activity_time

    @property
    def cluster_memory_mb(self):
        """Gets the cluster_memory_mb of this ClusterInfo.

        Total amount of cluster memory, in megabytes.  # noqa: E501

        :return: The cluster_memory_mb of this ClusterInfo.
        :rtype: int
        """
        return self._cluster_memory_mb

    @cluster_memory_mb.setter
    def cluster_memory_mb(self, cluster_memory_mb):
        """Sets the cluster_memory_mb of this ClusterInfo.

        Total amount of cluster memory, in megabytes.  # noqa: E501

        :param cluster_memory_mb: The cluster_memory_mb of this ClusterInfo.
        :type cluster_memory_mb: int
        """

        self._cluster_memory_mb = cluster_memory_mb

    @property
    def cluster_cores(self):
        """Gets the cluster_cores of this ClusterInfo.

        Number of CPU cores available for this cluster. This can be fractional since certain node types are configured to share cores between Spark nodes on the same instance.  # noqa: E501

        :return: The cluster_cores of this ClusterInfo.
        :rtype: float
        """
        return self._cluster_cores

    @cluster_cores.setter
    def cluster_cores(self, cluster_cores):
        """Sets the cluster_cores of this ClusterInfo.

        Number of CPU cores available for this cluster. This can be fractional since certain node types are configured to share cores between Spark nodes on the same instance.  # noqa: E501

        :param cluster_cores: The cluster_cores of this ClusterInfo.
        :type cluster_cores: float
        """

        self._cluster_cores = cluster_cores

    @property
    def default_tags(self):
        """Gets the default_tags of this ClusterInfo.

        An object with key value pairs. The key length must be between 1 and 127 UTF-8 characters, inclusive. The value length must be less than or equal to 255 UTF-8 characters. For a list of all restrictions, see AWS Tag Restrictions: <https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html#tag-restrictions>  # noqa: E501

        :return: The default_tags of this ClusterInfo.
        :rtype: Dict[str, str]
        """
        return self._default_tags

    @default_tags.setter
    def default_tags(self, default_tags):
        """Sets the default_tags of this ClusterInfo.

        An object with key value pairs. The key length must be between 1 and 127 UTF-8 characters, inclusive. The value length must be less than or equal to 255 UTF-8 characters. For a list of all restrictions, see AWS Tag Restrictions: <https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html#tag-restrictions>  # noqa: E501

        :param default_tags: The default_tags of this ClusterInfo.
        :type default_tags: Dict[str, str]
        """

        self._default_tags = default_tags

    @property
    def cluster_log_status(self):
        """Gets the cluster_log_status of this ClusterInfo.


        :return: The cluster_log_status of this ClusterInfo.
        :rtype: LogSyncStatus
        """
        return self._cluster_log_status

    @cluster_log_status.setter
    def cluster_log_status(self, cluster_log_status):
        """Sets the cluster_log_status of this ClusterInfo.


        :param cluster_log_status: The cluster_log_status of this ClusterInfo.
        :type cluster_log_status: LogSyncStatus
        """

        self._cluster_log_status = cluster_log_status

    @property
    def termination_reason(self):
        """Gets the termination_reason of this ClusterInfo.


        :return: The termination_reason of this ClusterInfo.
        :rtype: TerminationReason
        """
        return self._termination_reason

    @termination_reason.setter
    def termination_reason(self, termination_reason):
        """Sets the termination_reason of this ClusterInfo.


        :param termination_reason: The termination_reason of this ClusterInfo.
        :type termination_reason: TerminationReason
        """

        self._termination_reason = termination_reason
