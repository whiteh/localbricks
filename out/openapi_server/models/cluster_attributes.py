# coding: utf-8

from __future__ import absolute_import
from datetime import date, datetime  # noqa: F401

from typing import List, Dict  # noqa: F401

from openapi_server.models.base_model_ import Model
from openapi_server.models.aws_attributes import AwsAttributes
from openapi_server.models.cluster_log_conf import ClusterLogConf
from openapi_server.models.cluster_source import ClusterSource
from openapi_server.models.docker_image import DockerImage
from openapi_server.models.init_script_info import InitScriptInfo
from openapi_server import util

from openapi_server.models.aws_attributes import AwsAttributes  # noqa: E501
from openapi_server.models.cluster_log_conf import ClusterLogConf  # noqa: E501
from openapi_server.models.cluster_source import ClusterSource  # noqa: E501
from openapi_server.models.docker_image import DockerImage  # noqa: E501
from openapi_server.models.init_script_info import InitScriptInfo  # noqa: E501

class ClusterAttributes(Model):
    """NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).

    Do not edit the class manually.
    """

    def __init__(self, cluster_name=None, spark_version=None, spark_conf=None, aws_attributes=None, node_type_id=None, driver_node_type_id=None, ssh_public_keys=None, custom_tags=None, cluster_log_conf=None, init_scripts=None, docker_image=None, spark_env_vars=None, autotermination_minutes=None, enable_elastic_disk=None, instance_pool_id=None, cluster_source=None, policy_id=None):  # noqa: E501
        """ClusterAttributes - a model defined in OpenAPI

        :param cluster_name: The cluster_name of this ClusterAttributes.  # noqa: E501
        :type cluster_name: str
        :param spark_version: The spark_version of this ClusterAttributes.  # noqa: E501
        :type spark_version: str
        :param spark_conf: The spark_conf of this ClusterAttributes.  # noqa: E501
        :type spark_conf: Dict[str, object]
        :param aws_attributes: The aws_attributes of this ClusterAttributes.  # noqa: E501
        :type aws_attributes: AwsAttributes
        :param node_type_id: The node_type_id of this ClusterAttributes.  # noqa: E501
        :type node_type_id: str
        :param driver_node_type_id: The driver_node_type_id of this ClusterAttributes.  # noqa: E501
        :type driver_node_type_id: str
        :param ssh_public_keys: The ssh_public_keys of this ClusterAttributes.  # noqa: E501
        :type ssh_public_keys: List[str]
        :param custom_tags: The custom_tags of this ClusterAttributes.  # noqa: E501
        :type custom_tags: Dict[str, str]
        :param cluster_log_conf: The cluster_log_conf of this ClusterAttributes.  # noqa: E501
        :type cluster_log_conf: ClusterLogConf
        :param init_scripts: The init_scripts of this ClusterAttributes.  # noqa: E501
        :type init_scripts: List[InitScriptInfo]
        :param docker_image: The docker_image of this ClusterAttributes.  # noqa: E501
        :type docker_image: DockerImage
        :param spark_env_vars: The spark_env_vars of this ClusterAttributes.  # noqa: E501
        :type spark_env_vars: Dict[str, object]
        :param autotermination_minutes: The autotermination_minutes of this ClusterAttributes.  # noqa: E501
        :type autotermination_minutes: int
        :param enable_elastic_disk: The enable_elastic_disk of this ClusterAttributes.  # noqa: E501
        :type enable_elastic_disk: bool
        :param instance_pool_id: The instance_pool_id of this ClusterAttributes.  # noqa: E501
        :type instance_pool_id: str
        :param cluster_source: The cluster_source of this ClusterAttributes.  # noqa: E501
        :type cluster_source: ClusterSource
        :param policy_id: The policy_id of this ClusterAttributes.  # noqa: E501
        :type policy_id: str
        """
        self.openapi_types = {
            'cluster_name': str,
            'spark_version': str,
            'spark_conf': Dict[str, object],
            'aws_attributes': AwsAttributes,
            'node_type_id': str,
            'driver_node_type_id': str,
            'ssh_public_keys': List[str],
            'custom_tags': Dict[str, str],
            'cluster_log_conf': ClusterLogConf,
            'init_scripts': List[InitScriptInfo],
            'docker_image': DockerImage,
            'spark_env_vars': Dict[str, object],
            'autotermination_minutes': int,
            'enable_elastic_disk': bool,
            'instance_pool_id': str,
            'cluster_source': ClusterSource,
            'policy_id': str
        }

        self.attribute_map = {
            'cluster_name': 'cluster_name',
            'spark_version': 'spark_version',
            'spark_conf': 'spark_conf',
            'aws_attributes': 'aws_attributes',
            'node_type_id': 'node_type_id',
            'driver_node_type_id': 'driver_node_type_id',
            'ssh_public_keys': 'ssh_public_keys',
            'custom_tags': 'custom_tags',
            'cluster_log_conf': 'cluster_log_conf',
            'init_scripts': 'init_scripts',
            'docker_image': 'docker_image',
            'spark_env_vars': 'spark_env_vars',
            'autotermination_minutes': 'autotermination_minutes',
            'enable_elastic_disk': 'enable_elastic_disk',
            'instance_pool_id': 'instance_pool_id',
            'cluster_source': 'cluster_source',
            'policy_id': 'policy_id'
        }

        self._cluster_name = cluster_name
        self._spark_version = spark_version
        self._spark_conf = spark_conf
        self._aws_attributes = aws_attributes
        self._node_type_id = node_type_id
        self._driver_node_type_id = driver_node_type_id
        self._ssh_public_keys = ssh_public_keys
        self._custom_tags = custom_tags
        self._cluster_log_conf = cluster_log_conf
        self._init_scripts = init_scripts
        self._docker_image = docker_image
        self._spark_env_vars = spark_env_vars
        self._autotermination_minutes = autotermination_minutes
        self._enable_elastic_disk = enable_elastic_disk
        self._instance_pool_id = instance_pool_id
        self._cluster_source = cluster_source
        self._policy_id = policy_id

    @classmethod
    def from_dict(cls, dikt) -> 'ClusterAttributes':
        """Returns the dict as a model

        :param dikt: A dict.
        :type: dict
        :return: The ClusterAttributes of this ClusterAttributes.  # noqa: E501
        :rtype: ClusterAttributes
        """
        return util.deserialize_model(dikt, cls)

    @property
    def cluster_name(self):
        """Gets the cluster_name of this ClusterAttributes.

        Cluster name requested by the user. This doesn’t have to be unique. If not specified at creation, the cluster name is an empty string.  # noqa: E501

        :return: The cluster_name of this ClusterAttributes.
        :rtype: str
        """
        return self._cluster_name

    @cluster_name.setter
    def cluster_name(self, cluster_name):
        """Sets the cluster_name of this ClusterAttributes.

        Cluster name requested by the user. This doesn’t have to be unique. If not specified at creation, the cluster name is an empty string.  # noqa: E501

        :param cluster_name: The cluster_name of this ClusterAttributes.
        :type cluster_name: str
        """

        self._cluster_name = cluster_name

    @property
    def spark_version(self):
        """Gets the spark_version of this ClusterAttributes.

        The runtime version of the cluster, for example “5.0.x-scala2.11”. You can retrieve a list of available runtime versions by using the [Runtime versions](https://docs.databricks.com/dev-tools/api/latest/clusters.html#runtime-versions) API call.  # noqa: E501

        :return: The spark_version of this ClusterAttributes.
        :rtype: str
        """
        return self._spark_version

    @spark_version.setter
    def spark_version(self, spark_version):
        """Sets the spark_version of this ClusterAttributes.

        The runtime version of the cluster, for example “5.0.x-scala2.11”. You can retrieve a list of available runtime versions by using the [Runtime versions](https://docs.databricks.com/dev-tools/api/latest/clusters.html#runtime-versions) API call.  # noqa: E501

        :param spark_version: The spark_version of this ClusterAttributes.
        :type spark_version: str
        """

        self._spark_version = spark_version

    @property
    def spark_conf(self):
        """Gets the spark_conf of this ClusterAttributes.

        An arbitrary object where the object key is a configuration propery name and the value is a configuration property value.  # noqa: E501

        :return: The spark_conf of this ClusterAttributes.
        :rtype: Dict[str, object]
        """
        return self._spark_conf

    @spark_conf.setter
    def spark_conf(self, spark_conf):
        """Sets the spark_conf of this ClusterAttributes.

        An arbitrary object where the object key is a configuration propery name and the value is a configuration property value.  # noqa: E501

        :param spark_conf: The spark_conf of this ClusterAttributes.
        :type spark_conf: Dict[str, object]
        """

        self._spark_conf = spark_conf

    @property
    def aws_attributes(self):
        """Gets the aws_attributes of this ClusterAttributes.


        :return: The aws_attributes of this ClusterAttributes.
        :rtype: AwsAttributes
        """
        return self._aws_attributes

    @aws_attributes.setter
    def aws_attributes(self, aws_attributes):
        """Sets the aws_attributes of this ClusterAttributes.


        :param aws_attributes: The aws_attributes of this ClusterAttributes.
        :type aws_attributes: AwsAttributes
        """

        self._aws_attributes = aws_attributes

    @property
    def node_type_id(self):
        """Gets the node_type_id of this ClusterAttributes.

        This field encodes, through a single value, the resources available to each of the Spark nodes in this cluster. For example, the Spark nodes can be provisioned and optimized for memory or compute intensive workloads A list of available node types can be retrieved by using the [List node types](https://docs.databricks.com/dev-tools/api/latest/clusters.html#list-node-types) API call.  # noqa: E501

        :return: The node_type_id of this ClusterAttributes.
        :rtype: str
        """
        return self._node_type_id

    @node_type_id.setter
    def node_type_id(self, node_type_id):
        """Sets the node_type_id of this ClusterAttributes.

        This field encodes, through a single value, the resources available to each of the Spark nodes in this cluster. For example, the Spark nodes can be provisioned and optimized for memory or compute intensive workloads A list of available node types can be retrieved by using the [List node types](https://docs.databricks.com/dev-tools/api/latest/clusters.html#list-node-types) API call.  # noqa: E501

        :param node_type_id: The node_type_id of this ClusterAttributes.
        :type node_type_id: str
        """

        self._node_type_id = node_type_id

    @property
    def driver_node_type_id(self):
        """Gets the driver_node_type_id of this ClusterAttributes.

        The node type of the Spark driver. This field is optional; if unset, the driver node type is set as the same value as `node_type_id` defined above.  # noqa: E501

        :return: The driver_node_type_id of this ClusterAttributes.
        :rtype: str
        """
        return self._driver_node_type_id

    @driver_node_type_id.setter
    def driver_node_type_id(self, driver_node_type_id):
        """Sets the driver_node_type_id of this ClusterAttributes.

        The node type of the Spark driver. This field is optional; if unset, the driver node type is set as the same value as `node_type_id` defined above.  # noqa: E501

        :param driver_node_type_id: The driver_node_type_id of this ClusterAttributes.
        :type driver_node_type_id: str
        """

        self._driver_node_type_id = driver_node_type_id

    @property
    def ssh_public_keys(self):
        """Gets the ssh_public_keys of this ClusterAttributes.

        SSH public key contents that is added to each Spark node in this cluster. The corresponding private keys can be used to login with the user name `ubuntu` on port `2200`. Up to 10 keys can be specified.  # noqa: E501

        :return: The ssh_public_keys of this ClusterAttributes.
        :rtype: List[str]
        """
        return self._ssh_public_keys

    @ssh_public_keys.setter
    def ssh_public_keys(self, ssh_public_keys):
        """Sets the ssh_public_keys of this ClusterAttributes.

        SSH public key contents that is added to each Spark node in this cluster. The corresponding private keys can be used to login with the user name `ubuntu` on port `2200`. Up to 10 keys can be specified.  # noqa: E501

        :param ssh_public_keys: The ssh_public_keys of this ClusterAttributes.
        :type ssh_public_keys: List[str]
        """

        self._ssh_public_keys = ssh_public_keys

    @property
    def custom_tags(self):
        """Gets the custom_tags of this ClusterAttributes.

        An object with key value pairs. The key length must be between 1 and 127 UTF-8 characters, inclusive. The value length must be less than or equal to 255 UTF-8 characters. For a list of all restrictions, see AWS Tag Restrictions: <https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html#tag-restrictions>  # noqa: E501

        :return: The custom_tags of this ClusterAttributes.
        :rtype: Dict[str, str]
        """
        return self._custom_tags

    @custom_tags.setter
    def custom_tags(self, custom_tags):
        """Sets the custom_tags of this ClusterAttributes.

        An object with key value pairs. The key length must be between 1 and 127 UTF-8 characters, inclusive. The value length must be less than or equal to 255 UTF-8 characters. For a list of all restrictions, see AWS Tag Restrictions: <https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html#tag-restrictions>  # noqa: E501

        :param custom_tags: The custom_tags of this ClusterAttributes.
        :type custom_tags: Dict[str, str]
        """

        self._custom_tags = custom_tags

    @property
    def cluster_log_conf(self):
        """Gets the cluster_log_conf of this ClusterAttributes.


        :return: The cluster_log_conf of this ClusterAttributes.
        :rtype: ClusterLogConf
        """
        return self._cluster_log_conf

    @cluster_log_conf.setter
    def cluster_log_conf(self, cluster_log_conf):
        """Sets the cluster_log_conf of this ClusterAttributes.


        :param cluster_log_conf: The cluster_log_conf of this ClusterAttributes.
        :type cluster_log_conf: ClusterLogConf
        """

        self._cluster_log_conf = cluster_log_conf

    @property
    def init_scripts(self):
        """Gets the init_scripts of this ClusterAttributes.

        The configuration for storing init scripts. Any number of destinations can be specified. The scripts are executed sequentially in the order provided. If `cluster_log_conf` is specified, init script logs are sent to `<destination>/<cluster-ID>/init_scripts`.  # noqa: E501

        :return: The init_scripts of this ClusterAttributes.
        :rtype: List[InitScriptInfo]
        """
        return self._init_scripts

    @init_scripts.setter
    def init_scripts(self, init_scripts):
        """Sets the init_scripts of this ClusterAttributes.

        The configuration for storing init scripts. Any number of destinations can be specified. The scripts are executed sequentially in the order provided. If `cluster_log_conf` is specified, init script logs are sent to `<destination>/<cluster-ID>/init_scripts`.  # noqa: E501

        :param init_scripts: The init_scripts of this ClusterAttributes.
        :type init_scripts: List[InitScriptInfo]
        """

        self._init_scripts = init_scripts

    @property
    def docker_image(self):
        """Gets the docker_image of this ClusterAttributes.


        :return: The docker_image of this ClusterAttributes.
        :rtype: DockerImage
        """
        return self._docker_image

    @docker_image.setter
    def docker_image(self, docker_image):
        """Sets the docker_image of this ClusterAttributes.


        :param docker_image: The docker_image of this ClusterAttributes.
        :type docker_image: DockerImage
        """

        self._docker_image = docker_image

    @property
    def spark_env_vars(self):
        """Gets the spark_env_vars of this ClusterAttributes.

        An arbitrary object where the object key is an environment variable name and the value is an environment variable value.  # noqa: E501

        :return: The spark_env_vars of this ClusterAttributes.
        :rtype: Dict[str, object]
        """
        return self._spark_env_vars

    @spark_env_vars.setter
    def spark_env_vars(self, spark_env_vars):
        """Sets the spark_env_vars of this ClusterAttributes.

        An arbitrary object where the object key is an environment variable name and the value is an environment variable value.  # noqa: E501

        :param spark_env_vars: The spark_env_vars of this ClusterAttributes.
        :type spark_env_vars: Dict[str, object]
        """

        self._spark_env_vars = spark_env_vars

    @property
    def autotermination_minutes(self):
        """Gets the autotermination_minutes of this ClusterAttributes.

        Automatically terminates the cluster after it is inactive for this time in minutes. If not set, this cluster is not be automatically terminated. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination.  # noqa: E501

        :return: The autotermination_minutes of this ClusterAttributes.
        :rtype: int
        """
        return self._autotermination_minutes

    @autotermination_minutes.setter
    def autotermination_minutes(self, autotermination_minutes):
        """Sets the autotermination_minutes of this ClusterAttributes.

        Automatically terminates the cluster after it is inactive for this time in minutes. If not set, this cluster is not be automatically terminated. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination.  # noqa: E501

        :param autotermination_minutes: The autotermination_minutes of this ClusterAttributes.
        :type autotermination_minutes: int
        """

        self._autotermination_minutes = autotermination_minutes

    @property
    def enable_elastic_disk(self):
        """Gets the enable_elastic_disk of this ClusterAttributes.

        Autoscaling Local Storage: when enabled, this cluster dynamically acquires additional disk space when its Spark workers are running low on disk space. This feature requires specific AWS permissions to function correctly. Refer to [Autoscaling local storage](https://docs.databricks.com/clusters/configure.html#autoscaling-local-storage) for details.  # noqa: E501

        :return: The enable_elastic_disk of this ClusterAttributes.
        :rtype: bool
        """
        return self._enable_elastic_disk

    @enable_elastic_disk.setter
    def enable_elastic_disk(self, enable_elastic_disk):
        """Sets the enable_elastic_disk of this ClusterAttributes.

        Autoscaling Local Storage: when enabled, this cluster dynamically acquires additional disk space when its Spark workers are running low on disk space. This feature requires specific AWS permissions to function correctly. Refer to [Autoscaling local storage](https://docs.databricks.com/clusters/configure.html#autoscaling-local-storage) for details.  # noqa: E501

        :param enable_elastic_disk: The enable_elastic_disk of this ClusterAttributes.
        :type enable_elastic_disk: bool
        """

        self._enable_elastic_disk = enable_elastic_disk

    @property
    def instance_pool_id(self):
        """Gets the instance_pool_id of this ClusterAttributes.

        The optional ID of the instance pool to which the cluster belongs. Refer to [Pools](https://docs.databricks.com/clusters/instance-pools/index.html) for details.  # noqa: E501

        :return: The instance_pool_id of this ClusterAttributes.
        :rtype: str
        """
        return self._instance_pool_id

    @instance_pool_id.setter
    def instance_pool_id(self, instance_pool_id):
        """Sets the instance_pool_id of this ClusterAttributes.

        The optional ID of the instance pool to which the cluster belongs. Refer to [Pools](https://docs.databricks.com/clusters/instance-pools/index.html) for details.  # noqa: E501

        :param instance_pool_id: The instance_pool_id of this ClusterAttributes.
        :type instance_pool_id: str
        """

        self._instance_pool_id = instance_pool_id

    @property
    def cluster_source(self):
        """Gets the cluster_source of this ClusterAttributes.


        :return: The cluster_source of this ClusterAttributes.
        :rtype: ClusterSource
        """
        return self._cluster_source

    @cluster_source.setter
    def cluster_source(self, cluster_source):
        """Sets the cluster_source of this ClusterAttributes.


        :param cluster_source: The cluster_source of this ClusterAttributes.
        :type cluster_source: ClusterSource
        """

        self._cluster_source = cluster_source

    @property
    def policy_id(self):
        """Gets the policy_id of this ClusterAttributes.

        A [cluster policy](https://docs.databricks.com/dev-tools/api/latest/policies.html) ID.  # noqa: E501

        :return: The policy_id of this ClusterAttributes.
        :rtype: str
        """
        return self._policy_id

    @policy_id.setter
    def policy_id(self, policy_id):
        """Sets the policy_id of this ClusterAttributes.

        A [cluster policy](https://docs.databricks.com/dev-tools/api/latest/policies.html) ID.  # noqa: E501

        :param policy_id: The policy_id of this ClusterAttributes.
        :type policy_id: str
        """

        self._policy_id = policy_id
