# coding: utf-8

from __future__ import absolute_import
from datetime import date, datetime  # noqa: F401

from typing import List, Dict  # noqa: F401

from openapi_server.models.base_model_ import Model
from openapi_server.models.auto_scale import AutoScale
from openapi_server.models.aws_attributes import AwsAttributes
from openapi_server.models.cluster_log_conf import ClusterLogConf
from openapi_server.models.init_script_info import InitScriptInfo
from openapi_server import util

from openapi_server.models.auto_scale import AutoScale  # noqa: E501
from openapi_server.models.aws_attributes import AwsAttributes  # noqa: E501
from openapi_server.models.cluster_log_conf import ClusterLogConf  # noqa: E501
from openapi_server.models.init_script_info import InitScriptInfo  # noqa: E501

class NewCluster(Model):
    """NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).

    Do not edit the class manually.
    """

    def __init__(self, num_workers=None, autoscale=None, spark_version=None, spark_conf=None, aws_attributes=None, node_type_id=None, driver_node_type_id=None, ssh_public_keys=None, custom_tags=None, cluster_log_conf=None, init_scripts=None, spark_env_vars=None, enable_elastic_disk=None, driver_instance_pool_id=None, instance_pool_id=None, policy_id=None):  # noqa: E501
        """NewCluster - a model defined in OpenAPI

        :param num_workers: The num_workers of this NewCluster.  # noqa: E501
        :type num_workers: int
        :param autoscale: The autoscale of this NewCluster.  # noqa: E501
        :type autoscale: AutoScale
        :param spark_version: The spark_version of this NewCluster.  # noqa: E501
        :type spark_version: str
        :param spark_conf: The spark_conf of this NewCluster.  # noqa: E501
        :type spark_conf: Dict[str, object]
        :param aws_attributes: The aws_attributes of this NewCluster.  # noqa: E501
        :type aws_attributes: AwsAttributes
        :param node_type_id: The node_type_id of this NewCluster.  # noqa: E501
        :type node_type_id: str
        :param driver_node_type_id: The driver_node_type_id of this NewCluster.  # noqa: E501
        :type driver_node_type_id: str
        :param ssh_public_keys: The ssh_public_keys of this NewCluster.  # noqa: E501
        :type ssh_public_keys: List[str]
        :param custom_tags: The custom_tags of this NewCluster.  # noqa: E501
        :type custom_tags: Dict[str, str]
        :param cluster_log_conf: The cluster_log_conf of this NewCluster.  # noqa: E501
        :type cluster_log_conf: ClusterLogConf
        :param init_scripts: The init_scripts of this NewCluster.  # noqa: E501
        :type init_scripts: List[InitScriptInfo]
        :param spark_env_vars: The spark_env_vars of this NewCluster.  # noqa: E501
        :type spark_env_vars: Dict[str, object]
        :param enable_elastic_disk: The enable_elastic_disk of this NewCluster.  # noqa: E501
        :type enable_elastic_disk: bool
        :param driver_instance_pool_id: The driver_instance_pool_id of this NewCluster.  # noqa: E501
        :type driver_instance_pool_id: str
        :param instance_pool_id: The instance_pool_id of this NewCluster.  # noqa: E501
        :type instance_pool_id: str
        :param policy_id: The policy_id of this NewCluster.  # noqa: E501
        :type policy_id: str
        """
        self.openapi_types = {
            'num_workers': int,
            'autoscale': AutoScale,
            'spark_version': str,
            'spark_conf': Dict[str, object],
            'aws_attributes': AwsAttributes,
            'node_type_id': str,
            'driver_node_type_id': str,
            'ssh_public_keys': List[str],
            'custom_tags': Dict[str, str],
            'cluster_log_conf': ClusterLogConf,
            'init_scripts': List[InitScriptInfo],
            'spark_env_vars': Dict[str, object],
            'enable_elastic_disk': bool,
            'driver_instance_pool_id': str,
            'instance_pool_id': str,
            'policy_id': str
        }

        self.attribute_map = {
            'num_workers': 'num_workers',
            'autoscale': 'autoscale',
            'spark_version': 'spark_version',
            'spark_conf': 'spark_conf',
            'aws_attributes': 'aws_attributes',
            'node_type_id': 'node_type_id',
            'driver_node_type_id': 'driver_node_type_id',
            'ssh_public_keys': 'ssh_public_keys',
            'custom_tags': 'custom_tags',
            'cluster_log_conf': 'cluster_log_conf',
            'init_scripts': 'init_scripts',
            'spark_env_vars': 'spark_env_vars',
            'enable_elastic_disk': 'enable_elastic_disk',
            'driver_instance_pool_id': 'driver_instance_pool_id',
            'instance_pool_id': 'instance_pool_id',
            'policy_id': 'policy_id'
        }

        self._num_workers = num_workers
        self._autoscale = autoscale
        self._spark_version = spark_version
        self._spark_conf = spark_conf
        self._aws_attributes = aws_attributes
        self._node_type_id = node_type_id
        self._driver_node_type_id = driver_node_type_id
        self._ssh_public_keys = ssh_public_keys
        self._custom_tags = custom_tags
        self._cluster_log_conf = cluster_log_conf
        self._init_scripts = init_scripts
        self._spark_env_vars = spark_env_vars
        self._enable_elastic_disk = enable_elastic_disk
        self._driver_instance_pool_id = driver_instance_pool_id
        self._instance_pool_id = instance_pool_id
        self._policy_id = policy_id

    @classmethod
    def from_dict(cls, dikt) -> 'NewCluster':
        """Returns the dict as a model

        :param dikt: A dict.
        :type: dict
        :return: The NewCluster of this NewCluster.  # noqa: E501
        :rtype: NewCluster
        """
        return util.deserialize_model(dikt, cls)

    @property
    def num_workers(self):
        """Gets the num_workers of this NewCluster.

        If num_workers, number of worker nodes that this cluster must have. A cluster has one Spark driver and num_workers executors for a total of num_workers + 1 Spark nodes. When reading the properties of a cluster, this field reflects the desired number of workers rather than the actual current number of workers. For example, if a cluster is resized from 5 to 10 workers, this field immediately updates to reflect the target size of 10 workers, whereas the workers listed in `spark_info` gradually increase from 5 to 10 as the new nodes are provisioned.  # noqa: E501

        :return: The num_workers of this NewCluster.
        :rtype: int
        """
        return self._num_workers

    @num_workers.setter
    def num_workers(self, num_workers):
        """Sets the num_workers of this NewCluster.

        If num_workers, number of worker nodes that this cluster must have. A cluster has one Spark driver and num_workers executors for a total of num_workers + 1 Spark nodes. When reading the properties of a cluster, this field reflects the desired number of workers rather than the actual current number of workers. For example, if a cluster is resized from 5 to 10 workers, this field immediately updates to reflect the target size of 10 workers, whereas the workers listed in `spark_info` gradually increase from 5 to 10 as the new nodes are provisioned.  # noqa: E501

        :param num_workers: The num_workers of this NewCluster.
        :type num_workers: int
        """

        self._num_workers = num_workers

    @property
    def autoscale(self):
        """Gets the autoscale of this NewCluster.


        :return: The autoscale of this NewCluster.
        :rtype: AutoScale
        """
        return self._autoscale

    @autoscale.setter
    def autoscale(self, autoscale):
        """Sets the autoscale of this NewCluster.


        :param autoscale: The autoscale of this NewCluster.
        :type autoscale: AutoScale
        """

        self._autoscale = autoscale

    @property
    def spark_version(self):
        """Gets the spark_version of this NewCluster.

        The Spark version of the cluster. A list of available Spark versions can be retrieved by using the [Runtime versions](https://docs.databricks.com/dev-tools/api/latest/clusters.html#runtime-versions) API call. This field is required.  # noqa: E501

        :return: The spark_version of this NewCluster.
        :rtype: str
        """
        return self._spark_version

    @spark_version.setter
    def spark_version(self, spark_version):
        """Sets the spark_version of this NewCluster.

        The Spark version of the cluster. A list of available Spark versions can be retrieved by using the [Runtime versions](https://docs.databricks.com/dev-tools/api/latest/clusters.html#runtime-versions) API call. This field is required.  # noqa: E501

        :param spark_version: The spark_version of this NewCluster.
        :type spark_version: str
        """
        if spark_version is None:
            raise ValueError("Invalid value for `spark_version`, must not be `None`")  # noqa: E501

        self._spark_version = spark_version

    @property
    def spark_conf(self):
        """Gets the spark_conf of this NewCluster.

        An arbitrary object where the object key is a configuration propery name and the value is a configuration property value.  # noqa: E501

        :return: The spark_conf of this NewCluster.
        :rtype: Dict[str, object]
        """
        return self._spark_conf

    @spark_conf.setter
    def spark_conf(self, spark_conf):
        """Sets the spark_conf of this NewCluster.

        An arbitrary object where the object key is a configuration propery name and the value is a configuration property value.  # noqa: E501

        :param spark_conf: The spark_conf of this NewCluster.
        :type spark_conf: Dict[str, object]
        """

        self._spark_conf = spark_conf

    @property
    def aws_attributes(self):
        """Gets the aws_attributes of this NewCluster.


        :return: The aws_attributes of this NewCluster.
        :rtype: AwsAttributes
        """
        return self._aws_attributes

    @aws_attributes.setter
    def aws_attributes(self, aws_attributes):
        """Sets the aws_attributes of this NewCluster.


        :param aws_attributes: The aws_attributes of this NewCluster.
        :type aws_attributes: AwsAttributes
        """

        self._aws_attributes = aws_attributes

    @property
    def node_type_id(self):
        """Gets the node_type_id of this NewCluster.

        This field encodes, through a single value, the resources available to each of the Spark nodes in this cluster. For example, the Spark nodes can be provisioned and optimized for memory or compute intensive workloads A list of available node types can be retrieved by using the [List node types](https://docs.databricks.com/dev-tools/api/latest/clusters.html#list-node-types) API call. This field is required.  # noqa: E501

        :return: The node_type_id of this NewCluster.
        :rtype: str
        """
        return self._node_type_id

    @node_type_id.setter
    def node_type_id(self, node_type_id):
        """Sets the node_type_id of this NewCluster.

        This field encodes, through a single value, the resources available to each of the Spark nodes in this cluster. For example, the Spark nodes can be provisioned and optimized for memory or compute intensive workloads A list of available node types can be retrieved by using the [List node types](https://docs.databricks.com/dev-tools/api/latest/clusters.html#list-node-types) API call. This field is required.  # noqa: E501

        :param node_type_id: The node_type_id of this NewCluster.
        :type node_type_id: str
        """
        if node_type_id is None:
            raise ValueError("Invalid value for `node_type_id`, must not be `None`")  # noqa: E501

        self._node_type_id = node_type_id

    @property
    def driver_node_type_id(self):
        """Gets the driver_node_type_id of this NewCluster.

        The node type of the Spark driver. This field is optional; if unset, the driver node type is set as the same value as `node_type_id` defined above.  # noqa: E501

        :return: The driver_node_type_id of this NewCluster.
        :rtype: str
        """
        return self._driver_node_type_id

    @driver_node_type_id.setter
    def driver_node_type_id(self, driver_node_type_id):
        """Sets the driver_node_type_id of this NewCluster.

        The node type of the Spark driver. This field is optional; if unset, the driver node type is set as the same value as `node_type_id` defined above.  # noqa: E501

        :param driver_node_type_id: The driver_node_type_id of this NewCluster.
        :type driver_node_type_id: str
        """

        self._driver_node_type_id = driver_node_type_id

    @property
    def ssh_public_keys(self):
        """Gets the ssh_public_keys of this NewCluster.

        SSH public key contents that are added to each Spark node in this cluster. The corresponding private keys can be used to login with the user name `ubuntu` on port `2200`. Up to 10 keys can be specified.  # noqa: E501

        :return: The ssh_public_keys of this NewCluster.
        :rtype: List[str]
        """
        return self._ssh_public_keys

    @ssh_public_keys.setter
    def ssh_public_keys(self, ssh_public_keys):
        """Sets the ssh_public_keys of this NewCluster.

        SSH public key contents that are added to each Spark node in this cluster. The corresponding private keys can be used to login with the user name `ubuntu` on port `2200`. Up to 10 keys can be specified.  # noqa: E501

        :param ssh_public_keys: The ssh_public_keys of this NewCluster.
        :type ssh_public_keys: List[str]
        """

        self._ssh_public_keys = ssh_public_keys

    @property
    def custom_tags(self):
        """Gets the custom_tags of this NewCluster.

        An object with key value pairs. The key length must be between 1 and 127 UTF-8 characters, inclusive. The value length must be less than or equal to 255 UTF-8 characters. For a list of all restrictions, see AWS Tag Restrictions: <https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html#tag-restrictions>  # noqa: E501

        :return: The custom_tags of this NewCluster.
        :rtype: Dict[str, str]
        """
        return self._custom_tags

    @custom_tags.setter
    def custom_tags(self, custom_tags):
        """Sets the custom_tags of this NewCluster.

        An object with key value pairs. The key length must be between 1 and 127 UTF-8 characters, inclusive. The value length must be less than or equal to 255 UTF-8 characters. For a list of all restrictions, see AWS Tag Restrictions: <https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html#tag-restrictions>  # noqa: E501

        :param custom_tags: The custom_tags of this NewCluster.
        :type custom_tags: Dict[str, str]
        """

        self._custom_tags = custom_tags

    @property
    def cluster_log_conf(self):
        """Gets the cluster_log_conf of this NewCluster.


        :return: The cluster_log_conf of this NewCluster.
        :rtype: ClusterLogConf
        """
        return self._cluster_log_conf

    @cluster_log_conf.setter
    def cluster_log_conf(self, cluster_log_conf):
        """Sets the cluster_log_conf of this NewCluster.


        :param cluster_log_conf: The cluster_log_conf of this NewCluster.
        :type cluster_log_conf: ClusterLogConf
        """

        self._cluster_log_conf = cluster_log_conf

    @property
    def init_scripts(self):
        """Gets the init_scripts of this NewCluster.

        The configuration for storing init scripts. Any number of scripts can be specified. The scripts are executed sequentially in the order provided. If `cluster_log_conf` is specified, init script logs are sent to `<destination>/<cluster-id>/init_scripts`.  # noqa: E501

        :return: The init_scripts of this NewCluster.
        :rtype: List[InitScriptInfo]
        """
        return self._init_scripts

    @init_scripts.setter
    def init_scripts(self, init_scripts):
        """Sets the init_scripts of this NewCluster.

        The configuration for storing init scripts. Any number of scripts can be specified. The scripts are executed sequentially in the order provided. If `cluster_log_conf` is specified, init script logs are sent to `<destination>/<cluster-id>/init_scripts`.  # noqa: E501

        :param init_scripts: The init_scripts of this NewCluster.
        :type init_scripts: List[InitScriptInfo]
        """

        self._init_scripts = init_scripts

    @property
    def spark_env_vars(self):
        """Gets the spark_env_vars of this NewCluster.

        An arbitrary object where the object key is an environment variable name and the value is an environment variable value.  # noqa: E501

        :return: The spark_env_vars of this NewCluster.
        :rtype: Dict[str, object]
        """
        return self._spark_env_vars

    @spark_env_vars.setter
    def spark_env_vars(self, spark_env_vars):
        """Sets the spark_env_vars of this NewCluster.

        An arbitrary object where the object key is an environment variable name and the value is an environment variable value.  # noqa: E501

        :param spark_env_vars: The spark_env_vars of this NewCluster.
        :type spark_env_vars: Dict[str, object]
        """

        self._spark_env_vars = spark_env_vars

    @property
    def enable_elastic_disk(self):
        """Gets the enable_elastic_disk of this NewCluster.

        Autoscaling Local Storage: when enabled, this cluster dynamically acquires additional disk space when its Spark workers are running low on disk space. This feature requires specific AWS permissions to function correctly - refer to [Autoscaling local storage](https://docs.databricks.com/clusters/configure.html#autoscaling-local-storage) for details.  # noqa: E501

        :return: The enable_elastic_disk of this NewCluster.
        :rtype: bool
        """
        return self._enable_elastic_disk

    @enable_elastic_disk.setter
    def enable_elastic_disk(self, enable_elastic_disk):
        """Sets the enable_elastic_disk of this NewCluster.

        Autoscaling Local Storage: when enabled, this cluster dynamically acquires additional disk space when its Spark workers are running low on disk space. This feature requires specific AWS permissions to function correctly - refer to [Autoscaling local storage](https://docs.databricks.com/clusters/configure.html#autoscaling-local-storage) for details.  # noqa: E501

        :param enable_elastic_disk: The enable_elastic_disk of this NewCluster.
        :type enable_elastic_disk: bool
        """

        self._enable_elastic_disk = enable_elastic_disk

    @property
    def driver_instance_pool_id(self):
        """Gets the driver_instance_pool_id of this NewCluster.

        The optional ID of the instance pool to use for the driver node. You must also specify `instance_pool_id`. Refer to [Instance Pools API](https://docs.databricks.com/dev-tools/api/latest/instance-pools.html) for details.  # noqa: E501

        :return: The driver_instance_pool_id of this NewCluster.
        :rtype: str
        """
        return self._driver_instance_pool_id

    @driver_instance_pool_id.setter
    def driver_instance_pool_id(self, driver_instance_pool_id):
        """Sets the driver_instance_pool_id of this NewCluster.

        The optional ID of the instance pool to use for the driver node. You must also specify `instance_pool_id`. Refer to [Instance Pools API](https://docs.databricks.com/dev-tools/api/latest/instance-pools.html) for details.  # noqa: E501

        :param driver_instance_pool_id: The driver_instance_pool_id of this NewCluster.
        :type driver_instance_pool_id: str
        """

        self._driver_instance_pool_id = driver_instance_pool_id

    @property
    def instance_pool_id(self):
        """Gets the instance_pool_id of this NewCluster.

        The optional ID of the instance pool to use for cluster nodes. If `driver_instance_pool_id` is present, `instance_pool_id` is used for worker nodes only. Otherwise, it is used for both the driver node and worker nodes. Refer to [Instance Pools API](https://docs.databricks.com/dev-tools/api/latest/instance-pools.html) for details.  # noqa: E501

        :return: The instance_pool_id of this NewCluster.
        :rtype: str
        """
        return self._instance_pool_id

    @instance_pool_id.setter
    def instance_pool_id(self, instance_pool_id):
        """Sets the instance_pool_id of this NewCluster.

        The optional ID of the instance pool to use for cluster nodes. If `driver_instance_pool_id` is present, `instance_pool_id` is used for worker nodes only. Otherwise, it is used for both the driver node and worker nodes. Refer to [Instance Pools API](https://docs.databricks.com/dev-tools/api/latest/instance-pools.html) for details.  # noqa: E501

        :param instance_pool_id: The instance_pool_id of this NewCluster.
        :type instance_pool_id: str
        """

        self._instance_pool_id = instance_pool_id

    @property
    def policy_id(self):
        """Gets the policy_id of this NewCluster.

        A [cluster policy](https://docs.databricks.com/dev-tools/api/latest/policies.html) ID.  # noqa: E501

        :return: The policy_id of this NewCluster.
        :rtype: str
        """
        return self._policy_id

    @policy_id.setter
    def policy_id(self, policy_id):
        """Sets the policy_id of this NewCluster.

        A [cluster policy](https://docs.databricks.com/dev-tools/api/latest/policies.html) ID.  # noqa: E501

        :param policy_id: The policy_id of this NewCluster.
        :type policy_id: str
        """

        self._policy_id = policy_id
